[
  {
    "question_number": 1,
    "question_text": "A security engineer is reviewing event logs because an employee successfully connected a personal Windows laptop to the corporate network, which is against company policy. Company policy allows all Windows 10 and 11 laptops to connect to the system as long as the MDM agent installed by IT is running. Only compliant devices can connect, and the logic in the system to evaluate compliant laptops is as follows:\nif laptop['OSversion'] >= 10:\n    if laptop['agentRunning']:\n        return COMPLIANT\n    else:\n        return NON_COMPLIANT\nelse:\n    return COMPLIANT\nWhich of the following most likely occurred when the employee connected a personally owned Windows laptop and was allowed on the network?",
    "options": {
      "A": "The agent was not running on the laptop, which triggered a false positive.",
      "B": "The OS was a valid version, but the MDM agent was not installed, triggering a true positive.",
      "C": "The OS was running a Windows version below 10 and triggered a false negative.",
      "D": "The OS version was higher than 11, and the MDM agent was running, triggering a true negative."
    },
    "correct_answer": "C",
    "explanation": "The provided logic dictates that if the OS version is less than 10, the system returns 'COMPLIANT' regardless of the 'agentRunning' status. This means a Windows laptop with an OS version below 10 would be allowed onto the network, which is against company policy but considered 'COMPLIANT' by the system's logic. This scenario represents a false negative, where the system incorrectly classifies a non-compliant device as compliant.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 2,
    "question_text": "An organization is working to secure its development process to ensure developers cannot deploy artifacts directly into the production environment. Which of the following security practice recommendations would be the best to accomplish this objective?",
    "options": {
      "A": "Implement least privilege access to all systems.",
      "B": "Roll out security awareness training for all users.",
      "C": "Set up policies and systems with separation of duties.",
      "D": "Enforce job rotations for all developers and administrators.",
      "E": "Utilize mandatory vacations for all developers.",
      "F": "Review all access to production systems on a quarterly basis."
    },
    "correct_answer": "C",
    "explanation": "Separation of duties (SoD) is the most effective control to prevent a single individual from having too much control or the ability to commit fraud or errors without detection. By implementing policies and systems with separation of duties, an organization can ensure that the responsibility for deploying artifacts to production is divided among multiple individuals, preventing any single developer from having direct, unauthorized access to push code to production.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 3,
    "question_text": "A security architect discovers the following while reviewing code for a company's website: selection = \"SELECT Item FROM Catalog WHERE ItemID = \" & Request(\"ItemID\")\nWhich of the following should the security architect recommend?",
    "options": {
      "A": "Client-side processing",
      "B": "Query parameterization",
      "C": "Data normalization",
      "D": "Escape character blocking",
      "E": "URL encoding"
    },
    "correct_answer": "B",
    "explanation": "The code snippet shows a classic SQL injection vulnerability, where user input (Request(\"ItemID\")) is directly concatenated into a SQL query. Query parameterization (also known as prepared statements) is the most effective way to prevent SQL injection. It separates the SQL code from the user-provided data, ensuring that the input is treated as a literal value rather than executable SQL code.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 4,
    "question_text": "A security architect needs to enable a container orchestrator for DevSecOps and SOAR initiatives. The engineer has discovered that several Ansible YAML files used for the automation of configuration management have the following content:\n$ hostnamectl\nCOMPTIA001\n$ cat /etc/ansible/ansible.cfg\n[inventory]\nenable_plugins = kubernetes.core.k8s\n$ cat /etc/ansible/projects/roles/k8/default/main.yml\n---\n- Name: Create a Kubernetes Service Objects\n  kubernetes.core.k8s:\n    state: present\n    definition:\n      apiVersion: v2\n      kind: Service\n$ cat /etc/kubernetes/manifests\ninsecure-bind-address =localhost\"\nWhich of the following should the engineer do to correct the security issues presented within this content?",
    "options": {
      "A": "Update the kubernetes.core.k8s module to kubernetes.core.k8s_service in the main.yml file.",
      "B": "Update the COMPTIA001 hostname to localhost using the hostnamectl command.",
      "C": "Update the state: present module to state: absent in the main.yml file.",
      "D": "Update or remove the ansible.cfg file.",
      "E": "Update the insecure-bind-address from localhost to the COMPTIA001 in the manifests file."
    },
    "correct_answer": "D",
    "explanation": "The key security issue here is the `insecure-bind-address =localhost` in the `/etc/kubernetes/manifests` file. This configuration binds a service to localhost, which means it is only accessible from within the same machine and is generally considered insecure if not properly secured. The `ansible.cfg` file contains `enable_plugins = kubernetes.core.k8s`, which enables the Kubernetes module. However, the `insecure-bind-address` is found in the Kubernetes manifests, suggesting a misconfiguration in the Kubernetes deployment itself rather than an Ansible issue directly. To correct this, the `insecure-bind-address` should be removed or changed to a secure, appropriate value. The ansible.cfg file is not directly causing the insecurity, but it is part of the automation that might be deploying insecure configurations. If the `ansible.cfg` enables a plugin that is known to create insecure binding addresses by default, then modifying or removing it might be the appropriate action to prevent future insecure deployments. Otherwise, the problem description implies the *content* itself is the issue, and the `ansible.cfg` would be the source of *how* that content is deployed.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 5,
    "question_text": "A CRM company leverages a CSP PaaS service to host and publish its SaaS product. Recently, a large customer requested that all infrastructure components must meet strict regulatory requirements, including configuration management, patch management, and life-cycle management. Which of the following organizations is responsible for ensuring those regulatory requirements are met?",
    "options": {
      "A": "The CRM company",
      "B": "The CRM company's customer",
      "C": "The CSP",
      "D": "The regulatory body"
    },
    "correct_answer": "A",
    "explanation": "In a PaaS (Platform as a Service) model, the cloud service provider (CSP) manages the underlying infrastructure (servers, storage, networking, and virtualization), but the customer (the CRM company in this case) is responsible for the applications and data deployed on the platform. Regulatory compliance for the SaaS product, including configuration management, patch management, and life-cycle management of the application itself, remains the responsibility of the CRM company. The CSP is responsible for the security and compliance of the platform, but not the applications running on it.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 6,
    "question_text": "Company A is merging with Company B. Company A is a small, local company. Company B has a large, global presence. The two companies have a lot of duplication in their IT systems, processes, and procedures. On the new Chief Information Officer's (CIO's) first day, a fire breaks out at Company B's main data center. Which of the following actions should the CIO take first?",
    "options": {
      "A": "Determine whether the incident response plan has been tested at both companies, and use it to respond.",
      "B": "Review the incident response plans, and engage the disaster recovery plan while relying on the IT leaders from both companies.",
      "C": "Ensure hot, warm, and mobile disaster recovery sites are available, and give an update to the companies' leadership teams.",
      "D": "Initiate Company A's IT systems processes and procedures, assess the damage, and perform a BIA."
    },
    "correct_answer": "B",
    "explanation": "In an incident like a fire at a data center, the immediate priority is to activate the existing incident response and disaster recovery plans. Since the CIO is new and the companies are merging, reviewing the existing plans and engaging the current IT leaders for both companies is crucial to ensure a coordinated and effective response. This step is about leveraging existing resources and knowledge to address the immediate crisis, before assessing damages (D) or ensuring recovery sites (C), which are subsequent steps in a disaster recovery process. Testing (A) is for preparedness, not an immediate response.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 7,
    "question_text": "The results of an internal audit indicate several employees reused passwords that were previously included in a published list of compromised passwords.\nThe company has the following employee password policy:\nAttribute | Requirement\nComplexity | Enabled\nCharacter class | Special character, number\nLength | 10 characters\nHistory | 5\nMaximum age | 60 days\nMinimum age | 0\nWhich of the following should be implemented to best address the password reuse issue? (Choose two.)",
    "options": {
      "A": "Increase the minimum age to two days.",
      "B": "Increase the history to 20.",
      "C": "Increase the character length to 12.",
      "D": "Add case-sensitive requirements to character class.",
      "E": "Decrease the maximum age to 30 days.",
      "F": "Remove the complexity requirements.",
      "G": "Increase the maximum age to 120 days."
    },
    "correct_answer": "A B",
    "explanation": "To address password reuse from a published list of compromised passwords, the most effective measures are to increase the password history and potentially enforce a minimum password age. Increasing the password history (B) from 5 to 20 would prevent users from reusing any of their last 20 passwords, making it much harder to reuse a compromised password from a list. Increasing the minimum age (A) to two days (from 0) prevents users from quickly changing their password multiple times to cycle through the history and reuse an old one immediately.",
    "is_simulation": false,
    "question_type": "multiple_choice_multiple_answer"
  },
  {
    "question_number": 8,
    "question_text": "A mobile administrator is reviewing the following mobile device DHCP logs to ensure the proper mobile settings are applied to managed devices:\nAssign,192.168.1.10,UserA-MobileDevice,0236FB12CA0B\nAssign,192.168.1.23,UserA-MobileDevice,0E6FACFAD9\nAssign,192.168.1.11,UserA-MobileDevice,BAC034EF9451\nAssign,192.168.1.33,UserA-MobileDevice,0E938663221B\nWhich of the following mobile configuration settings is the mobile administrator verifying?",
    "options": {
      "A": "Service set identifier authentication",
      "B": "Wireless network auto joining",
      "C": "802.1X with mutual authentication",
      "D": "Association MAC address randomization"
    },
    "correct_answer": "D",
    "explanation": "The DHCP logs show multiple MAC addresses (e.g., 0236FB12CA0B, 0E6FACFAD9, BAC034EF9451, 0E938663221B) associated with the same mobile device (UserA-MobileDevice). This behavior, where a device uses different MAC addresses over time or per network, is characteristic of MAC address randomization. Mobile operating systems, such as iOS and Android, implement MAC address randomization to enhance user privacy by making it more difficult to track a device's location across different Wi-Fi networks.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 9,
    "question_text": "A security analyst is investigating a possible insider threat incident that involves the use of an unauthorized USB from a shared account to exfiltrate data. The event did not create an alert. The analyst has confirmed the USB hardware ID is not on the device allow list, but has not yet confirmed the owner of the USB device. Which of the following actions should the analyst take next?",
    "options": {
      "A": "Classify the incident as a false positive.",
      "B": "Classify the incident as a false negative.",
      "C": "Classify the incident as a true positive.",
      "D": "Classify the incident as a true negative."
    },
    "correct_answer": "B",
    "explanation": "The incident involves an unauthorized USB device being used to exfiltrate data, which is a malicious activity. The system *did not* create an alert, even though it should have, indicating a failure in detection. The analyst has confirmed the USB hardware ID is not on the device allow list, which means the event should have been flagged as a security concern. Therefore, this situation is a **false negative**: a malicious event occurred, but the security system failed to detect or alert on it.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 10,
    "question_text": "Which of the following security features do email signatures provide?",
    "options": {
      "A": "Non-repudiation",
      "B": "Body encryption",
      "C": "Code signing",
      "D": "Sender authentication",
      "E": "Chain of custody"
    },
    "correct_answer": "A",
    "explanation": "Email signatures, specifically digital signatures, provide non-repudiation. Non-repudiation ensures that the sender of a message cannot later deny having sent it, and the integrity of the message cannot be denied. A digital signature binds the sender's identity to the message content in a way that is verifiable by the recipient and prevents alteration of the message during transit.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 11,
    "question_text": "A software development company wants to ensure that users can confirm the software is legitimate when installing it. Which of the following is the best way for the company to achieve this security objective?",
    "options": {
      "A": "Code signing",
      "B": "Non-repudiation",
      "C": "Key escrow",
      "D": "Private keys"
    },
    "correct_answer": "A",
    "explanation": "Code signing is the best way to achieve this objective. Code signing involves digitally signing software executables and scripts to verify the author's identity and ensure that the code has not been tampered with or altered since it was signed. When a user installs signed software, their operating system or security software can verify the digital signature, assuring them that the software comes from a trusted source and is legitimate.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 12,
    "question_text": "While performing mandatory monthly patch updates on a production application server, the security analyst reports an instance of buffer overflow for a new application that was migrated to the cloud and is also publicly exposed. Security policy requires that only internal users have access to the application. Which of the following should the analyst implement to mitigate the issues reported? (Choose two.)",
    "options": {
      "A": "Configure firewall rules to block all external traffic.",
      "B": "Enable input validation for all fields.",
      "C": "Enable automatic updates to be installed on all servers.",
      "D": "Configure the security group to enable external traffic.",
      "E": "Set up a DLP policy to alert for exfiltration on all application servers.",
      "F": "Enable nightly vulnerability scans."
    },
    "correct_answer": "A B",
    "explanation": "The problem states two key issues: a buffer overflow vulnerability in a new, publicly exposed application, and a security policy that requires only internal users to access the application. \n\n1.  **A. Configure firewall rules to block all external traffic:** This directly addresses the policy requirement that only internal users should access the application and reduces the attack surface from the public internet. This is a crucial first step for a publicly exposed application that should only be internal.\n2.  **B. Enable input validation for all fields:** A buffer overflow vulnerability is typically caused by insufficient input validation, allowing an attacker to provide more data than a buffer can handle, leading to overwriting adjacent memory. Implementing robust input validation is a direct and effective mitigation for buffer overflow vulnerabilities.",
    "is_simulation": false,
    "question_type": "multiple_choice_multiple_answer"
  },
  {
    "question_number": 13,
    "question_text": "PKI can be used to support security requirements in the change management process. Which of the following capabilities does PKI provide for messages?",
    "options": {
      "A": "Non-repudiation",
      "B": "Confidentiality",
      "C": "Delivery receipts",
      "D": "Attestation"
    },
    "correct_answer": "A",
    "explanation": "PKI (Public Key Infrastructure) provides several security capabilities, including authentication, confidentiality, integrity, and non-repudiation. For messages, non-repudiation is a key capability provided by digital signatures, which are a core component of PKI. When a message is digitally signed using a private key, the recipient can verify the signature using the corresponding public key, proving the sender's identity and ensuring the message has not been altered. This prevents the sender from later denying they sent the message.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 14,
    "question_text": "Several unlabeled documents in a cloud document repository contain cardholder information. Which of the following configuration changes should be made to the DLP system to correctly label these documents in the future?",
    "options": {
      "A": "Digital rights management",
      "B": "Network traffic decryption",
      "C": "Regular expressions",
      "D": "Watermarking"
    },
    "correct_answer": "C",
    "explanation": "To correctly label documents containing specific sensitive information like cardholder data (e.g., credit card numbers), Data Loss Prevention (DLP) systems use detection methods. Regular expressions (regex) are a powerful and flexible way to define patterns that match sensitive data formats (e.g., a 16-digit credit card number with specific prefixes and checksums). Configuring the DLP system with appropriate regular expressions would allow it to automatically identify and label documents containing cardholder information.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 15,
    "question_text": "A systems administrator at a web-hosting provider has been tasked with renewing the public certificates of all customer sites. Which of the following would best support multiple domain names while minimizing the amount of certificates needed?",
    "options": {
      "A": "OCSP",
      "B": "CRL",
      "C": "SAN",
      "D": "CA"
    },
    "correct_answer": "C",
    "explanation": "Subject Alternative Name (SAN) certificates allow a single certificate to secure multiple domain names. This is the most efficient way to manage certificates for numerous customer sites, as it minimizes the number of certificates that need to be obtained, installed, and renewed, directly addressing the requirement of 'minimizing the amount of certificates needed' while supporting 'multiple domain names'.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 16,
    "question_text": "Which of the following best explain why organizations prefer to utilize code that is digitally signed? (Choose two.)",
    "options": {
      "A": "It provides origin assurance.",
      "B": "It verifies integrity.",
      "C": "It provides increased confidentiality.",
      "D": "It integrates with DRMs.",
      "E": "It verifies the recipient's identity.",
      "F": "It ensures the code is free of malware."
    },
    "correct_answer": "A B",
    "explanation": "Digital code signing provides two primary security benefits:\n\n1.  **A. It provides origin assurance:** The digital signature verifies the identity of the publisher or author of the code, assuring the user that the code comes from a known and trusted source.\n2.  **B. It verifies integrity:** The digital signature ensures that the code has not been altered or tampered with since it was signed. If even a single bit of the code is changed, the signature will become invalid.",
    "is_simulation": false,
    "question_type": "multiple_choice_multiple_answer"
  },
  {
    "question_number": 17,
    "question_text": "A security engineer receives reports through the organization's bug bounty program about remote code execution in a specific component in a custom application. Management wants to properly secure the component and proactively avoid similar issues. Which of the following is the best approach to uncover additional vulnerable paths in the application?",
    "options": {
      "A": "Leverage an exploitation framework to uncover vulnerabilities.",
      "B": "Use fuzz testing to uncover potential vulnerabilities in the application.",
      "C": "Utilize a software composition analysis tool to report known vulnerabilities.",
      "D": "Reverse engineer the application to look for vulnerable code paths.",
      "E": "Analyze the use of an HTTP intercepting proxy to dynamically uncover issues."
    },
    "correct_answer": "B",
    "explanation": "Fuzz testing (fuzzing) is an automated software testing technique that involves providing invalid, unexpected, or random data as inputs to a computer program. The program is then monitored for exceptions such as crashes, built-in code assertions, or potential memory leaks. This technique is highly effective at uncovering unknown vulnerabilities (zero-days) and unexpected behavior, including new remote code execution paths, which is exactly what the management wants to proactively avoid after the initial finding.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 18,
    "question_text": "A security technician is investigating a system that tracks inventory via a batch update each night. The technician is concerned that the system poses a risk to the business, as errors are occasionally generated and reported inventory appears incorrect. The following output log is provided:\nStarting Boxes = 20\nTransaction | Operation | Running total\n1 | + 10 | 30\n2 | + 30 | 50\n3 | + 10 | 60\n4 | - 10 | 50\n5 | - 40 | (below zero\nbalance) =10\n6 | + 30 | 20\n7 | + 10 | 60\n8 | + 40 | 60\nThe technician reviews the output of the batch job and discovers that the inventory was never less than zero, and the final inventory was 100 rather than 60. Which of the following should the technician do to resolve this issue?",
    "options": {
      "A": "Ensure that the application is using memory-safe functions to prevent integer overflows.",
      "B": "Recommend thread-safe processes in the code to eliminate race conditions.",
      "C": "Require the developers to include exception handlers to accommodate out-of-bounds results.",
      "D": "Move the batch processing from client side to server side to remove client processing inconsistencies."
    },
    "correct_answer": "C",
    "explanation": "The log output shows a \"below zero balance\" condition that is incorrectly represented as \"=10\" and then the final inventory incorrectly reported as 100 instead of 60. This indicates an issue where the system's logic is not properly handling out-of-bounds or unexpected results (like negative inventory) and is not correctly calculating the final total. Exception handlers in programming allow applications to gracefully manage errors or unexpected conditions, such as calculations resulting in values outside the expected range (e.g., negative inventory where only positive is valid). By implementing exception handlers, developers can catch and properly address these \"out-of-bounds\" results, preventing incorrect inventory reporting and ensuring data integrity.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 19,
    "question_text": "A programmer is reviewing the following proprietary piece of code that was identified as a vulnerability due to users being authenticated when they provide incorrect credentials:\nWhich of the following should the programmer implement to remediate the code vulnerability?",
    "options": {
      "A": "Salted hashing via the proprietary SHASH function",
      "B": "Input validation in the first two lines of code",
      "C": "Atomic execution of subroutines",
      "D": "TOCTOU remediation in SET USERACL",
      "E": "Database connection over encrypted channels"
    },
    "correct_answer": "B",
    "explanation": "The vulnerability description states that users are being authenticated even when they provide incorrect credentials. Looking at the code:\n\n`GET USERID`\n`GET PASS`\n`JUMP TO ALLOWUSER:`\n`IF USERID == GETDBUSER(USERID) AND HASH(PASS) == GETDBPASS(USERID)`\n`EXIT`\n`ALLOWUSER:`\n`SET USERACL(USERID)`\n\nThe issue likely lies in how `GET USERID` and `GET PASS` are handled, or in the logic that follows immediately after. If `GET USERID` or `GET PASS` can return a value that, when compared or hashed, bypasses the actual credential check, then input validation is missing. For example, if an empty or specific malformed input for USERID or PASS can lead to a true condition for the IF statement, then input validation is needed. While the code mentions a `HASH` function, the problem isn't about the strength of the hashing itself but about the authentication *logic* allowing incorrect credentials. The most direct and fundamental fix for allowing incorrect credentials is to ensure that the inputs (`USERID` and `PASS`) are rigorously validated before they are used in the authentication comparison.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 20,
    "question_text": "A senior cybersecurity engineer is solving a digital certificate issue in which the CA denied certificate issuance due to failed subject identity validation. At which of the following steps within the PKI enrollment process would the denial have occurred?",
    "options": {
      "A": "RAB",
      "B": "OCSP",
      "C": "CA",
      "D": "IdP"
    },
    "correct_answer": "C",
    "explanation": "In the PKI enrollment process, 'subject identity validation' is the step where the Certificate Authority (CA) verifies the identity of the entity requesting the certificate. If the CA denies issuance due to failed subject identity validation, it means this denial occurred at the **CA** step of the process. The CA is responsible for vetting the identity of the certificate requestor before signing and issuing the certificate. OCSP (Online Certificate Status Protocol) is for checking certificate revocation status after issuance, not during initial issuance. RAB (Registration Authority/Broker) would be the entity receiving the request before it goes to the CA, but the CA is the one performing the actual validation and denial.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 21,
    "question_text": "An internal user can send encrypted emails successfully to all recipients, except one, at an external organization. When the internal user attempts to send encrypted emails to this external recipient, a security error message appears. The issue does not affect unencrypted emails. The external recipient can send encrypted emails to internal users. Which of the following is the most likely cause of the issue?",
    "options": {
      "A": "The validity dates of the external recipient’s private key do not match the SSH keys with which the internal user is accessing the system.",
      "B": "The external recipient has an expired public/private key pair that has not been revoked by the CA.",
      "C": "The internal user's company email servers have an incorrect implementation of OCSP and CRL settings.",
      "D": "The external recipient's email address and the email address associated with the external recipient's public key are mismatched."
    },
    "correct_answer": "D",
    "explanation": "For encrypted email (e.g., S/MIME or PGP), the sender encrypts the email using the recipient's public key. If the external recipient can send encrypted emails to internal users, their public key is likely valid and trusted by the internal user's system. However, if the internal user *cannot* send encrypted emails to the external recipient and receives a security error, it often points to an issue with how the internal user's system perceives or uses the external recipient's public key. A common issue is that the email address the internal user is sending to does not precisely match the email address embedded in the external recipient's public key certificate. Digital certificates bind an identity (like an email address) to a public key. If the email address used for sending doesn't match the certificate's subject or Subject Alternative Name (SAN), the encryption process might fail.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 22,
    "question_text": "A security administrator is setting up a virtualization solution that needs to run services from a single host. Each service should be the only one running in its environment. Each environment needs to have its own operating system as a base but share the kernel version and properties of the running host. Which of the following technologies would best meet these requirements?",
    "options": {
      "A": "Containers",
      "B": "Type 1 hypervisor",
      "C": "Type 2 hypervisor",
      "D": "Virtual desktop infrastructure",
      "E": "Emulation"
    },
    "correct_answer": "A",
    "explanation": "The key requirements are:\n- Run services from a single host.\n- Each service is the only one running in its environment (isolation).\n- Each environment has its own operating system as a base but shares the kernel version and properties of the running host.\n\nContainers (like Docker or LXC) fit these requirements perfectly. They provide lightweight virtualization by isolating processes and their dependencies, allowing multiple isolated 'environments' to run on a single host. Critically, containers share the host operating system's kernel, which distinguishes them from virtual machines that run their own kernel on top of a hypervisor.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 23,
    "question_text": "A company has data it would like to aggregate from its PLCs for data visualization and predictive maintenance purposes. Which of the following is the most likely destination for the tag data from the PLCs?",
    "options": {
      "A": "External drive",
      "B": "Cloud storage",
      "C": "System aggregator",
      "D": "Local historian"
    },
    "correct_answer": "D",
    "explanation": "PLCs (Programmable Logic Controllers) are used in industrial control systems (ICS) to automate processes. They generate 'tag data' which are real-time values from sensors and control points. For data visualization and predictive maintenance, this high-volume, time-series data is typically collected and stored by a **local historian**. A historian is a specialized database designed to store and manage time-stamped process data efficiently, often located on-premises or very close to the operational technology (OT) network for low latency and reliability before being potentially aggregated to higher-level systems or cloud storage for analytics.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 24,
    "question_text": "Which of the following is the best way to protect the website browsing history for an executive who travels to foreign countries where internet usage is closely monitored?",
    "options": {
      "A": "DOH",
      "B": "EAP-TLS",
      "C": "Geofencing",
      "D": "Private browsing mode"
    },
    "correct_answer": "A",
    "explanation": "In foreign countries where internet usage is closely monitored, protecting browsing history requires encrypting DNS queries to prevent eavesdropping and censorship. DNS over HTTPS (DoH) encrypts DNS queries and responses using the HTTPS protocol, making it much harder for third parties (including ISPs or state-sponsored monitoring entities) to snoop on DNS requests and thus infer browsing history. While VPNs also provide broader encryption, DoH specifically addresses the DNS lookup privacy which is often a significant leak point.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 25,
    "question_text": "A systems administrator is working with the SOC to identify potential intrusions associated with ransomware. The SOC wants the systems administrator to perform network-level analysis to identify outbound traffic from any infected machines. Which of the following is the most appropriate action for the systems administrator to take?",
    "options": {
      "A": "Monitor for IoCs associated with C&C communications.",
      "B": "Tune alerts to Identify changes to administrative groups.",
      "C": "Review NetFlow logs for unexpected increases in egress traffic.",
      "D": "Perform binary hash comparisons to identify infected devices."
    },
    "correct_answer": "C",
    "explanation": "Ransomware, after encrypting data, often attempts to communicate with a command-and-control (C2) server or exfiltrate data. This communication typically involves outbound (egress) network traffic. NetFlow logs (or similar flow data like IPFIX or sFlow) capture metadata about network conversations, including source/destination IP addresses, ports, protocols, and the amount of data transferred. Reviewing NetFlow logs for unexpected increases in egress traffic would be a highly effective way to identify machines potentially compromised by ransomware, as large data transfers (exfiltration) or unusual C2 communications would manifest as anomalies in traffic volume or patterns.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 26,
    "question_text": "A retail organization wants to properly test and verify its capabilities to detect and/or prevent specific TTPs as mapped to the MITRE ATTACK framework specific to APTs. Which of the following should be used by the organization to accomplish this goal?",
    "options": {
      "A": "Tabletop exercise",
      "B": "Penetration test",
      "C": "Sandbox detonation",
      "D": "Honeypot"
    },
    "correct_answer": "B",
    "explanation": "The MITRE ATT&CK framework provides a comprehensive list of adversary tactics and techniques (TTPs). To 'properly test and verify its capabilities to detect and/or prevent specific TTPs' of APTs, a penetration test is the most appropriate method. A penetration test (or red teaming, which is a more advanced form) actively simulates real-world adversary TTPs to evaluate the effectiveness of an organization's security controls, including detection and prevention mechanisms. Tabletop exercises (A) are discussion-based, sandboxing (C) is for analyzing malware in isolation, and honeypots (D) are for luring and learning from attackers, not directly for testing internal detection/prevention capabilities against specific TTPs.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 27,
    "question_text": "IoCs were missed during a recent security incident due to the reliance on a signature-based detection platform. A security engineer must recommend a solution that can be implemented to address this shortcoming. Which of the following would be the most appropriate recommendation?",
    "options": {
      "A": "FIM",
      "B": "SASEC",
      "C": "UEBA",
      "D": "CSPM",
      "E": "EAP"
    },
    "correct_answer": "C",
    "explanation": "The core problem is that IoCs (Indicators of Compromise) were missed because the organization relies on a *signature-based detection platform*. Signature-based systems are effective against known threats but fail to detect novel or polymorphic attacks. To address this shortcoming, a solution that moves beyond signatures is needed.\n\nUEBA (User and Entity Behavior Analytics) is designed to detect anomalous behavior by users or entities that deviate from established baselines. It uses machine learning and statistical analysis to identify suspicious patterns that might indicate an attack, even if no specific signatures exist. This directly addresses the limitation of signature-based detection by focusing on behavior rather than known attack patterns. CSPM (Cloud Security Posture Management) is for cloud configuration compliance, FIM (File Integrity Monitoring) is for file changes, and EAP (Extensible Authentication Protocol) is for authentication.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 28,
    "question_text": "A company that provides services to clients who work with highly sensitive data would like to provide assurance that the data's confidentiality is maintained in a dynamic, low-risk environment. Which of the following would best achieve this goal? (Choose two.)",
    "options": {
      "A": "Install a SOAR on all endpoints.",
      "B": "Hash all files.",
      "C": "Install SIEM within a SOC.",
      "D": "Encrypt all data and files at rest, in transit, and in use.",
      "E": "Configure SOAR to monitor and intercept files and data leaving the network.",
      "F": "Implement file integrity monitoring."
    },
    "correct_answer": "D F",
    "explanation": "To maintain data confidentiality in a dynamic, low-risk environment, especially with highly sensitive data, the following two options are most effective:\n\n1.  **D. Encrypt all data and files at rest, in transit, and in use:** Encryption is the primary technical control for ensuring confidentiality. Encrypting data at rest (on storage), in transit (over networks), and in use (while being processed in memory, using technologies like confidential computing) ensures that even if data is accessed by unauthorized parties, it remains unreadable. This directly protects confidentiality.\n2.  **F. Implement file integrity monitoring:** While encryption protects confidentiality, File Integrity Monitoring (FIM) helps maintain integrity and indirectly supports confidentiality by detecting unauthorized changes to sensitive files. If an attacker bypasses other controls and attempts to modify or exfiltrate data, FIM can alert to unauthorized access or modification, indicating a potential breach of confidentiality. In a 'dynamic, low-risk environment,' detecting unauthorized changes is crucial for maintaining a secure posture.",
    "is_simulation": false,
    "question_type": "multiple_choice_multiple_answer"
  },
  {
    "question_number": 29,
    "question_text": "An organization wants to implement an access control system based on its data classification policy that includes the following data types:\nConfidential -\nRestricted -\nInternal -\nPublic Flag for Review -\nThe access control system should support SSO federation to map users into groups. Each group should only access systems that process and store data at the classification assigned to the group. Which of the following should the organization implement to enforce its requirements with a minimal impact to systems and resources?",
    "options": {
      "A": "A tagging strategy in which all resources are assigned a tag based on the data classification type, and a system that enforces attribute-based access control",
      "B": "Role-based access control that maps data types to internal roles, which are defined in the human resources department's source of truth system",
      "C": "Network microsegmentation based on data types, and a network access control system enforcing mandatory access control based on the user principal",
      "D": "A rule-based access control strategy enforced by the SSO system with rules managed by the internal LDAP and applied on a per-system basis"
    },
    "correct_answer": "A",
    "explanation": "The core requirement is to enforce access based on data classification (Confidential, Restricted, Internal, Public Flag for Review) with minimal impact to systems and resources, using SSO federation to map users to groups, and allowing groups to access systems based on the data classification. \n\n**Attribute-Based Access Control (ABAC)** is the most flexible and scalable solution for this scenario. ABAC grants access based on attributes (characteristics) of the user, the resource, and the environment, rather than fixed roles or rules. By implementing a tagging strategy where resources are assigned a tag based on their data classification (e.g., 'data_type: confidential'), and then using ABAC to enforce policies (e.g., 'Users with clearance_level: high can access resources tagged data_type: confidential'), the organization can achieve fine-grained control that aligns directly with its classification policy. This approach minimizes impact because access decisions are made dynamically based on attributes, rather than requiring complex, per-system rule configurations or rigid role assignments.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 30,
    "question_text": "A security analyst was monitoring the networks of a group of companies. The analyst identified several periods of concentrated, coordinated activity by unknown actors. The activity repeated at regular intervals and affected all the companies. Minor hardware outages that correlated with the same times as the discovered activity escalated in severity. Which of the following threat actors was most likely involved?",
    "options": {
      "A": "An organized crime collective running a ransomware campaign",
      "B": "A group of politically motivated hackers",
      "C": "Disgruntled employees who were recently terminated",
      "D": "An advanced persistent threat financed by a nation-state"
    },
    "correct_answer": "D",
    "explanation": "The description points strongly towards an **Advanced Persistent Threat (APT) financed by a nation-state (D)**:\n\n*   **Concentrated, coordinated activity by unknown actors:** APTs are typically well-funded, highly skilled, and operate in a coordinated manner, often with anonymity.\n*   **Activity repeated at regular intervals:** This indicates a methodical, sustained campaign rather than a sporadic attack. Nation-state APTs often engage in long-term espionage or disruption activities.\n*   **Affected all the companies (in a group):** This suggests a broad target scope, possibly for intelligence gathering or widespread disruption, characteristic of nation-state campaigns.\n*   **Minor hardware outages that correlated with the same times as the discovered activity escalated in severity:** This indicates an attacker with the capability and intent to cause disruption or damage, and potentially to escalate their impact over time. Nation-state actors may seek to disrupt critical infrastructure or industrial control systems, which could manifest as hardware outages.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 31,
    "question_text": "The company's client service team is receiving a large number of inquiries from clients regarding a new vulnerability. Which of the following would provide the customer service team with a consistent message to deliver directly to clients?",
    "options": {
      "A": "Communication plan",
      "B": "Response playbook",
      "C": "Disaster recovery procedure",
      "D": "Automated runbook"
    },
    "correct_answer": "B",
    "explanation": "A **response playbook** (also known as an incident response playbook or runbook) is a predefined set of procedures and guidelines for handling specific types of incidents. In this scenario, a playbook would outline the steps for the client service team to respond to inquiries about a new vulnerability, including providing consistent messaging, FAQs, and escalation procedures. This ensures that all team members deliver the same accurate information to clients, which is crucial for managing customer perception and trust during a security event.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 32,
    "question_text": "A company wants to use a process to embed a sign of ownership covertly inside a proprietary document without adding any identifying attributes. Which of the following would be best to use as part of the process to support copyright protections of the document?",
    "options": {
      "A": "Steganography",
      "B": "E-signature",
      "C": "Watermarking",
      "D": "Cryptography"
    },
    "correct_answer": "A",
    "explanation": "Steganography is the practice of concealing a file, message, image, or video within another file, message, image, or video. The goal is to hide the existence of the communication. In this context, embedding a 'sign of ownership covertly' without 'adding any identifying attributes' (meaning, not obviously visible or detectable as an attribute) into a proprietary document for copyright protection aligns precisely with steganography. Watermarking (C) is a type of steganography, but steganography is the broader and more accurate term for covert embedding.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 33,
    "question_text": "Which of the following utilizes policies that route packets to ensure only specific types of traffic are being sent to the correct destination based on application usage?",
    "options": {
      "A": "SDN",
      "B": "pcap",
      "C": "vmstat",
      "D": "DNSSEC",
      "E": "VPC"
    },
    "correct_answer": "A",
    "explanation": "Software-Defined Networking (SDN) is an architectural approach to networking that enables network administrators to manage network services through abstraction of lower-level functionality. SDN separates the network's control plane from the data plane, allowing network behavior to be dynamically programmed and managed through software. This programmability allows for the implementation of policies that can route packets based on various criteria, including application usage, ensuring that specific types of traffic are directed to the correct destinations. This capability is central to modern network security and traffic management, as it allows for agile and granular control over network flows.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 34,
    "question_text": "An incident response team completed recovery from offline backup for several workstations. The workstations were subjected to a ransomware attack after users fell victim to a spear-phishing campaign, despite a robust training program. Which of the following questions should be considered during the lessons-learned phase to most likely reduce the risk of reoccurrence? (Choose two.)",
    "options": {
      "A": "Are there opportunities for legal recourse against the originators of the spear-phishing campaign?",
      "B": "What internal and external stakeholders need to be notified of the breach?",
      "C": "Which methods can be implemented to increase speed of offline backup recovery?",
      "D": "What measurable user behaviors were exhibited that contributed to the compromise?",
      "E": "Which technical controls, if implemented, would provide defense when user training fails?",
      "F": "Which user roles are most often targeted by spear phishing attacks?"
    },
    "correct_answer": "D E",
    "explanation": "In a lessons-learned phase following a spear-phishing and ransomware incident, the goal is to identify root causes and implement improvements to prevent recurrence. \n\n1.  **D. What measurable user behaviors were exhibited that contributed to the compromise?** Even with robust training, users might still fall victim. Understanding *why* users clicked or opened malicious content (e.g., specific lures, cognitive biases) through behavioral analysis helps refine training and identify gaps in user awareness.\n2.  **E. Which technical controls, if implemented, would provide defense when user training fails?** Since user training failed in this instance, it's critical to identify technical safeguards that can act as a fallback. Examples include email sandboxing, advanced endpoint detection and response (EDR), web content filtering, attachment filtering, or isolation mechanisms that prevent ransomware from spreading even if a user clicks a malicious link or opens an infected attachment. These technical controls provide defense in depth.",
    "is_simulation": false,
    "question_type": "multiple_choice_multiple_answer"
  },
  {
    "question_number": 35,
    "question_text": "Two companies that recently merged would like to unify application access between the companies, without initially merging internal authentication stores. Which of the following technical strategies would best meet this objective?",
    "options": {
      "A": "Federation",
      "B": "RADIUS",
      "C": "TACACS+",
      "D": "MFA",
      "E": "ABAC"
    },
    "correct_answer": "A",
    "explanation": "Federation (specifically identity federation) is the best strategy to unify application access between two distinct organizations (or domains) without merging their underlying authentication stores. Identity federation allows users from one organization (identity provider) to access resources in another organization (service provider) using their existing credentials, without requiring new accounts or synchronizing user directories. This approach supports Single Sign-On (SSO) across the merged entities while maintaining the independence of their respective internal authentication systems.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 36,
    "question_text": "An analyst needs to evaluate all images and documents that are publicly shared on a website. Which of the following would be the best tool to evaluate the metadata of these files?",
    "options": {
      "A": "OllyDbg",
      "B": "ExifTool",
      "C": "Volatility",
      "D": "Ghidra"
    },
    "correct_answer": "B",
    "explanation": "ExifTool is a free and open-source software program for reading, writing, and manipulating image, audio, video, and PDF metadata. It supports various metadata formats, including EXIF, GPS, IPTC, XMP, JFIF, GeoTIFF, ICC Profile, Photoshop IRB, FlashPix, AFCP, and ID3, as well as the manufacturer-specific metadata of many digital cameras. For evaluating the metadata of publicly shared images and documents, ExifTool is the most suitable and widely used tool.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 37,
    "question_text": "An organization has deployed a cloud-based application that provides virtual event services globally to clients. During a typical event, thousands of users access various entry pages within a short period of time. The entry pages include sponsor-related content that is relatively static and is pulled from a database. When the first major event occurs, users report poor response time on the entry pages. Which of the following features is the most appropriate for the company to implement?",
    "options": {
      "A": "Horizontal scalability",
      "B": "Vertical scalability",
      "C": "Containerization",
      "D": "Static code analysis",
      "E": "Caching"
    },
    "correct_answer": "E",
    "explanation": "The problem describes a scenario where 'sponsor-related content that is relatively static' is causing 'poor response time' when 'thousands of users access various entry pages within a short period of time'. This is a classic use case for **caching**. Caching stores frequently accessed static or semi-static data (like sponsor content) closer to the users or in faster memory, reducing the need to repeatedly fetch it from the database or origin server. This significantly improves response times, especially during high-demand events. While horizontal scalability (A) can help with overall load, caching specifically addresses the latency for static content retrieval from a database.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 38,
    "question_text": "An organization's board of directors has asked the Chief Information Security Officer to build a third-party management program. Which of the following best explains a reason for this request?",
    "options": {
      "A": "Risk transference",
      "B": "Supply chain visibility",
      "C": "Support availability",
      "D": "Vulnerability management"
    },
    "correct_answer": "B",
    "explanation": "The primary reason for a board of directors to request a third-party management program is to gain **supply chain visibility**. In today's interconnected business environment, organizations rely heavily on third-party vendors and suppliers. These relationships introduce significant risks, as a vulnerability or breach in a third party's systems can directly impact the organization's security, data, and operations. A robust third-party management program aims to assess, monitor, and manage these risks throughout the entire supply chain, ensuring that the organization understands and addresses potential security gaps introduced by its vendors.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 39,
    "question_text": "A company is rewriting a vulnerable application and adding the mprotect() system call in multiple parts of the application's code that was being leveraged by a recent exploitation tool. Which of the following should be enabled to ensure the application can leverage the new system call against similar attacks in the future?",
    "options": {
      "A": "TPM",
      "B": "Secure boot",
      "C": "NX bit",
      "D": "HSM"
    },
    "correct_answer": "C",
    "explanation": "The `mprotect()` system call is used to change memory protection settings (read, write, execute) for a region of memory. Its use in an application, especially in the context of preventing exploitation, often relates to enforcing memory safety. The **NX bit (No-Execute bit)**, also known as the XD bit (Execute Disable bit) in Intel CPUs, is a hardware-based security feature that marks certain areas of memory as non-executable. This prevents malicious code from running in non-executable memory regions, which is a common technique used in buffer overflow attacks and other memory corruption exploits.\n\nWhen `mprotect()` is used to mark memory pages as non-executable (e.g., data segments), the NX bit feature in the CPU enforces this protection at the hardware level. Therefore, enabling the NX bit ensures that the application can leverage the memory protection provided by `mprotect()` effectively against exploitation tools that attempt to execute code in data segments.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 40,
    "question_text": "Which of the following items should be included when crafting a disaster recovery plan?",
    "options": {
      "A": "Redundancy",
      "B": "Testing exercises",
      "C": "Autoscaling",
      "D": "Competitor locations"
    },
    "correct_answer": "B",
    "explanation": "A disaster recovery plan (DRP) outlines the procedures an organization will follow to recover and resume business operations after a disaster. While redundancy (A) is a *component* of disaster recovery infrastructure, and autoscaling (C) is a cloud capability related to resilience, **testing exercises (B)** are an essential *component* of the DRP itself. Regular testing (e.g., tabletop exercises, functional tests, full-scale simulations) ensures that the plan is viable, that personnel are trained, and that recovery objectives (RTO/RPO) can be met. Without testing, a DRP's effectiveness is unknown.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 41,
    "question_text": "A web application server is running a legacy operating system with an unpatched RCE vulnerability. The server cannot be upgraded until the corresponding application code is changed. Which of the following compensating controls would best prevent successful exploitation?",
    "options": {
      "A": "Segmentation",
      "B": "CASB",
      "C": "HIPS",
      "D": "UEBA"
    },
    "correct_answer": "A",
    "explanation": "Segmentation (specifically network segmentation) is the best compensating control in this scenario. If a legacy operating system with an unpatched Remote Code Execution (RCE) vulnerability cannot be upgraded immediately, limiting its network exposure is crucial. By segmenting the network, the vulnerable server can be isolated from other critical systems and from unnecessary user access. This means that even if the server is exploited, the attacker's ability to move laterally within the network and compromise other assets is significantly restricted. This control reduces the impact and likelihood of successful exploitation spreading.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 42,
    "question_text": "Which of the following is the reason why security engineers often cannot upgrade the security of embedded facility automation systems?",
    "options": {
      "A": "They are constrained by available compute.",
      "B": "They lack x86-64 processors.",
      "C": "They lack EEPROM.",
      "D": "They are not logic-bearing devices."
    },
    "correct_answer": "A",
    "explanation": "Embedded facility automation systems (Operational Technology - OT) often have significant constraints, making security upgrades challenging. One major constraint is their limited computational resources, or **available compute (A)**. These systems are often designed for specific, highly efficient tasks with minimal processing power, memory, and storage to reduce cost, power consumption, and physical size. This limited compute often means they cannot support modern security features, complex encryption, frequent updates, or robust endpoint security agents, which require more processing power than the embedded system was designed for.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 43,
    "question_text": "A security analyst identified a vulnerable and deprecated runtime engine that Is supporting a public-facing banking application. The developers anticipate the transition to modern development environments will take at least a month. Which of the following controls would best mitigate the risk without interrupting the service during the transition?",
    "options": {
      "A": "Shutting down the systems until the code is ready",
      "B": "Uninstalling the impacted runtime engine",
      "C": "Selectively blocking traffic on the affected port",
      "D": "Configuring IPS and WAF with signatures"
    },
    "correct_answer": "D",
    "explanation": "The key challenge is mitigating risk for a public-facing, vulnerable application without interrupting service during a month-long transition. \n\n**D. Configuring IPS and WAF with signatures:** An Intrusion Prevention System (IPS) and Web Application Firewall (WAF) can be deployed in front of the vulnerable application. They can be configured with signatures (or rules) to detect and block known exploits targeting the vulnerable runtime engine. This provides a virtual patch or protective layer, allowing the application to remain online while the underlying code is being remediated, thus mitigating risk without interrupting service.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 44,
    "question_text": "A security architect wants to ensure a remote host's identity and decides that pinning the X.509 certificate to the device is the most effective solution. Which of the following must happen first?",
    "options": {
      "A": "Use Distinguished Encoding Rules (DER) for the certificate.",
      "B": "Extract the private key from the certificate.",
      "C": "Use an out-of-band method to obtain the certificate.",
      "D": "Compare the retrieved certificate with the embedded certificate."
    },
    "correct_answer": "C",
    "explanation": "Certificate pinning involves hardcoding or 'pinning' the expected public key or certificate (or its hash) directly within an application or client. This ensures that when the client connects to a server, it only trusts that specific certificate or public key, even if a Certificate Authority (CA) issues a different, fraudulent certificate. To implement pinning effectively and securely, the client must first obtain the legitimate certificate's public key (or its hash) through a trusted **out-of-band method**. This prevents an attacker from supplying a malicious certificate during an initial connection attempt. Once the trusted certificate is obtained out-of-band, it can then be embedded into the application for future validation.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 45,
    "question_text": "A company hired a third-party consultant to run a cybersecurity incident simulation in order to identify security gaps and prepare stakeholders for a potential incident. Which of the following best describes this activity?",
    "options": {
      "A": "Tabletop exercise",
      "B": "Walk-through review",
      "C": "Lessons learned",
      "D": "Business impact analysis"
    },
    "correct_answer": "A",
    "explanation": "A **tabletop exercise** (A) is a discussion-based incident simulation that involves key stakeholders (e.g., management, IT, legal, communications) walking through a hypothetical cybersecurity incident scenario. Its purpose is to test the organization's incident response plan, identify security gaps, clarify roles and responsibilities, and prepare stakeholders for a potential real-world incident. This aligns perfectly with the description of the activity.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 46,
    "question_text": "A security officer is requiring all personnel working on a special project to obtain a security clearance requisite with the level of all information being accessed. Data on this network must be protected at the same level of each clearance holder. The need to know must be verified by the data owner. Which of the following should the security officer do to meet these requirements?",
    "options": {
      "A": "Create a rule to authorize personnel only from certain IPs to access the files.",
      "B": "Assign labels to the files and require formal access authorization.",
      "C": "Assign attributes to each file and allow authorized users to share the files.",
      "D": "Assign roles to users and authorize access to files based on the roles."
    },
    "correct_answer": "B",
    "explanation": "The requirements describe a need for a robust data classification and access control mechanism that aligns with security clearances and the 'need to know' principle:\n\n*   **Security clearance requisite with the level of all information being accessed:** This implies different levels of access based on a user's clearance.\n*   **Data protected at the same level of each clearance holder:** This means data needs to be classified and protected according to its sensitivity.\n*   **Need to know verified by the data owner:** This is a core principle where access is granted only when necessary for an individual to perform their job duties.\n\n**B. Assign labels to the files and require formal access authorization:** This approach directly implements a mandatory access control (MAC) model, often seen in environments requiring high security. Data is assigned a sensitivity label (e.g., Confidential, Secret, Top Secret), and users are assigned clearance levels. The system then enforces that users can only access data for which their clearance level meets or exceeds the data's sensitivity label, and a 'need to know' is explicitly granted by the data owner. This method is highly effective for enforcing strict access control based on data sensitivity and user clearance.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 47,
    "question_text": "A security team receives alerts regarding impossible travel and possible brute-force attacks after normal business hours. After reviewing more logs, the team determines that specific users were targeted and attempts were made to transfer data to an unknown site. Which of the following should the team do to help mitigate these issues?",
    "options": {
      "A": "Create a firewall rule to prevent those users from accessing sensitive data.",
      "B": "Restrict uploading activity to only authorized sites.",
      "C": "Enable packet captures to continue to run for the source and destination related to the file transfer.",
      "D": "Disable login activity for those users after business hours."
    },
    "correct_answer": "B",
    "explanation": "The alerts indicate possible compromised user accounts (impossible travel, brute-force attacks) and successful data transfer to an unknown site (data exfiltration). To mitigate these issues, especially the data exfiltration:\n\n**B. Restrict uploading activity to only authorized sites:** This is a direct control against data exfiltration. By implementing controls that limit where users or systems can upload data (e.g., only to sanctioned cloud storage, internal file servers, or approved business partners), it becomes significantly harder for attackers using compromised credentials to transfer sensitive data to their own external sites. This works alongside detecting impossible travel and brute-force attempts by limiting the ultimate impact of such compromises.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 48,
    "question_text": "A company recently acquired a SaaS company and performed a gap analysis. The results of the gap analysis Indicate security controls are absent throughout the SDLC and have led to several vulnerable production releases. Which of the following security tools best reduces the risk of vulnerable code being pushed to production in the future?",
    "options": {
      "A": "Static application security testing",
      "B": "Regression testing",
      "C": "Code signing",
      "D": "Sandboxing"
    },
    "correct_answer": "A",
    "explanation": "The core problem is that security controls are absent throughout the SDLC (Software Development Life Cycle), leading to vulnerable code in production. To address this proactively and reduce the risk of future vulnerable code, **Static Application Security Testing (SAST)** is the most effective tool. SAST analyzes application source code, bytecode, or binary code for security vulnerabilities without actually executing the application. It can be integrated early in the SDLC (e.g., during development or commit stages) to identify security flaws before the code even reaches testing or production, which is crucial for shifting security left.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 49,
    "question_text": "Which of the following is the best reason for obtaining file hashes from a confiscated laptop?",
    "options": {
      "A": "To prevent metadata tampering on each file",
      "B": "To later validate the integrity of each file",
      "C": "To generate unique identifiers for each file",
      "D": "To preserve the chain of custody of files"
    },
    "correct_answer": "B",
    "explanation": "When dealing with confiscated digital evidence like a laptop, maintaining forensic integrity is paramount. Obtaining file hashes (e.g., MD5, SHA-1, SHA-256) of all files (or an image of the drive) at the time of confiscation serves as a digital fingerprint. The best reason for doing this is **to later validate the integrity of each file (B)**. If there's any dispute or question about whether a file has been altered during analysis or storage, recalculating its hash and comparing it to the original hash proves (or disproves) its integrity. This is a fundamental step in digital forensics to ensure that the evidence remains untampered with and admissible.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 50,
    "question_text": "A security analyst is using data provided from a recent penetration test to calculate CVSS scores to prioritize remediation. Which of the following metric groups would the analyst need to determine to get the overall scores? (Choose three.)",
    "options": {
      "A": "Temporal",
      "B": "Availability",
      "C": "Integrity",
      "D": "Confidentiality",
      "E": "Base",
      "F": "Environmental",
      "G": "Impact",
      "H": "Attack vector"
    },
    "correct_answer": "A E F",
    "explanation": "The Common Vulnerability Scoring System (CVSS) uses three metric groups to calculate a vulnerability score:\n\n1.  **Base Metrics (E):** These represent the intrinsic characteristics of a vulnerability that are constant over time and across user environments. They include sub-metrics like Attack Vector, Attack Complexity, Privileges Required, User Interaction, Scope, Confidentiality Impact, Integrity Impact, and Availability Impact.\n2.  **Temporal Metrics (A):** These reflect the characteristics of a vulnerability that change over time, but not across user environments. They include Exploitability (e.g., existence of exploit code), Remediation Level (e.g., official fix available), and Report Confidence.\n3.  **Environmental Metrics (F):** These reflect the characteristics of a vulnerability that are relevant to a specific user's environment. They allow for customization of the CVSS score based on the importance of the affected IT asset within an organization. They include Security Requirements (Confidentiality, Integrity, Availability) and Modified Base Metrics.",
    "is_simulation": false,
    "question_type": "multiple_choice_multiple_answer"
  },
  {
    "question_number": 51,
    "question_text": "Which of the following describes how a risk assessment is performed when an organization has a critical vendor that provides multiple products?",
    "options": {
      "A": "At the individual product level",
      "B": "Through the selection of a random product",
      "C": "Using a third-party audit report",
      "D": "By choosing a major product"
    },
    "correct_answer": "A",
    "explanation": "When an organization has a critical vendor that provides multiple products, a comprehensive risk assessment should be performed at the **individual product level (A)**. Each product may have different architectures, security controls, data processing methods, and regulatory implications. Assessing at a high level (e.g., the vendor as a whole) or randomly selecting products (B) might miss specific risks associated with critical individual products. While third-party audit reports (C) are useful, they are often generalized and may not provide sufficient depth for critical products. Focusing only on a major product (D) would neglect risks in other critical products. Therefore, a granular assessment per product ensures all specific risks are identified and managed.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 52,
    "question_text": "A security engineer is performing a vulnerability management scan on multihomed Linux systems. The engineer notices that the vulnerability count is high due to the fact that each vulnerability is multiplied by the number of NICs on each system. Which of the following should the engineer do to deduplicate the vulnerabilities and to associate the vulnerabilities with a particular host?",
    "options": {
      "A": "Use a SCAP scanner.",
      "B": "Deploy an agent.",
      "C": "Initiate a discovery scan.",
      "D": "Perform an Nmap scan."
    },
    "correct_answer": "B",
    "explanation": "When vulnerability scanners identify hosts by IP address, multihomed systems (systems with multiple NICs and IP addresses) can appear as multiple distinct hosts, leading to duplicated vulnerabilities for the same physical or virtual machine. To deduplicate vulnerabilities and accurately associate them with a particular host, **deploying an agent (B)** on each Linux system is the most effective solution. An agent-based scanner runs directly on the host, identifying vulnerabilities based on the host's internal configuration, installed software, and system files, regardless of the number of IP addresses or NICs. This ensures a single, accurate view of vulnerabilities per host.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 53,
    "question_text": "Which of the following best describes a risk associated with using facial recognition to locally authenticate to a mobile device?",
    "options": {
      "A": "Data remanence",
      "B": "Deepfake",
      "C": "Metadata scraping",
      "D": "Biometric impersonation"
    },
    "correct_answer": "D",
    "explanation": "The primary risk associated with facial recognition for local authentication on a mobile device is **biometric impersonation (D)**. This refers to an attacker attempting to bypass the authentication by presenting a fake biometric (e.g., a photo, video, or 3D mask of the legitimate user's face) to the recognition system. While technologies like liveness detection aim to mitigate this, it remains a significant challenge. Other options like deepfake (B) are related to synthetic media but are more about creating convincing fake content rather than directly a risk *to authentication* via impersonation *of the biometric itself* (though deepfakes could be used to create the fake biometric). Data remanence (A) and metadata scraping (C) are general data security risks not specific to biometric authentication.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 54,
    "question_text": "The principal security analyst for a global manufacturer is investigating a security incident related to abnormal behavior in the ICS network. A controller was restarted as part of the troubleshooting process, and the following issue was identified when the controller was restarted:\nSECURE BOOT FAILED:\nFIRMWARE MISMATCH EXPECTED UXFDC479 ACTUAL 0x79F31B\nDuring the investigation, this modified firmware version was identified on several other controllers at the site. The official vendor firmware versions do not have this checksum. Which of the following stages of the MITRE ATT&CK framework for ICS includes this technique?",
    "options": {
      "A": "Evasion",
      "B": "Persistence",
      "C": "Collection",
      "D": "Lateral movement"
    },
    "correct_answer": "B",
    "explanation": "The key phrase here is \"modified firmware version was identified on several other controllers at the site. The official vendor firmware versions do not have this checksum.\" This indicates that an attacker has altered the firmware on the controllers. When an attacker modifies system firmware, it is typically to establish a durable presence on the compromised system that can survive reboots, system resets, or even OS re-installations. This tactic aligns directly with the **Persistence** tactic in the MITRE ATT&CK framework for ICS. Persistence aims to maintain access to a system or network over time, even after restarts or credential changes. Modifying firmware is a highly stealthy and persistent method.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 55,
    "question_text": "A web service provider has just taken on a very large contract that comes with requirements that are currently not being implemented. In order to meet contractual requirements, the company must achieve the following thresholds:\n99.99% uptime\nLoad time in 3 seconds -\nResponse time = <1.0 seconds -\nStarting with the computing environment, which of the following should a security engineer recommend to best meet the requirements? (Choose three.)",
    "options": {
      "A": "Installing a firewall at corporate headquarters",
      "B": "Deploying a content delivery network",
      "C": "Implementing server clusters",
      "D": "Employing bare-metal loading of applications",
      "E": "Lowering storage input/output",
      "F": "Implementing RAID on the backup servers",
      "G": "Utilizing redundant power for all developer workstations"
    },
    "correct_answer": "B C E",
    "explanation": "The requirements are high uptime (99.99%), low load time (<3s), and low response time (<1.0s). These are all related to application performance and availability. Starting with the computing environment:\n\n1.  **B. Deploying a content delivery network (CDN):** CDNs cache static and dynamic content closer to end-users geographically. This significantly reduces load times and response times for users around the world, directly addressing the speed requirements for a global service.\n2.  **C. Implementing server clusters:** Server clustering involves grouping multiple servers to work together as a single system. This provides high availability (contributing to 99.99% uptime) and load balancing, distributing requests across multiple servers to improve response times and handle increased traffic efficiently.\n3.  **E. Lowering storage input/output (I/O):** Poor storage I/O performance can be a major bottleneck for applications, leading to slow load and response times, especially for database-intensive applications or those serving large files. Optimizing storage (e.g., using faster storage like SSDs, optimizing database queries, or employing read/write caching) directly improves how quickly data can be accessed and processed, thus enhancing application performance and meeting speed requirements.",
    "is_simulation": false,
    "question_type": "multiple_choice_multiple_answer"
  },
  {
    "question_number": 56,
    "question_text": "An analyst is working to address a potential compromise of a corporate endpoint and discovers the attacker accessed a user's credentials. However, it is unclear if the system baseline was modified to achieve persistence. Which of the following would most likely support forensic activities in this scenario?",
    "options": {
      "A": "Side-channel analysis",
      "B": "Bit-level disk duplication",
      "C": "Software composition analysis",
      "D": "SCAP scanner"
    },
    "correct_answer": "B",
    "explanation": "To determine if a system baseline was modified (indicating persistence or other malicious changes), a forensic analyst needs an exact, immutable copy of the compromised system's storage. **Bit-level disk duplication (B)**, also known as forensic imaging or bit-stream imaging, creates an exact sector-by-sector copy of a storage device. This ensures that every bit of data, including hidden files, deleted files, and slack space, is preserved. This forensic image can then be analyzed offline without altering the original evidence, allowing the analyst to compare it against a known good baseline (if available) or search for any unauthorized modifications, hidden files, or persistence mechanisms the attacker might have implemented.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 57,
    "question_text": "A company is decommissioning old servers and hard drives that contain sensitive data. Which of the following best protects against data leakage?",
    "options": {
      "A": "Purging",
      "B": "Clearing",
      "C": "Shredding",
      "D": "Degaussing"
    },
    "correct_answer": "C",
    "explanation": "To best protect against data leakage from hard drives containing sensitive data during decommissioning, the most secure method is **shredding (C)**. Shredding physically destroys the hard drive, rendering the data unrecoverable. While degaussing (D) can magnetically erase data, and purging/clearing (A, B) involve software-based overwriting, physical destruction (shredding) offers the highest level of assurance against data recovery, especially for highly sensitive information. It completely eliminates the risk of data leakage from the media.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 58,
    "question_text": "An engineer has had scaling issues with a web application hosted on premises and would like to move to a serverless architecture. Which of the following cloud benefits would be best to utilize for this project?",
    "options": {
      "A": "Cost savings for hosting",
      "B": "Automation of resource provisioning",
      "C": "Providing geo-redundant hosting",
      "D": "Eliminating need to patch"
    },
    "correct_answer": "B",
    "explanation": "The core problem is 'scaling issues' with an on-premises web application, and the proposed solution is moving to a serverless architecture in the cloud. One of the most significant benefits of serverless computing that directly addresses scaling issues is the **automation of resource provisioning (B)**. In a serverless model (e.g., AWS Lambda, Azure Functions), the cloud provider automatically provisions and scales the underlying compute resources (servers, containers) in response to demand, without requiring the engineer to manually manage or provision them. This eliminates the operational burden of dealing with scaling and ensures that the application can handle fluctuating loads seamlessly.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 59,
    "question_text": "An organization needs to classify its systems and data in accordance with external requirements. Which of the following roles is best qualified to perform this task?",
    "options": {
      "A": "Systems administrator",
      "B": "Data owner",
      "C": "Data processor",
      "D": "Data custodian",
      "E": "Data steward"
    },
    "correct_answer": "B",
    "explanation": "The **Data Owner (B)** is typically the individual or department responsible for the business value and content of the data, and for deciding its classification (e.g., confidential, public) and the appropriate security controls. While other roles like data custodian (C) manage and protect the data, and data steward (E) handle data quality and metadata, the ultimate responsibility for data classification according to its business value and external requirements rests with the data owner.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 60,
    "question_text": "A company is developing an application that will be used to perform e-commerce transactions for a subscription-based service. The application must be able to use previously saved payment methods to perform recurring transactions. Which of the following is the most appropriate?",
    "options": {
      "A": "Tokenization through an HSM",
      "B": "Self-encrypting disks with field-level encryption",
      "C": "NX/XN Implementation to minimize data retention",
      "D": "Token-based access for application users",
      "E": "Address space layout randomization"
    },
    "correct_answer": "A",
    "explanation": "For e-commerce transactions involving saved payment methods and recurring transactions, the company needs a secure way to store sensitive cardholder data without directly holding it, to reduce PCI DSS scope and minimize risk. **Tokenization through an HSM (Hardware Security Module) (A)** is the most appropriate and secure solution. \n\n*   **Tokenization:** Replaces sensitive payment data (e.g., credit card number) with a unique, non-sensitive substitute called a token. This token can be used for recurring transactions without exposing the actual card number.\n*   **HSM:** An HSM is a physical computing device that safeguards and manages digital keys for strong authentication and provides cryptoprocessing. Using an HSM ensures that the tokenization process (generation, storage, and de-tokenization of payment data) is performed within a highly secure, tamper-resistant hardware boundary, protecting the sensitive payment data at its most vulnerable points.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 61,
    "question_text": "A security technician is trying to connect a remote site to the central office over a site-to-site VPN. The technician has verified the source and destination IP addresses are correct, but the technician is unable to get the remote site to connect. The following error message keeps repeating:\nAn error has occurred during Phase 1 handshake. Deleting keys and retrying...\nWhich of the following is most likely the reason the connection is failing?",
    "options": {
      "A": "The IKE hashing algorithm uses different key lengths on each VPN device.",
      "B": "The IPSec settings allow more than one cipher suite on both devices.",
      "C": "The Die-Hellman group on both sides matches but is a legacy group.",
      "D": "The remote VPN is attempting to connect with a protocol other than SSL/TLS."
    },
    "correct_answer": "A",
    "explanation": "The error message \"An error has occurred during Phase 1 handshake. Deleting keys and retrying...\" is a strong indicator of a mismatch in the Phase 1 IKE (Internet Key Exchange) parameters between the two VPN peers. Phase 1 (also known as Main Mode) of IKE is responsible for establishing a secure channel between the two VPN endpoints and negotiating security parameters. These parameters include: hashing algorithms (e.g., MD5, SHA), encryption algorithms (e.g., AES, 3DES), authentication methods (e.g., pre-shared key, certificates), and Diffie-Hellman groups. If any of these parameters do not match on both sides, the Phase 1 handshake will fail. While option A mentions key lengths, which is a common mismatch, option B talks about cipher suites (which are a collection of algorithms) but states it allows more than one. Option C points to a legacy Diffie-Hellman group that matches but doesn't explain the failure. Option D is about protocol type mismatch, but the core issue is the Phase 1 handshake parameters. The most precise answer indicating a common cause for a Phase 1 failure when both sides are trying to connect and repeatedly failing on the handshake is a mismatch in the cryptographic parameters, such as key lengths or algorithms, as suggested by 'A'.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 62,
    "question_text": "A security analyst received the following finding from a cloud security assessment tool:\nVirtual Machine Data Disk is encrypted with the default encryption key.\nBecause the organization hosts highly sensitive data files, regulations dictate it must be encrypted so It is unreadable to the CSP. Which of the following should be implemented to remediate the finding and meet the regulatory requirement? (Choose two.)",
    "options": {
      "A": "Disk encryption with customer-provided keys",
      "B": "Disk encryption with keys from a third party",
      "C": "Row-level encryption with a key escrow",
      "D": "File-level encryption with cloud vendor-provided keys",
      "E": "File-level encryption with customer-provided keys",
      "F": "Disk-level encryption with a cross-signed certificate"
    },
    "correct_answer": "A E",
    "explanation": "The core requirement is that sensitive data must be encrypted in a way that makes it 'unreadable to the CSP'. This implies that the encryption keys must be controlled by the customer, not the cloud provider's default key management system.\n\n1.  **A. Disk encryption with customer-provided keys (Customer-Managed Keys/CMK):** This option allows the customer to provide and manage their own encryption keys for disk encryption. The cloud provider uses these keys to encrypt/decrypt the disk, but they do not have direct access to the keys themselves, ensuring the data remains unreadable to the CSP.\n2.  **E. File-level encryption with customer-provided keys:** Similar to disk encryption, but applied at the file level. If the organization needs granular control over specific sensitive files, encrypting them with customer-provided keys ensures that only the customer can decrypt and read the content, again making it unreadable to the CSP. This is a common and robust solution when data confidentiality from the cloud provider is a regulatory requirement.",
    "is_simulation": false,
    "question_type": "multiple_choice_multiple_answer"
  },
  {
    "question_number": 63,
    "question_text": "A security analyst discovers a new device on the company's dedicated IoT subnet during the most recent vulnerability scan. The scan results show numerous open ports and insecure protocols in addition to default usernames and passwords. A camera needs to transmit video to the security server in the IoT subnet. Which of the following should the security analyst recommend to securely operate the camera?",
    "options": {
      "A": "Harden the camera configuration.",
      "B": "Send camera logs to the SIEM.",
      "C": "Encrypt the camera's video stream.",
      "D": "Place the camera on an isolated segment."
    },
    "correct_answer": "A",
    "explanation": "The vulnerability scan results for the new IoT camera explicitly state \"numerous open ports and insecure protocols in addition to default usernames and passwords.\" The most immediate and fundamental step to secure the camera and mitigate these identified vulnerabilities is to **harden its configuration (A)**. This involves:\n\n*   Closing unnecessary open ports.\n*   Disabling insecure protocols.\n*   Changing default usernames and passwords to strong, unique credentials.\n\nWhile isolating the segment (D) is a good network control, encrypting the video stream (C) is good for confidentiality, and sending logs to SIEM (B) is good for monitoring, none of these directly address the fundamental configuration weaknesses found on the device itself. Hardening the device's configuration is the first and most direct remediation for the issues identified.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 64,
    "question_text": "The Chief Information Security Officer of a large multinational organization has asked the security risk manager to use risk scenarios during a risk analysis. Which of the following is the most likely reason for this approach?",
    "options": {
      "A": "To connect risks to business objectives",
      "B": "To ensure a consistent approach to risk",
      "C": "To present a comprehensive view of risk",
      "D": "To provide context to the relevancy of risk"
    },
    "correct_answer": "D",
    "explanation": "Risk scenarios describe specific events or sequences of events that could lead to an undesirable outcome (a risk). When a CISO asks for risk scenarios during a risk analysis, the most likely reason is **to provide context to the relevancy of risk (D)**. Scenarios help to translate abstract risks (e.g., 'data breach') into concrete, relatable narratives (e.g., 'an attacker compromises system X, steals data Y, leading to Z impact'). This contextualization makes the risks more understandable to various stakeholders, including the board and business leaders who may not be cybersecurity experts, allowing them to better grasp the potential impact and relevancy to business operations, and thus make informed decisions about risk management investments.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 65,
    "question_text": "A security engineer would like to control configurations on mobile devices while fulfilling the following requirements:\nSupport and control Apple and Android devices.\nThe device must be corporate-owned.\nWhich of the following would enable the engineer to meet these requirements? (Choose two.)",
    "options": {
      "A": "Create a group policy to lock down mobile devices.",
      "B": "Update verbiage in the acceptable use policy for the internet.",
      "C": "Implement an MDM solution.",
      "D": "Implement a captive portal solution.",
      "E": "Update policy to prohibit the use of BYOD devices.",
      "F": "Implement a RADIUS solution."
    },
    "correct_answer": "C E",
    "explanation": "The requirements are to control configurations on *corporate-owned* Apple and Android devices.\n\n1.  **C. Implement an MDM solution (Mobile Device Management):** An MDM solution is specifically designed to manage and control configurations on mobile devices, including both Apple (iOS/iPadOS) and Android. It allows organizations to enforce policies, push configurations, manage applications, and secure corporate data on these devices. This directly fulfills the core need to 'control configurations' and 'support and control Apple and Android devices'.\n2.  **E. Update policy to prohibit the use of BYOD devices:** Since the requirement specifies that 'The device must be corporate-owned,' an administrative policy prohibiting Bring Your Own Device (BYOD) usage ensures that only devices under full corporate control (and thus manageable by the MDM) are used for corporate purposes. This is a foundational policy decision that supports the technical implementation.",
    "is_simulation": false,
    "question_type": "multiple_choice_multiple_answer"
  },
  {
    "question_number": 66,
    "question_text": "A pharmaceutical company uses a cloud provider to host thousands of independent resources in object storage. The company needs a practical and effective means of discovering data, monitoring changes, and identifying suspicious activity. Which of the following would best meet these requirements?",
    "options": {
      "A": "A machine-learning-based data security service",
      "B": "A file integrity monitoring service",
      "C": "A cloud configuration assessment and compliance service",
      "D": "An automated data classification system"
    },
    "correct_answer": "A",
    "explanation": "For discovering data, monitoring changes, and identifying suspicious activity in thousands of independent resources in object storage, especially for a pharmaceutical company (which likely handles sensitive research/patient data), a **machine-learning-based data security service (A)** is the most comprehensive and effective solution. These services leverage AI/ML to:\n\n*   **Discover data:** Automatically identify and categorize sensitive data (e.g., PII, PHI, research data) stored across various objects.\n*   **Monitor changes:** Establish baselines for normal data access and modification patterns and detect anomalous changes.\n*   **Identify suspicious activity:** Use behavioral analytics to flag unusual access patterns, exfiltration attempts, or unauthorized data movements that a rule-based system might miss. This scales well with \"thousands of independent resources.\"\n\nWhile other options like FIM (B) and automated data classification (D) contribute to data security, a machine-learning-based service integrates these capabilities with advanced anomaly detection suitable for large-scale, dynamic cloud environments.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 67,
    "question_text": "A security analyst is assessing a new application written in Java. The security analyst must determine which vulnerabilities exist during runtime. Which of the following would provide the most exhaustive list of vulnerabilities while meeting the objective?",
    "options": {
      "A": "Input validation",
      "B": "Dynamic analysis",
      "C": "Side-channel analysis",
      "D": "Fuzz testing",
      "E": "Static analysis"
    },
    "correct_answer": "B",
    "explanation": "The key phrase in the question is \"determine which vulnerabilities exist during runtime.\" \n\n**Dynamic Application Security Testing (DAST)** performs a runtime analysis by executing the application and attacking it from the outside, much like a malicious hacker would. It simulates attacks against the running application to identify vulnerabilities such as injection flaws, broken authentication, cross-site scripting, and security misconfigurations. DAST can find vulnerabilities that only manifest during the execution of the application and provides an attacker's view of the application's security posture. While fuzz testing (D) is a type of dynamic testing, DAST is a broader category focused on identifying vulnerabilities in running web applications.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 68,
    "question_text": "Recently, two large engineering companies in the same line of business decided to approach cyberthreats in a united way. Which of the following best describes this unified approach?",
    "options": {
      "A": "NDA",
      "B": "SOW",
      "C": "SLA",
      "D": "MOU"
    },
    "correct_answer": "D",
    "explanation": "A **Memorandum of Understanding (MOU) (D)** is a formal agreement between two or more parties that outlines the terms and details of a mutual understanding or common line of action. It's often used in scenarios where parties agree to cooperate on a specific objective, such as sharing threat intelligence or coordinating on cybersecurity defenses, without creating a legally binding contract (like an SLA or SOW) or disclosing confidential information broadly (like an NDA, which focuses on secrecy rather than unified action). This best describes a 'unified approach' for two companies to collaborate on cyberthreats.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 69,
    "question_text": "A regulated company is in the process of refreshing its entire infrastructure. The company has a business-critical process running on an old 2008 Windows server. If this server fails, the company would lose millions of dollars in revenue. Which of the following actions should the company should take?",
    "options": {
      "A": "Accept the risk as the cost of doing business.",
      "B": "Create an organizational risk register for project prioritization.",
      "C": "Implement network compensating controls.",
      "D": "Purchase insurance to offset the cost if a failure occurred."
    },
    "correct_answer": "B",
    "explanation": "The company is refreshing its infrastructure, and there's a business-critical process on an old, vulnerable server that, if it fails, leads to significant financial loss. This represents a high-impact, high-risk situation that needs to be addressed strategically during the infrastructure refresh. \n\n**B. Create an organizational risk register for project prioritization:** This is the most appropriate *first* action. A risk register formally documents identified risks, their potential impact, likelihood, and existing or planned mitigation strategies. By adding this critical server's risk (and others discovered during the refresh) to the risk register, the company can then formally assess and prioritize this risk alongside other organizational risks, allocate resources, and schedule its remediation or migration within the broader infrastructure refresh project. This ensures that the problem is not ignored and is addressed systematically according to its business criticality.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 70,
    "question_text": "A security engineer needs to ensure production containers are automatically scanned for vulnerabilities before they are accepted into the production environment. Which of the following should the engineer use to automatically incorporate vulnerability scanning on every commit?",
    "options": {
      "A": "Code repository",
      "B": "CI/CD pipeline",
      "C": "Integrated development environment",
      "D": "Container orchestrator"
    },
    "correct_answer": "B",
    "explanation": "To automatically incorporate vulnerability scanning on every commit for production containers, the security engineer should use a **CI/CD (Continuous Integration/Continuous Delivery) pipeline (B)**. CI/CD pipelines automate the build, test, and deployment processes. Security scanning tools (like static analysis, dynamic analysis, or container vulnerability scanners) can be integrated as automated gates or stages within the CI/CD pipeline. This ensures that every time code is committed and a new container image is built, it undergoes a security scan before it can proceed to later stages or be accepted into the production environment, effectively shifting security left.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 71,
    "question_text": "A security architect recommends replacing the company's monolithic software application with a containerized solution. Historically, secrets have been stored in the application's configuration files. Which of the following changes should the security architect make in the new system?",
    "options": {
      "A": "Use a secrets management tool.",
      "B": "Save secrets in key escrow.",
      "C": "Store the secrets inside the Dockerfiles.",
      "D": "Run all Dockerfiles in a randomized namespace."
    },
    "correct_answer": "A",
    "explanation": "Storing secrets (like API keys, database credentials, or sensitive configuration values) directly in application configuration files or Dockerfiles (C) is a major security vulnerability, especially in containerized environments where images might be shared or stored in repositories. The best practice for managing secrets in modern application architectures, especially with containers, is to **use a secrets management tool (A)**. Tools like HashiCorp Vault, AWS Secrets Manager, Azure Key Vault, or Kubernetes Secrets (when properly secured) allow for centralized, encrypted storage and dynamic retrieval of secrets, reducing the risk of hardcoded credentials and improving secret rotation and access control.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 72,
    "question_text": "A security engineer is assessing a new tool to segment data and communications between domains. The assessment must determine how data transmission controls can be bypassed without detection. Which of the following techniques should the security engineer use?",
    "options": {
      "A": "Machine-learning statistical analysis",
      "B": "Fuzz testing",
      "C": "Covert channel analysis",
      "D": "Protocol analysis"
    },
    "correct_answer": "C",
    "explanation": "The objective is to determine how \"data transmission controls can be bypassed without detection\" within a new tool designed to \"segment data and communications between domains.\" This directly points to the concept of **covert channels (C)**. A covert channel is a communication path that is not intended for communication and is typically hidden within a legitimate communication channel or system resource. Attackers use covert channels to exfiltrate data or establish command and control in a stealthy manner, bypassing standard security controls. A security engineer would perform covert channel analysis to identify if the new segmentation tool could inadvertently allow data to be transmitted in an unauthorized and undetectable way (e.g., through timing, storage, or protocol manipulation).",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 73,
    "question_text": "During an adversarial simulation exercise, an external team was able to gain access to sensitive information and systems without the organization detecting this activity. Which of the following mitigation strategies should the organization use to best resolve the findings?",
    "options": {
      "A": "Configuring a honeypot for adversary characterization",
      "B": "Leveraging simulators for attackers",
      "C": "Setting up a honey network for attackers",
      "D": "Utilizing decoy accounts and documents"
    },
    "correct_answer": "D",
    "explanation": "The core problem is that the organization failed to detect the adversarial activity. To improve detection capabilities, especially against stealthy attackers who might gain access, **utilizing decoy accounts and documents (D)** is an effective strategy. These are fake, tempting assets (e.g., dormant user accounts, fictitious sensitive documents, fake network shares) strategically placed within the network. Since these decoys should never be accessed by legitimate users or systems, any interaction with them generates high-fidelity alerts, immediately indicating malicious activity or unauthorized access attempts. This helps detect attackers who have bypassed initial defenses and are performing reconnaissance or lateral movement, addressing the lack of detection during the simulation.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 74,
    "question_text": "A help desk technician is troubleshooting an issue with an employee's laptop that will not boot into its operating system. The employee reported the laptop had been stolen but then found it one day later. The employee has asked the technician for help recovering important data. The technician has identified the following:\nThe laptop operating system was not configured with BitLocker.\nThe hard drive has no hardware failures.\nData is present and readable on the hard drive, although it appears to be illegible.\nWhich of the following is the most likely reason the technician is unable to retrieve legible data from the hard drive?",
    "options": {
      "A": "The employee's password was changed, and the new password needs to be used.",
      "B": "The PKI certificate was revoked, and a new one must be installed.",
      "C": "The hard drive experienced crypto-shredding.",
      "D": "The technician is using the incorrect cipher to read the data."
    },
    "correct_answer": "C",
    "explanation": "The key information is that the laptop was stolen and later found, BitLocker was *not* configured, yet the data is present and *illegible* but readable. This scenario strongly suggests **crypto-shredding (C)**. Crypto-shredding is a data sanitization method where the encryption key used to encrypt the data is deliberately destroyed. Even if the data itself remains on the drive, without the key, it becomes permanently irrecoverable and illegible. This can happen if a remote wipe command was issued to a system that utilizes always-on encryption (even without BitLocker, many modern SSDs have built-in encryption that can be controlled by the OS or enterprise management tools) and the key was securely erased. Since BitLocker wasn't active, it rules out a simple password change. Incorrect cipher (D) is less likely if the data is just 'illegible' rather than corrupt, and PKI revocation (B) relates to certificates for authentication/encryption of communication, not local disk data.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 75,
    "question_text": "This system was recently patched following the exploitation of a vulnerability by an attacker to enable data exfiltration.\nDespite the vulnerability being patched, it is likely that a malicious TCP service is still running and the adversary has achieved persistence by creating a systemd service.\nExamples of commands to use:\nkill, killall\nlsof\nman, --help (use for assistance)\nnetstat (useful flags: a, n, g, u)\nps (useful flag: a)\nsystemctl (to control systemd)\nPlease note: the list of commands shown above is not exhaustive. All native commands are available.\nUsing the following credentials:\n• Username: labadmin\n• Password: Passw0rd!\nInvestigate to identify indicators of compromise and then remediate them. You will need to make at least two changes:\n1. End the compromised process that is using a malicious TCP service.\n2. Remove the malicious persistence agent by disabling the service's ability to start on boot.",
    "options": {},
    "correct_answer": null,
    "explanation": "STEP 1: Identify and Kill the Malicious TCP Process\n1. List listening TCP services with associated processes:\nsudo netstat -tlnp\nLook for suspicious services (e.g., uncommon ports like 4444, 1337, etc.)\n2. Verify the process details:\nps aux | grep <PID>\n3. Terminate the suspicious process:\nsudo kill <PID>\nOr use sudo killall <process-name> if needed.\nSTEP 2: Disable the Malicious systemd Service (Persistence)\n1. List all systemd services:\nsystemctl list-units --type=service\n2. Look for any unusual or suspicious service (often not part of a typical system, e.g., revshell.service, malicious.service, backdoor.service, etc.)\n3. Check the service path and content (optional but helpful):\nsystemctl status <suspicious-service>\ncat /etc/systemd/system/<suspicious-service>.service\n4. Disable the malicious service:\nsudo systemctl disable <suspicious-service>\n5. Stop the service (if still running):\nsudo systemctl stop <suspicious-service>\nOnce these steps are done you will have:\n• Ended the malicious process.\n• Disabled its persistence via systemd.\nAvoid: Disabling SSH or systemd, changing passwords, rebooting, or touching the\n172.162.0.0 network adapter.",
    "is_simulation": true,
    "question_type": "simulation"
  },
  {
    "question_number": 76,
    "question_text": "You are tasked with integrating a new B2B client application with an existing OAuth workflow that must meet the following requirements:\n• The application does not need to know the users’ credentials.\n• An approval interaction between the users and the HTTP service must be orchestrated.\n• The application must have limited access to users’ data.\nUse the drop-down menus to select the action items for the appropriate locations. All placeholders must be filled.\nIf at any time you would like to bring back the initial state of the simulation, please click the Reset All button.",
    "options": {},
    "correct_answer": null,
    "explanation": "This is a simulation question. The correct response involves interacting with a graphical user interface to select appropriate OAuth workflow steps (e.g., Authorize access to other applications, Access issued tokens, Grant access) for different stages of the process (Resource owner, Authorization server, Resource server, B2B client application). Since this requires a visual interaction, a text-based explanation is not feasible. The diagram illustrates the flow of actions and components in an OAuth workflow.",
    "is_simulation": true,
    "question_type": "simulation"
  },
  {
    "question_number": 77,
    "question_text": "An IPSec solution is being deployed. The configuration files for both the VPN concentrator and the AAA server are shown in the diagram.\nComplete the configuration files to meet the following requirements:\n• The EAP method must use mutual certificate-based authentication (with issued client certificates).\n• The IKEv2 cipher suite must be configured to the most secure authenticated mode of operation.\n• The secret must contain at least one uppercase character, one lowercase character, one numeric character, and one special character, and it must meet a minimum length requirement of eight characters.\nClick on the AAA server and VPN concentrator to complete the configuration. Fill in the appropriate fields and make selections from the drop-down menus.\nIf at any time you would like to bring back the initial state of the simulation, please click the Reset All button.",
    "options": {},
    "correct_answer": null,
    "explanation": "This is a simulation question. The correct response involves interacting with the graphical interface of the VPN concentrator and AAA server configuration files to select specific values for `proposals`, `secret`, `default_eap_type`, and `ip_addr` fields, ensuring they meet the given cryptographic and password complexity requirements. \n\nFor VPN Concentrator:\n- `proposals`: Select `aes256gcm128` (most secure IKEv2 cipher suite).\n- `server`: Enter the IP address of the AAA server (10.1.0.10).\n- `secret`: Enter a strong password like `Str0ng@Key` (meets complexity and length requirements).\n\nFor AAA Server:\n- `default_eap_type`: Select `tls` (for mutual certificate-based authentication).\n- `ip_addr`: Enter the IP address of the VPN concentrator (10.1.2.1).\n- `secret`: Enter the same strong password `Str0ng@Key` as used on the VPN concentrator (secrets must match).",
    "is_simulation": true,
    "question_type": "simulation"
  },
  {
    "question_number": 78,
    "question_text": "An incident occurred at Site A when an attacker successfully caused water pressure to increase in the pump room.\nThe organization is concerned about reoccurrence of this attack and that similar attacks might be successful on other cyber-physical systems within the network.\nAll devices and components reside on a flat network within the 10.1.0.0/16 space.\nTake the appropriate actions to reduce the risk of reoccurrence of this and other environmental security vulnerabilities.\nSelect the component(s) at Sites A and B that have environmental impact potential. Then, select the corrective action that will best reduce the risk of incident reoccurrence.\nIf at any time you would like to bring back the initial state of the simulation, please click the Reset All button.",
    "options": {},
    "correct_answer": null,
    "explanation": "This is a simulation question. The correct response involves identifying specific components on the network diagrams for Site A and Site B that have environmental impact potential (e.g., SCADA master controller, PLC, Pumps) and then selecting the most effective corrective action. Given the issue of increased water pressure and a flat network, the best action is to implement network isolation (segmentation or VLANs) for these critical cyber-physical systems (CPS) to prevent lateral movement and reduce the risk of reoccurrence.\n\nCorrect mapping would involve selecting:\n- **Components with environmental impact potential:** SCADA master controller, PLC, Pumps (from both Site A and Site B).\n- **Corrective action:** Isolate from the network (e.g., using microsegmentation or VLANs).",
    "is_simulation": true,
    "question_type": "simulation"
  },
  {
    "question_number": 79,
    "question_text": "During the course of normal SOC operations, three anomalous events occurred and were flagged as potential IoCs. Evidence for each of these potential IoCs is provided.\nReview each of the events and select the appropriate analysis and action options for each IoC.\nIf at any time you would like to bring back the initial state of the simulation, please click the Reset All button.",
    "options": {},
    "correct_answer": null,
    "explanation": "This is a simulation question. The correct response involves reviewing three distinct IoC events and selecting the appropriate analysis and action for each. The exact options for analysis and action will be provided in a dropdown menu within the simulation, but based on the provided log snippets in the question description (which were not fully provided in the text-only output), here's the general approach:\n\n**IoC 1 (DNS traffic):**\n- **Indicators:** DNS queries for suspicious subdomains (e.g., `update.s.domain`, `*.s.domain`), responses with odd CNAME and A records. Activity resembling contact with a malicious domain.\n- **Analysis:** Someone is footprinting a network subnet or the service is attempting to resolve a malicious domain.\n- **Action:** Configure the DNS server to prevent recursion, Implement a blocklist for known malicious ports, or block DNS requests across the WAN interface (depending on specific findings).\n\n**IoC 2 (ICMP traffic):**\n- **Indicators:** ICMP Echo (ping) requests from an internal IP (e.g., 10.0.5.5) to multiple internal hosts. All packets dropped. Suggests a device is probing the network.\n- **Analysis:** Someone is footprinting a network subnet.\n- **Action:** Block ping requests across the WAN interface.\n\n**IoC 3 (Application/Bittorrent traffic):**\n- **Indicators:** Presence of BitTorrent traffic (e.g., `//announce?info_hash`, `peer_id`, `application/x-bittorrent`). Indicates P2P protocol activity.\n- **Analysis:** An employee is using P2P services to download files.\n- **Action:** Enforce endpoint controls on third-party software installations, or deploy a network-based DLP solution.",
    "is_simulation": true,
    "question_type": "simulation"
  },
  {
    "question_number": 80,
    "question_text": "A security administrator needs to automate alerting. The server generates structured log files that need to be parsed to determine whether an alarm has been triggered. Given the following code function:\ndef parse_logs(logfile):\n  with open(logfile) as log_file:\n    for line in log_file:\n      parsed_log = json.loads(line)\n      if parsed_log[\"error_log\"]:[\"system_1\"][\"InAlarmState\"]:\n        print(\"System 1 is in alarm state!\")\nWhich of the following is most likely the log input that the code will parse?",
    "options": {
      "A": "{\"error_log\":\n   {\"system_1\":\n   {\"InAlarmState\": True\n   }",
      "B": "<\"error_log\"><\"system_1\">",
      "C": "{\"error_log\":\n   \"system_1\":\n   \"InAlarmState\": True\n   }",
      "D": "<\"error_log\": {\"system_1\": {\"InAlarmState\": True }}}"
    },
    "correct_answer": "A",
    "explanation": "The Python code snippet uses `json.loads(line)` to parse each line of the log file. This indicates that the log file contains JSON (JavaScript Object Notation) formatted data. The code then attempts to access `parsed_log[\"error_log\"][\"system_1\"][\"InAlarmState\"]`. This means the JSON structure must have a top-level key named `\"error_log\"`, under which there's an object with a key `\"system_1\"`, and within that, an object with a key `\"InAlarmState\"` that holds a boolean value. Option A is the only one that presents a valid JSON structure that matches this nested access pattern and uses the boolean value `True` as indicated by the conditional check.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 81,
    "question_text": "A financial technology firm works collaboratively with business partners in the industry to share threat intelligence within a central platform. This collaboration gives partner organizations the ability to obtain and share data associated with emerging threats from a variety of adversaries.\nWhich of the following should the organization most likely leverage to facilitate this activity?\n(Choose two.)",
    "options": {
      "A": "CWPP",
      "B": "YARA",
      "C": "ATT&CK",
      "D": "STIX",
      "E": "TAXII",
      "F": "JTAG"
    },
    "correct_answer": "D E",
    "explanation": "The scenario describes a need for collaborative threat intelligence sharing within an industry. This points to standardized formats and protocols for exchanging threat information.\n\n1.  **D. STIX (Structured Threat Information Expression):** STIX is a standardized, structured language for describing cyber threat information. It enables organizations to share threat intelligence in a consistent, machine-readable format.\n2.  **E. TAXII (Trusted Automated Exchange of Intelligence Information):** TAXII is the secure, automated transport mechanism for sharing STIX-formatted cyber threat information. It defines an API (Application Programming Interface) that allows organizations to exchange STIX data over HTTPS. STIX and TAXII are often used together to facilitate automated, standardized threat intelligence sharing.",
    "is_simulation": false,
    "question_type": "multiple_choice_multiple_answer"
  },
  {
    "question_number": 82,
    "question_text": "A company wants to invest in research capabilities with the goal to operationalize the research output. Which of the following is the best option for a security architect to recommend?",
    "options": {
      "A": "Dark web monitoring",
      "B": "Threat intelligence platform",
      "C": "Honeypots",
      "D": "Continuous adversary emulation"
    },
    "correct_answer": "B",
    "explanation": "To \"operationalize research output\" in security, a company needs a system to collect, process, analyze, and integrate threat information into security operations. A **Threat Intelligence Platform (TIP) (B)** is designed precisely for this purpose. A TIP aggregates threat data from various sources (including research output), enriches it, correlates it, and enables its consumption by other security tools (like SIEM, SOAR, EDR, firewalls) and security teams. This allows the company to turn raw threat data into actionable intelligence that can be used to improve defenses and detect threats effectively.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 83,
    "question_text": "A company is concerned about the security of customer data. The IT department has configured all web applications with appropriate access controls to restrict to only authorized users. Which of the following solutions addresses this concern?",
    "options": {
      "A": "SIEM",
      "B": "Vulnerability scanner",
      "C": "DLP",
      "D": "Threat intelligence platform"
    },
    "correct_answer": "C",
    "explanation": "The core concern is the \"security of customer data,\" and while access controls restrict *who* can access the data, the question implies a broader concern about preventing unauthorized disclosure or exfiltration of this data. **Data Loss Prevention (DLP) (C)** is specifically designed to address this. DLP solutions monitor, detect, and block sensitive data (like customer data) from being exfiltrated or misused. They can identify data based on content, context, and destination, preventing it from leaving the organization's control whether accidentally or maliciously.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 84,
    "question_text": "A security analyst reviews the following report:\nLocation | Chassis manufacturer | OS | Application Developer | Vendor\nProduct A | United States | Local company A | Debian 11 | Unknown | Charlie Security Consulting\nProduct B | United States | Global company B | Red Hat Enterprise Linux | Developer B | BigBox Vulnerabilities\nWhich of the following assessments is the analyst performing?",
    "options": {
      "A": "System",
      "B": "Supply chain",
      "C": "Quantitative",
      "D": "Organizational"
    },
    "correct_answer": "B",
    "explanation": "The report provides details about different products (Product A, Product B), their manufacturers (Chassis manufacturer, Local company A, Global company B), the OS they run (Debian 11, Red Hat Enterprise Linux), their application developer (Unknown, Developer B), and finally, the vendor responsible for security (Charlie Security Consulting, BigBox Vulnerabilities). This breakdown clearly indicates an assessment focused on the various components and parties involved in the creation and delivery of these products, which is characteristic of a **supply chain assessment (B)**. The analyst is examining the security posture across the entire chain of providers and developers involved in the products' lifecycle.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 85,
    "question_text": "A security researcher tells a company that one of its solutions is vulnerable to buffer overflow, leading to a malicious coding execution. Which of the following is the best way to avoid this vulnerability in future versions?",
    "options": {
      "A": "Testing for CSRF vulnerabilities before the application goes to production",
      "B": "Using SAST tools to find vulnerabilities as part of the pipeline",
      "C": "Implementing canary protection in an earlier life-cycle stage",
      "D": "Implementing pair programming to improve development capabilities"
    },
    "correct_answer": "B",
    "explanation": "The problem states a buffer overflow vulnerability, which is a coding execution flaw. To avoid this type of vulnerability in *future versions* and ideally detect it earlier in the development lifecycle, **Static Application Security Testing (SAST) tools (B)** are the most appropriate. SAST tools analyze source code or compiled code for security vulnerabilities *before* the application is run. Integrating SAST into the development pipeline (e.g., during code commit or build stages) allows developers to identify and fix buffer overflows and similar coding flaws early, significantly reducing the cost and effort of remediation compared to finding them in later stages like production.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 86,
    "question_text": "Users are experiencing a variety of issues when trying to access corporate resources. Examples include:\n• Connectivity issues between local computers and file servers between branch offices\n• Inability to download corporate applications on mobile endpoints while working remotely\n• Certificate errors when accessing internal web applications\nWhich of the following actions are the most relevant when troubleshooting the reported issues? (Choose two.)",
    "options": {
      "A": "Review VPN throughput.",
      "B": "Check IDS rules.",
      "C": "Restore static content on the CDN.",
      "D": "Enable secure authentication using NAC.",
      "E": "Implement advanced WAF rules.",
      "F": "Validate MDM asset compliance."
    },
    "correct_answer": "A F",
    "explanation": "The reported issues are: connectivity issues (branch offices to file servers), inability to download applications on mobile endpoints remotely, and certificate errors on internal web applications.\n\n1.  **A. Review VPN throughput:** Connectivity issues between branch offices and central resources often point to network performance bottlenecks. If a VPN is used for inter-office communication or remote access, insufficient VPN throughput can cause slow connections and failed downloads, directly impacting the first two issues.\n2.  **F. Validate MDM asset compliance:** Mobile Device Management (MDM) solutions are used to manage and secure mobile endpoints. If corporate applications cannot be downloaded on mobile endpoints or if there are general access issues, it could be due to the mobile devices not being compliant with MDM policies (e.g., device not enrolled, necessary configurations missing, security checks failing). This directly addresses the mobile endpoint issue and can indirectly affect overall resource access.",
    "is_simulation": false,
    "question_type": "multiple_choice_multiple_answer"
  },
  {
    "question_number": 87,
    "question_text": "A network engineer recorded the following test results:\nSource | Destination | Latency | Action\n192.168.1.55 | 192.168.1.205 | 10ms | Allow\n192.168.1.55 | 192.168.1.66 | 9ms | Allow\n192.168.1.55 | 192.168.1.101 | 11ms | Allow\n192.168.1.55 | 192.168.1.30 | 9ms | Allow\nAfter a new network security appliance was deployed, the results of the network test are as follows:\nSource | Destination | Latency | Action\n192.168.1.55 | 192.168.1.205 | 710ms | Allow\n192.168.1.55 | 192.168.1.66 | - | Drop\n192.168.1.55 | 192.168.1.101 | 211ms | Allow\n192.168.1.55 | 192.168.1.30 | 109ms | Allow\nWhich of the following network infrastructure components most likely produced these results?",
    "options": {
      "A": "IPS",
      "B": "CDN",
      "C": "VPN",
      "D": "IDS"
    },
    "correct_answer": "A",
    "explanation": "The initial test results show that all traffic between the source (192.168.1.55) and various destinations (192.168.1.205, 192.168.1.66, 192.168.1.101, 192.168.1.30) is `Allow`ed, with latencies in the range of 9-11ms. After a new security appliance is deployed, traffic to 192.168.1.66 changes from `Allow` to `Drop`, while other traffic remains `Allow`ed but with significantly increased latencies (e.g., 710ms, 211ms, 109ms). This behavior is most characteristic of an **Intrusion Prevention System (IPS) (A)**. An IPS is deployed in-line and actively monitors network traffic for malicious activity. When it detects a threat (which it might have done for traffic to 192.168.1.66, leading to the `Drop` action), it can block or drop that traffic. The significant increase in latency for the allowed traffic is also consistent with an in-line appliance performing deep packet inspection or other resource-intensive security checks. An IDS (D) would only alert, not drop traffic. CDN (B) and VPN (C) have different primary functions.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 88,
    "question_text": "A developer needs to improve the cryptographic strength of a password-storage component in a web application without completely replacing the crypto-module. Which of the following is the most appropriate technique?",
    "options": {
      "A": "Key splitting",
      "B": "Key escrow",
      "C": "Key rotation",
      "D": "Key encryption",
      "E": "Key stretching"
    },
    "correct_answer": "E",
    "explanation": "To improve the cryptographic strength of a password-storage component without replacing the entire crypto-module, **key stretching (E)** is the most appropriate technique. Key stretching (e.g., using algorithms like PBKDF2, bcrypt, or scrypt) makes brute-force attacks and rainbow table attacks more difficult by intentionally slowing down the hashing process. It does this by iteratively applying a hashing function many times, optionally with a salt, making each password attempt computationally more expensive for an attacker. This directly strengthens the security of stored passwords.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 89,
    "question_text": "A company wants to implement hardware security key authentication for accessing sensitive information systems. The goal is to prevent unauthorized users from gaining access with a stolen password. Which of the following models should the company implement to best solve this issue?",
    "options": {
      "A": "Rule-based",
      "B": "Time-based",
      "C": "Role-based",
      "D": "Context-based"
    },
    "correct_answer": "D",
    "explanation": "The goal is to prevent unauthorized users from gaining access with a stolen password, specifically by implementing hardware security key authentication for sensitive systems. While hardware keys are a strong form of authentication, combining them with a **context-based access control (D)** model would best solve the broader issue. Context-based access control makes authorization decisions based on attributes beyond just user identity, such as device health, location, time of day, network used, and user behavior. For example, even if a stolen password and hardware key are used, if the login attempt comes from an unusual location or device, or at an abnormal time (i.e., out of context), the system can deny access or prompt for additional verification. This dynamic evaluation of context enhances security by making it harder for an attacker to leverage stolen credentials.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 90,
    "question_text": "Which of the following is the main reason quantum computing advancements are leading companies and countries to deploy new encryption algorithms?",
    "options": {
      "A": "Encryption systems based on large prime numbers will be vulnerable to exploitation.",
      "B": "Zero Trust security architectures will require homomorphic encryption.",
      "C": "Perfect forward secrecy will prevent deployment of advanced firewall monitoring techniques.",
      "D": "Quantum computers will enable malicious actors to capture IP traffic in real time."
    },
    "correct_answer": "A",
    "explanation": "The primary concern with quantum computing advancements in cryptography is their potential to break widely used public-key encryption algorithms. Many current asymmetric encryption systems (like RSA and ECC) and digital signature algorithms rely on the computational difficulty of factoring large prime numbers or solving elliptic curve discrete logarithm problems. Quantum computers, with algorithms like Shor's algorithm, could efficiently solve these problems, rendering such encryption systems vulnerable to exploitation. This is why companies and countries are researching and deploying post-quantum cryptography (PQC) or quantum-resistant algorithms.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 91,
    "question_text": "A company is adopting microservice architecture in order to quickly remediate vulnerabilities and deploy to production. All of the microservices run on the same Linux platform. Significant time was spent updating the base OS before deploying code. Which of the following should the company do to make the process efficient?",
    "options": {
      "A": "Use Terraform scripts while creating golden images.",
      "B": "Create a cron job to run apt-update every 30 days.",
      "C": "Use snapshots to deploy code to existing compute instances.",
      "D": "Deploy a centralized update server."
    },
    "correct_answer": "A",
    "explanation": "The problem states that \"significant time was spent updating the base OS before deploying code.\" To make this process efficient in a microservice architecture, the company should embrace automation and immutable infrastructure principles. \n\n**A. Use Terraform scripts while creating golden images:** Terraform is an Infrastructure as Code (IaC) tool that allows defining and provisioning infrastructure using configuration files. Golden images are pre-configured, hardened, and patched virtual machine or container images. By using Terraform to automate the creation and management of these golden images (which already include the updated base OS), the deployment process becomes significantly more efficient. Instead of updating the OS on each instance before deployment, new instances are simply provisioned from the pre-built, secure, and up-to-date golden images. This aligns well with the speed and efficiency goals of microservices and DevOps.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 92,
    "question_text": "During a gap assessment, an organization notes that BYOD usage is a significant risk. The organization implemented administrative policies prohibiting BYOD usage. However, the organization has not implemented technical controls to prevent the unauthorized use of BYOD assets when accessing the organization's resources. Which of the following solutions should the organization implement to best reduce the risk of BYOD devices? (Choose two.)",
    "options": {
      "A": "Cloud IAM, to enforce the use of token-based MFA",
      "B": "Conditional access, to enforce user-to-device binding",
      "C": "NAC, to enforce device configuration requirements",
      "D": "PAM, to enforce local password policies",
      "E": "SD-WAN, to enforce web content filtering through external proxies",
      "F": "DLP, to enforce data protection capabilities"
    },
    "correct_answer": "B C",
    "explanation": "The problem states that administrative policies prohibit BYOD, but *technical controls* are lacking to prevent unauthorized BYOD access to organizational resources. The goal is to reduce the risk of BYOD.\n\n1.  **B. Conditional access, to enforce user-to-device binding:** Conditional access policies are powerful technical controls that evaluate various conditions (user identity, device state, location, application being accessed) before granting access. Enforcing user-to-device binding ensures that a specific user can only access resources from a designated, managed device. This directly prevents unauthorized BYOD devices from gaining access.\n2.  **C. NAC (Network Access Control), to enforce device configuration requirements:** NAC solutions inspect devices attempting to connect to the network. They can enforce device configuration requirements (e.g., presence of antivirus, patch level, security settings). If a BYOD device doesn't meet these corporate configuration standards (which corporate-owned devices would), NAC can deny it access to the network or restrict it to a guest segment, thus preventing unauthorized BYOD usage.",
    "is_simulation": false,
    "question_type": "multiple_choice_multiple_answer"
  },
  {
    "question_number": 93,
    "question_text": "An organization has several systems deployed in a public cloud and wants to confirm that when data retention periods are reached, the data is properly disposed of. Which of the following best meets the organization's needs?",
    "options": {
      "A": "Double encrypting the data using both asymmetric and symmetric keys managed by the cloud service provider",
      "B": "Utilizing a data-wiping software to overwrite the existing data",
      "C": "Encrypting the data with customer-managed keys and then deleting both the encryption key and the volume",
      "D": "Asking the cloud provider for copies of certificates of destruction"
    },
    "correct_answer": "C",
    "explanation": "The most effective way to ensure data is properly disposed of in the public cloud, particularly when relying on data retention periods, is through crypto-shredding, which is achieved by: **C. Encrypting the data with customer-managed keys and then deleting both the encryption key and the volume.**\n\n*   **Customer-managed keys:** This ensures that the customer (organization) has full control over the encryption keys. If the customer deletes the key, even the cloud provider cannot decrypt the data.\n*   **Deleting the encryption key:** This is the core of crypto-shredding. Once the key is destroyed, the encrypted data becomes mathematically unrecoverable, fulfilling the disposal requirement. This is generally more reliable than relying solely on cloud provider 'deletion' functions for data on shared underlying storage.\n*   **Deleting the volume (or storage resource):** This removes the encrypted data itself, but the primary mechanism for ensuring unrecoverability, in this scenario, is the key destruction.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 94,
    "question_text": "A security engineer reviews an after-action report from a previous security breach and notes a long lag time between detection and containment of a compromised account. The engineer suggests using SOAR to address this concern. Which of the following best explains the engineer's goal?",
    "options": {
      "A": "To prevent accounts from being compromised",
      "B": "To enable log correlation using machine learning",
      "C": "To orchestrate additional reporting for the security operations center",
      "D": "To prepare runbooks to automate future incident response"
    },
    "correct_answer": "D",
    "explanation": "SOAR (Security Orchestration, Automation, and Response) platforms are designed to improve security operations efficiency by automating and orchestrating tasks, especially during incident response. The problem states a \"long lag time between detection and containment.\" SOAR directly addresses this by: **D. To prepare runbooks to automate future incident response.** SOAR platforms allow security teams to define playbooks (automated workflows) for specific incident types. These playbooks can automate repetitive tasks, integrate with various security tools (e.g., EDR, SIEM, identity management), and perform actions like isolating compromised systems, blocking malicious IPs, or disabling compromised accounts. By automating these response actions, SOAR significantly reduces the manual effort and time required for containment, thereby shortening the lag time.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 95,
    "question_text": "During an audit at an organization, auditors find that developers are able to promote code to production. The auditors request a full review of all production changes. Which of the following should the organization implement to prevent a full review in the future?",
    "options": {
      "A": "Branch protection",
      "B": "Centralized code repositories",
      "C": "Interactive application security testing",
      "D": "Change control board"
    },
    "correct_answer": "D",
    "explanation": "The auditors found that \"developers are able to promote code to production\" without proper oversight, leading to a request for a \"full review of all production changes.\" To prevent this in the future and ensure controlled, authorized deployments, the organization should implement a **Change Control Board (D)**. A Change Control Board (CCB) is a formal group responsible for reviewing, evaluating, approving, and scheduling all significant changes to a system, including code deployments to production. This process ensures that changes are vetted, planned, documented, and approved by relevant stakeholders (including security, operations, and business owners) before they go live, preventing unauthorized or unreviewed deployments.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 96,
    "question_text": "A systems engineer is configuring SSO for a business that will be using SaaS applications for its remote-only workforce. Privileged actions in SaaS applications must be allowed only from corporate mobile devices that meet minimum security requirements, but BYOD must also be permitted for other activity. Which of the following would best meet this objective?",
    "options": {
      "A": "Block any connections from outside the business's network security boundary.",
      "B": "Install machine certificates on corporate devices and perform checks against the clients.",
      "C": "Configure device attestations and continuous authorization controls.",
      "D": "Deploy application protection policies using a corporate, cloud-based MDM solution."
    },
    "correct_answer": "C",
    "explanation": "The key requirements are to allow privileged actions *only from corporate mobile devices that meet minimum security requirements* while *permitting BYOD for other activity*. This calls for granular access control based on device trust and continuous evaluation.\n\n**C. Configure device attestations and continuous authorization controls:**\n*   **Device attestation:** This process verifies the integrity and security posture of a device (e.g., corporate-owned status, patch level, presence of security software, absence of jailbreak/root). It provides a high level of assurance that a device meets the 'minimum security requirements'.\n*   **Continuous authorization controls:** This means that access decisions are not just made at login but are continuously evaluated during a session based on changes in context (e.g., device posture, user behavior, location). This enables the system to differentiate between trusted corporate devices (for privileged actions) and less trusted BYOD devices (for general access), and to revoke access if a device's posture changes. This approach is fundamental to a Zero Trust model and effectively meets the stated objective.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 97,
    "question_text": "A systems administrator wants to reduce the number of failed patch deployments in an organization. The administrator discovers that system owners modify systems or applications in an ad hoc manner. Which of the following is the best way to reduce the number of failed patch deployments?",
    "options": {
      "A": "Compliance tracking",
      "B": "Situational awareness",
      "C": "Change management",
      "D": "Quality assurance"
    },
    "correct_answer": "C",
    "explanation": "The core problem leading to failed patch deployments is that \"system owners modify systems or applications in an ad hoc manner.\" This indicates a lack of formalized control over system configurations. To address this, the organization needs to implement a robust **change management (C)** process. Change management ensures that all modifications to IT systems, applications, and configurations are systematically reviewed, approved, documented, and tested before implementation. By enforcing a change management process, ad hoc modifications are prevented, ensuring that systems remain in a known, controlled state, which significantly reduces the likelihood of patch deployments failing due to unexpected configuration drifts.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 98,
    "question_text": "A network engineer must ensure that always-on VPN access is enabled but restricted to company assets. Which of the following best describes what the engineer needs to do?",
    "options": {
      "A": "Generate device certificates using the specific template settings needed.",
      "B": "Modify signing certificates in order to support IKE version 2.",
      "C": "Create a wildcard certificate for connections from public networks.",
      "D": "Add the VPN hostname as a SAN entry on the root certificate."
    },
    "correct_answer": "A",
    "explanation": "To ensure \"always-on VPN access is enabled but restricted to company assets,\" device-based authentication is crucial. This means the VPN client must be able to authenticate itself as a trusted company device.\n\n**A. Generate device certificates using the specific template settings needed:** This is the most appropriate action. Device certificates are X.509 digital certificates issued to individual devices. These certificates can contain specific attributes (via certificate templates) that identify them as corporate assets, include device IDs, and define their intended use. The always-on VPN client can then present this device certificate during the VPN connection process, and the VPN server can validate it against its trusted Certificate Authority (CA) and check its attributes to ensure only authorized corporate devices can establish the VPN tunnel. This provides strong device identity and allows for policy enforcement based on device characteristics.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 99,
    "question_text": "A security administrator is reviewing the following code snippet from a website component:\n<% call response.write(application(\"inc.tmp\").LoadFromFile(\"inc.tmp\")) %>\nA review of the inc.tmp file shows the following:\n21487592579325342038509345083453432452523435235345523453242353424523453452345389627656385793257839537854362038263053\n2804508325\nWhich of the following is most likely the reason for inaccuracies?",
    "options": {
      "A": "A content management solution plug-in has been exploited.",
      "B": "A search engine's bots are being blocked at the firewall.",
      "C": "The relevant stylesheet has become corrupted.",
      "D": "The WAF is configured to be in transparent mode."
    },
    "correct_answer": "A",
    "explanation": "The code snippet includes `<% call response.write(application(\"inc.tmp\").LoadFromFile(\"inc.tmp\")) %>`, which attempts to load and write the content of `inc.tmp`. The content of `inc.tmp` is a very long string of what appears to be numeric characters. This suggests a potential issue where the website component is trying to load and display content that is not as expected for a typical website stylesheet (which is usually text-based, like CSS). \n\nIf a content management solution (CMS) plug-in is exploited (A), an attacker might manipulate files or inject malicious code that changes how content is loaded or rendered, leading to unexpected and inaccurate output like the numeric string. This is a common attack vector where legitimate functionality is leveraged to display unintended content, possibly due to a vulnerability in the plug-in that allows file manipulation or content injection.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 100,
    "question_text": "An organization wants to implement a platform to better identify which specific assets are affected by a given vulnerability. Which of the following components provides the best foundation to achieve this goal?",
    "options": {
      "A": "SASE",
      "B": "CMDB",
      "C": "SBoM",
      "D": "SIEM"
    },
    "correct_answer": "B",
    "explanation": "To effectively identify which specific assets are affected by a given vulnerability, an organization needs a comprehensive and accurate inventory of its assets and their relationships. A **Configuration Management Database (CMDB) (B)** provides the best foundation for this. A CMDB is a database that stores information about an organization's IT assets (configuration items - CIs), including hardware, software, services, and their interdependencies. By having a well-maintained CMDB, security teams can quickly query for assets with specific characteristics (e.g., OS version, installed software) that are known to be vulnerable, and understand their impact based on their relationships to other CIs and business services.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 101,
    "question_text": "Which of the following best explains why AI output could be inaccurate?",
    "options": {
      "A": "Model poisoning",
      "B": "Social engineering",
      "C": "Output handling",
      "D": "Prompt injections"
    },
    "correct_answer": "A",
    "explanation": "**Model poisoning (A)** is a type of attack against machine learning models where an attacker injects malicious data into the training dataset. This corrupted data then influences the model's learning process, causing it to produce inaccurate, biased, or manipulated outputs when it is later deployed. For example, a model trained on poisoned data might incorrectly classify inputs, leading to inaccurate results that reflect the attacker's intentions rather than true patterns in the data. This directly explains why AI output could be inaccurate.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 102,
    "question_text": "A company runs a DAST scan on a web application. The tool outputs the following recommendations:\n• Use Cookie prefixes.\n• Content Security Policy - SameSite=strict is not set.\nWhich of the following vulnerabilities has the tool identified?",
    "options": {
      "A": "RCE",
      "B": "XSS",
      "C": "CSRF",
      "D": "TOCTOU"
    },
    "correct_answer": "C",
    "explanation": "The recommendations provided by the DAST tool are strong indicators of a **CSRF (Cross-Site Request Forgery) (C)** vulnerability:\n\n*   **Use Cookie prefixes (e.g., `__Host-` or `__Secure-`):** These prefixes help protect cookies from being sent in cross-site requests, which is a common vector for CSRF attacks. They ensure that cookies are sent only with requests to the domain they were set for and over secure connections.\n*   **Content Security Policy - SameSite=strict is not set:** The `SameSite` cookie attribute is a powerful defense against CSRF attacks. Setting `SameSite=Strict` ensures that the browser will only send the cookie with requests originating from the same site as the cookie. If it's not set (or set to `Lax` or `None`), cookies can be sent in cross-site requests, making CSRF attacks possible. Both recommendations are directly related to preventing or mitigating CSRF vulnerabilities.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 103,
    "question_text": "Which of the following best describes the reason a network architect would enable forward secrecy on all VPN tunnels?",
    "options": {
      "A": "This process is a requirement to enable hardware-accelerated cryptography.",
      "B": "This process reduces the success of attackers performing cryptanalysis.",
      "C": "The business requirements state that confidentiality is a critical success factor.",
      "D": "Modern cryptographic protocols list this process as a prerequisite for use."
    },
    "correct_answer": "B",
    "explanation": "Forward secrecy (or perfect forward secrecy - PFS) is a property of a key agreement protocol that ensures that a session key derived from a set of long-term keys will not be compromised if the long-term keys are compromised in the future. In the context of VPN tunnels, if an attacker records encrypted traffic and later compromises the server's long-term private key, forward secrecy ensures that they cannot decrypt the previously captured session traffic because each session's key was ephemeral and not derived directly from the long-term key in a way that allows retroactive decryption. Therefore, enabling forward secrecy **reduces the success of attackers performing cryptanalysis (B)**, specifically against past communications, even if future compromises occur. It protects the confidentiality of historical sessions.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 104,
    "question_text": "Which of the following best explains the importance of determining organizational risk appetite when operating with a constrained budget?",
    "options": {
      "A": "Risk appetite directly impacts acceptance of high-impact, low-likelihood events.",
      "B": "Organizational risk appetite varies from organization to organization.",
      "C": "Budgetary pressure drives risk mitigation planning in all companies.",
      "D": "Risk appetite directly influences which breaches are disclosed publicly."
    },
    "correct_answer": "A",
    "explanation": "Risk appetite is the amount and type of risk that an organization is willing to take on to achieve its strategic objectives. When operating with a constrained budget, understanding the organization's risk appetite becomes critical because it **directly impacts acceptance of high-impact, low-likelihood events (A)**. A clear risk appetite guides decision-making on which risks to mitigate (and thus allocate budget to) and which to accept (tolerate). With limited funds, an organization must prioritize spending on risks that fall outside its acceptable appetite. For example, if a high-impact, low-likelihood event falls within the organization's acceptable risk appetite, it might choose to accept that risk rather than spending a significant portion of its constrained budget to mitigate it, allowing funds to be allocated to risks that exceed the appetite.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 105,
    "question_text": "A company hired an email service provider called my-email.com to deliver company emails. The company started having several issues during the migration. A security engineer is troubleshooting and observes the following configuration snippet:\nDNS Records:\nHost | Type | Data | TTL\n@ | MX | mail.my-email.com | 45000\nwww | CNAME | web01.company.com | 3600\nemail | CNAME | web01.company.com | 3600\nsrv01 | A | 192.168.1.10 | 3600\nweb01 | A | 192.168.1.11 | 3600\n@ | TXT | \"v=spf ip4:192.168.1.11 ~all\" | 3600\nWhich of the following should the security engineer modify to fix the issue? (Choose two.)",
    "options": {
      "A": "The email CNAME record must be changed to a type A record pointing to 192.168.1.11",
      "B": "The TXT record must be changed to \"v=dmarc ip4:192.168.1.10 include:my-email.com ~all\"",
      "C": "The srv01 A record must be changed to a type CNAME record pointing to the email server",
      "D": "The email CNAME record must be changed to a type A record pointing to 192.168.1.10",
      "E": "The TXT record must be changed to \"v=dkim ip4:192.168.1.11 include:my-email.com ~all\"",
      "F": "The TXT record must be changed to \"v=spf ip4:192.168.1.10 include:my-email.com ~all\"",
      "G": "The srv01 A record must be changed to a type CNAME record pointing to the web01 server"
    },
    "correct_answer": "D F",
    "explanation": "The issue is with email delivery problems after migrating to a new email service provider (my-email.com). The DNS configuration snippet shows several records related to email (MX, CNAME, TXT). Email delivery issues are commonly related to misconfigured SPF, DKIM, or DMARC records, which are designed to prevent email spoofing and phishing.\n\n1.  **D. The email CNAME record must be changed to a type A record pointing to 192.168.1.10:** The snippet shows `email CNAME web01.company.com`. If `web01.company.com` is a web server and not the email server from `my-email.com`, then this CNAME record is incorrect. The email CNAME (or an A record directly) for the email service should point to the correct IP address or hostname of the new email service provider's servers (`my-email.com`). For example, if `my-email.com`'s mail server is `192.168.1.10`, then pointing the email CNAME to that A record for the email server would be appropriate, assuming `email.company.com` is where the email service is expected to be accessible.\n2.  **F. The TXT record must be changed to \"v=spf ip4:192.168.1.10 include:my-email.com ~all\":** The current SPF record is `v=spf ip4:192.168.1.11 ~all`. This SPF record specifies that only `192.168.1.11` is authorized to send email for `company.com`. However, emails are now being delivered by `my-email.com`. The SPF record needs to explicitly authorize `my-email.com`'s sending infrastructure. Adding `include:my-email.com` to the SPF record and correcting the `ip4` to include the correct sending IP (`192.168.1.10` if that is `my-email.com`'s IP, or just include its domain) would authorize `my-email.com` as a legitimate sender, which is crucial for preventing emails from being flagged as spam. The example TXT record provided in the option is a good candidate for correction if `192.168.1.10` is indeed the correct outbound IP for `my-email.com`.",
    "is_simulation": false,
    "question_type": "multiple_choice_multiple_answer"
  },
  {
    "question_number": 106,
    "question_text": "After a company discovered a zero-day vulnerability in its VPN solution, the company plans to deploy cloud-hosted resources to replace its current on-premises systems. An engineer must find an appropriate solution to facilitate trusted connectivity. Which of the following capabilities is the most relevant?",
    "options": {
      "A": "Container orchestration",
      "B": "Microsegmentation",
      "C": "Conditional access",
      "D": "Secure access service edge"
    },
    "correct_answer": "D",
    "explanation": "The scenario describes a transition from an on-premises VPN (which had a zero-day vulnerability) to cloud-hosted resources, with a need for \"trusted connectivity.\" This perfectly aligns with the purpose of a **Secure Access Service Edge (SASE) (D)** architecture. SASE converges network security functions (like firewalls, VPN, secure web gateways, CASB, DLP) with WAN capabilities into a single, cloud-native service. It is designed to provide secure, seamless, and trusted access to applications and data from anywhere, for any user or device, by pushing security enforcement to the edge, closer to the user. This approach effectively replaces traditional VPNs and perimeter security for a distributed workforce and cloud-based resources, offering robust trusted connectivity.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 107,
    "question_text": "Recent reports indicate that a software tool is being exploited. Attackers were able to bypass user access controls and load a database. A security analyst needs to find the vulnerability and recommend a mitigation. The analyst generates the following output:\nC:\\>whoami\nlocal\\user\nC:\\>net user local-user Welcome!\nThe command completed successfully!\nC:\\>loader.exe -admin db10ad3f\nLoading database. Please Wait...\nC:\\>loader.exe admin db10ad3f\nWhich of the following would the analyst most likely recommend?",
    "options": {
      "A": "Installing appropriate EDR tools to block pass-the-hash attempts",
      "B": "Adding additional time to software development to perform fuzz testing",
      "C": "Removing hard-coded credentials from the source code",
      "D": "Not allowing users to change their local passwords"
    },
    "correct_answer": "C",
    "explanation": "The output `cl:user@COMPTIA-PC> Welcome!` `c:\\loader.exe -admin db10ad3f` `c:\\loader.exe admin db10ad3f` shows a program (`loader.exe`) being executed with `admin db10ad3f` as parameters. The fact that the output contains `admin db10ad3f` and it's being used to load a database (`Loading database. Please Wait...`) strongly suggests that `db10ad3f` is a hard-coded password or credential being passed directly to the application. This is a severe security flaw because anyone who can see the command line (e.g., through process listings, logs, or reverse engineering) can discover the credential. Therefore, the most likely recommendation to mitigate this vulnerability is to **remove hard-coded credentials from the source code (C)** and instead use a secure method for credential management (e.g., secrets management tools, environment variables, or secure configuration files that are not publicly exposed).",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 108,
    "question_text": "The identity and access management team is sending logs to the SIEM for continuous monitoring. The deployed log collector is forwarding logs to the SIEM. However, only false positive alerts are being generated. Which of the following is the most likely reason for the inaccurate alerts?",
    "options": {
      "A": "The compute resources are insufficient to support the SIEM.",
      "B": "The SIEM indexes are too large.",
      "C": "The data is not being properly parsed.",
      "D": "The retention policy is not properly configured."
    },
    "correct_answer": "C",
    "explanation": "If a SIEM (Security Information and Event Management) is generating \"only false positive alerts,\" and the logs are being forwarded, the most likely reason is that **the data is not being properly parsed (C)**. SIEMs rely on parsing incoming log data into structured fields so that rules, correlations, and analytics can accurately interpret the events. If parsing is incorrect, the SIEM might misinterpret log entries, leading to triggers for benign events or failing to recognize true threats. For example, a username or IP address might be parsed incorrectly, leading to a rule firing when it shouldn't. Insufficient compute resources (A) or large indexes (B) would typically manifest as performance issues, not necessarily false positives. Retention policy (D) affects how long data is kept, not its accuracy in triggering alerts.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 109,
    "question_text": "The security team is receiving escalated support tickets stating that one of the company's publicly available websites is not loading as expected.\nGiven the following observations:\nServer | URL | Installed certificate | Age of installed certificate\nSALES10 | www.sales.com | *.sales.com | 366 days\nSALES10 | fulfillment.sales.com | *.sales.com | 282 days\nWEB27 | www.website.com | website.com | 418 days\nSALES20 | tracking.sales.com | tracking.sales.com | 240 days\nEVENT2 | event.sales.com | event.sales.com | 57 days\nWhich of the following is most likely the root cause?",
    "options": {
      "A": "A certificate signed by a global root certification authority has expired.",
      "B": "A protocol mismatch error is expected to occur when using outdated browsers.",
      "C": "One certificate is being bound to multiple websites on the same server.",
      "D": "Subject alternative names were not used appropriately for subdomains."
    },
    "correct_answer": "A",
    "explanation": "The table shows the `Age of installed certificate` for `WEB27` (www.website.com and website.com) as `418 days`. The standard validity period for publicly trusted SSL/TLS certificates has been shrinking, often limited to around 398 days or less by CAs and browsers. A certificate with an age of `418 days` is clearly past this common validity period, indicating that it has likely expired. An expired certificate will cause browsers to display security warnings and prevent users from accessing the website, aligning with the \"not loading as expected\" reports.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 110,
    "question_text": "A company acquires a location with a large infrastructure of legacy devices. Because of the hardware's age and the legacy software's limitations, the OS cannot be upgraded, and the machines cannot be virtualized. These machines are not publicly facing, but they do have internet access. The following controls are currently in place:\n• EDR\n• Anti-malware\n• Logging and monitoring\n• Host-based firewall\n• Proxied internet access\nA security architect needs to supplement the existing control strategy with one that restricts unauthorized software. Which of the following controls should the architect recommend to best supplement the existing environment?",
    "options": {
      "A": "SIEM",
      "B": "Isolation",
      "C": "Conditional access",
      "D": "Application control"
    },
    "correct_answer": "D",
    "explanation": "The core problem is legacy devices that cannot be upgraded or virtualized, have internet access, and the need to \"restrict unauthorized software.\" Given the existing controls (EDR, anti-malware, host firewall, proxy), the most direct and effective supplementary control for restricting unauthorized software is **application control (D)**.\n\nApplication control (also known as application whitelisting) is a security measure that specifies which applications are permitted to run on a system. Instead of trying to block known malicious software (which anti-malware and EDR do), application control only allows explicitly approved applications to execute. This is particularly effective for legacy systems where patching is difficult, as it prevents any unauthorized or unknown executables from running, even if a vulnerability is exploited. This creates a strong defense against malware and unauthorized software installations.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 111,
    "question_text": "An organization wants to create a threat model to identify vulnerabilities in its infrastructure. Which of the following should be prioritized first?",
    "options": {
      "A": "External-facing infrastructure with known exploited vulnerabilities",
      "B": "Internal infrastructure with high-severity and known exploited vulnerabilities",
      "C": "External-facing infrastructure with a low risk score and no known exploited vulnerabilities",
      "D": "External-facing infrastructure with a high risk score that can only be exploited with local access to the resource"
    },
    "correct_answer": "A",
    "explanation": "When performing threat modeling, the prioritization should focus on the most critical risks that are actively exploitable and directly expose the organization to attack. \n\n**A. External-facing infrastructure with known exploited vulnerabilities:** This combination represents the highest immediate risk. \"External-facing\" means it's directly accessible from the internet (high attack surface). \"Known exploited vulnerabilities\" means active exploits exist in the wild for these flaws, increasing the likelihood of a successful attack. Prioritizing these assets and vulnerabilities ensures that the most imminent and dangerous threats facing the organization are addressed first.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 112,
    "question_text": "A Chief Information Security Officer requests an action plan to remediate vulnerabilities. A security analyst reviews the output from a recent vulnerability scan and notices hundreds of unique vulnerabilities. The output includes the CVSS score, IP address, hostname, and the list of vulnerabilities. The analyst determines more information is needed in order to decide which vulnerabilities should be fixed immediately. Which of the following is the best source for this information?",
    "options": {
      "A": "Third-party risk review",
      "B": "Business impact analysis",
      "C": "Incident response playbook",
      "D": "Crisis management plan"
    },
    "correct_answer": "B",
    "explanation": "To prioritize which of \"hundreds of unique vulnerabilities\" should be fixed immediately, the analyst needs to understand the potential impact of each vulnerability's exploitation on the organization's business operations. A **Business Impact Analysis (BIA) (B)** is the best source for this information. A BIA identifies and evaluates the potential effects of a disruption to critical business functions and processes. It quantifies the financial and non-financial impacts of outages, defining recovery time objectives (RTOs) and recovery point objectives (RPOs). By correlating vulnerabilities with the critical business processes and data they could affect, the analyst can determine which vulnerabilities pose the highest business risk and therefore require immediate remediation, even if their CVSS score isn't the absolute highest.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 113,
    "question_text": "A security operations analyst is reviewing network traffic baselines for nightly database backups. Given the following information:\nDate | Time | Bandwidth consumed | SRC server | DST server\n12/1 | 12:01 a.m. | 11.70GB | PRDDB01 | BACKUP01\n12/2 | 12:01 a.m. | 11.57GB | PRDDB01 | BACKUPUP01\n12/3 | 12:01 a.m. | 97.00GB | PRDDB01 | 85.34.17.88\n12/4 | 12:01 a.m. | 0.00GB | PRDDB01 | BACKUP01\nWhich of the following should the security analyst do next?",
    "options": {
      "A": "Consult with a network engineer to determine the impact of bandwidth usage.",
      "B": "Quarantine PRDDB01 and then alert the database engineers.",
      "C": "Refer to the incident response playbook for the proper response.",
      "D": "Review all the network logs for further data exfiltration."
    },
    "correct_answer": "C",
    "explanation": "The table shows that on 12/3 and 12/4, there were unexpected increases in bandwidth consumed during database backups (97.00GB and 0.00GB vs. baselines around 11GB). Specifically, on 12/3, the `Dst server` was `85.34.17.88`, which is an external IP, unlike the usual internal `BACKUP01` and `BACKUPUP01`. This strongly suggests unauthorized data transfer or an attempt at exfiltration, which is an incident. The most appropriate next step for a security analyst upon detecting a potential incident is to **refer to the incident response playbook for the proper response (C)**. The playbook will provide predefined steps, roles, and procedures for containing, investigating, and remediating such an event, ensuring a structured and effective response.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 114,
    "question_text": "A company has a requirement in customer contracts that states applications must undergo external audits to identify vulnerabilities. Which of the following is the best action for the company to complete before hiring an external auditor?",
    "options": {
      "A": "Gather evidence for the audit.",
      "B": "Conduct an internal audit assessment.",
      "C": "Identify lessons learned from the audit.",
      "D": "Select samples for audit testing."
    },
    "correct_answer": "B",
    "explanation": "Before hiring an external auditor for vulnerability identification, the best action for the company to complete is to **conduct an internal audit assessment (B)**. Performing an internal assessment (e.g., internal vulnerability scans, penetration tests, code reviews) allows the company to identify and remediate known issues proactively. This \"pre-audit\" helps ensure that the organization is prepared for the external audit, potentially reduces the number of findings from the external auditor, and demonstrates a commitment to self-improvement. It's a strategic move to optimize the external audit process and its outcome.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 115,
    "question_text": "During DAST scanning, applications are consistently reporting code defects in open-source libraries that were used to build web applications. Most of the code defects are from using libraries with known vulnerabilities. The code defects are causing product deployment delays. Which of the following is the best way to uncover these issues earlier in the life cycle?",
    "options": {
      "A": "Directing application logs to the SIEM for continuous monitoring",
      "B": "Modifying the WAF polices to block against known vulnerabilities",
      "C": "Completing an IAST scan against the web application",
      "D": "Using a software dependency management solution"
    },
    "correct_answer": "D",
    "explanation": "The problem states that DAST scanning (which occurs late in the SDLC) is consistently finding \"code defects in open-source libraries that were used to build web applications,\" specifically from \"known vulnerabilities.\" To uncover these issues *earlier in the life cycle* (shifting left), the best approach is to use a **software dependency management solution (D)**. These tools (often referred to as Software Composition Analysis - SCA tools) analyze an application's dependencies (libraries, frameworks, and components), identify open-source components, and check them against databases of known vulnerabilities (CVEs). By integrating this into the build or development phase, vulnerabilities in third-party libraries can be identified and remediated much earlier, preventing deployment delays.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 116,
    "question_text": "A company's SIEM is designed to associate the company’s asset inventory with user events. Given the following report:\nHostname | Account | Attempted log-ins | Failed log-ins | Successful log-ins\nServer 1 | Sales_User | 3 | 0 | 3\nServer 2 | Accounting_User | 5 | 1 | 4\nServer 4 | Administrator | 2 | 0 | 0\nServer 4 | HR_User | 5 | 0 | 5\nServer 5 | Administrator | 0 | 0 | 0\nWhich of the following should a security engineer investigate first as part of a log audit?",
    "options": {
      "A": "An endpoint that is not submitting any logs",
      "B": "Potential activity indicating an attacker moving laterally in the network",
      "C": "A misconfigured syslog server creating false negatives",
      "D": "Unauthorized usage attempts of the administrator account"
    },
    "correct_answer": "D",
    "explanation": "The report shows several log entries for different servers and accounts. The most critical entry to investigate first is for `Server 4` associated with the `Administrator` account, which shows `2` attempted log-ins but `0` successful log-ins. This could indicate a brute-force attack or credential stuffing attempt against a highly privileged administrative account. Although there are no successful logins yet, any attempts against an Administrator account warrant immediate investigation due to the high potential impact of compromise. The `HR_User` account on `Server 4` also shows successful logins, but unauthorized usage attempts against the Administrator account are a higher priority.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 117,
    "question_text": "A developer receives feedback about code quality and efficiency. The developer needs to identify and resolve the following coding issues before submitting the code changes for peer review:\n• Indexing beyond arrays\n• Dereferencing null pointers\n• Potentially dangerous data type combos\n• Unreachable code\n• Non-portable constructs\nWhich of the following would be most appropriate for the developer to use in this situation?",
    "options": {
      "A": "Linting",
      "B": "SBoM",
      "C": "DAST",
      "D": "Branch protection",
      "E": "Software composition analysis"
    },
    "correct_answer": "A",
    "explanation": "The issues listed (indexing beyond arrays, dereferencing null pointers, dangerous data type combos, unreachable code, non-portable constructs) are all common programming errors or style issues that affect code quality, correctness, and potential bugs rather than direct security vulnerabilities (like injection or XSS). **Linting (A)** is a static code analysis process that checks source code for programmatic and stylistic errors, as well as potential bugs, suspicious constructs, or deviations from coding standards. Linters can identify many of the issues described, improving code quality and preventing runtime errors. While some of these might indirectly lead to vulnerabilities, linting is the most direct tool for identifying and resolving these types of code quality issues.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 118,
    "question_text": "A company wants to improve and automate the compliance of its cloud environments to meet industry standards. Which of the following resources should the company use to best achieve this goal?",
    "options": {
      "A": "Jenkins",
      "B": "Python",
      "C": "Ansible",
      "D": "PowerShell"
    },
    "correct_answer": "C",
    "explanation": "The goal is to \"improve and automate the compliance of its cloud environments to meet industry standards.\" This requires a tool that can define and enforce desired configurations across a large number of systems.\n\n**Ansible (C)** is an open-source automation tool for configuration management, application deployment, orchestration, and provisioning. It is agentless and uses YAML for playbooks, making it easy to define desired states and configurations. Ansible can be used to automate the deployment of security configurations, enforce compliance baselines, and ensure that cloud environments adhere to industry standards, thus directly addressing the need for automation and compliance in a scalable way.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 119,
    "question_text": "A company wants to protect against the most common attacks and rapidly integrate with different programming languages. Which of the following technologies is most likely to meet this need?",
    "options": {
      "A": "RASP",
      "B": "Cloud-based IDE",
      "C": "DAST",
      "D": "NIPS"
    },
    "correct_answer": "A",
    "explanation": "The requirements are to \"protect against the most common attacks\" and \"rapidly integrate with different programming languages.\"\n\n**RASP (Runtime Application Self-Protection) (A)** is a security technology that integrates directly into an application's runtime environment. It works by analyzing application behavior and context to detect and prevent attacks in real-time. RASP is effective against common attacks (like injections, XSS, broken authentication) because it understands the application's logic. Moreover, RASP solutions are designed to be integrated into various programming languages (e.g., Java, .NET, Node.js, Python, Ruby), offering broad language support and relatively rapid deployment compared to manual code changes or complex network security solutions. This makes it a strong fit for both requirements.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 120,
    "question_text": "An organization is concerned about insider threats from employees who have individual access to encrypted material. Which of the following techniques best addresses this issue?",
    "options": {
      "A": "SSO with MFA",
      "B": "Salting and hashing",
      "C": "Account federation with hardware tokens",
      "D": "SAE",
      "E": "Key splitting"
    },
    "correct_answer": "E",
    "explanation": "The concern is about \"insider threats from employees who have individual access to encrypted material.\" This implies that even authorized individuals with access to encryption keys could potentially misuse the encrypted data. To mitigate this, **key splitting (E)** is the best technique. Key splitting (also known as secret sharing or key sharding) involves breaking an encryption key into multiple components (shares), such that a predefined minimum number of shares (e.g., M out of N) are required to reconstruct the original key. This prevents any single individual (including an insider) from having unilateral access to the encrypted material. It enforces a form of separation of duties over sensitive keys, making it much harder for a single malicious insider to decrypt and misuse data.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 121,
    "question_text": "A manufacturing plant is updating its IT services. During discussions, the senior management team created the following list of considerations:\n• Staff turnover is high and seasonal.\n• Extreme conditions often damage endpoints.\n• Losses from downtime must be minimized.\n• Regulatory data retention requirements exist.\nWhich of the following best addresses the considerations?",
    "options": {
      "A": "Establishing further environmental controls to limit equipment damage",
      "B": "Using a non-persistent virtual desktop interface with thin clients",
      "C": "Deploying redundant file servers and configuring database journaling",
      "D": "Maintaining an inventory of spare endpoints for rapid deployment"
    },
    "correct_answer": "B",
    "explanation": "Let's analyze how **B. Using a non-persistent virtual desktop interface with thin clients** addresses the considerations:\n\n*   **Staff turnover is high and seasonal:** With non-persistent VDI, user desktops are created from a golden image each time a user logs in and are destroyed upon logout. This simplifies onboarding/offboarding significantly, as user environments can be quickly provisioned or de-provisioned, making it ideal for high turnover.\n*   **Extreme conditions often damage endpoints:** Thin clients are generally more robust and less susceptible to environmental damage than full desktop PCs. Even if a thin client is damaged, the user's persistent data and applications reside in the data center (or cloud), allowing them to quickly resume work from another thin client or device.\n*   **Losses from downtime must be minimized:** VDI centralizes desktop management, enabling rapid recovery. If a user's session crashes or a thin client fails, they can quickly reconnect to a new virtual desktop from any available thin client, minimizing personal downtime. The entire VDI infrastructure can also be made highly available.\n*   **Regulatory data retention requirements exist:** With data centralized in the VDI infrastructure rather than on individual endpoints, data retention, backup, and compliance become easier to manage. Data can be consistently retained and managed in a secure data center environment.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 122,
    "question_text": "A software vendor provides routine functionality and security updates to its global customer base. The vendor would like to ensure distributed updates are authorized, originate from only the company, and have not been modified by others. Which of the following solutions best supports these objectives?",
    "options": {
      "A": "Envelope encryption",
      "B": "File integrity monitoring",
      "C": "Application control",
      "D": "Code signing"
    },
    "correct_answer": "D",
    "explanation": "To ensure that distributed updates are **authorized**, **originate from only the company**, and **have not been modified by others**, **code signing (D)** is the best solution. Code signing involves digitally signing software updates with the vendor's private key. Customers can then use the vendor's public key to verify the signature. This process achieves:\n\n*   **Origin assurance:** Verifies that the update indeed came from the legitimate vendor.\n*   **Integrity:** Confirms that the update has not been tampered with since it was signed. If any modification occurs, the signature becomes invalid.\n*   **Authorization:** Implicitly, only authorized personnel with access to the signing key can produce valid signed updates. This ensures that users are installing legitimate and safe updates from the intended source.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 123,
    "question_text": "A security analyst detects a possible RAT infection on a computer in the internal network. After reviewing the details of the alert, the analyst identifies the initial vector of the attack was an email that was forwarded to multiple recipients in the same organizational unit. Which of the following should the analyst do first to minimize this type of threat in the future?",
    "options": {
      "A": "Move from an anti-malware software to an EDR solution.",
      "B": "Perform a penetration test to detect technology gaps on the anti-spam solution.",
      "C": "Configure an IPS solution in the internal network to mitigate infections.",
      "D": "Implement a security awareness program in the organization."
    },
    "correct_answer": "D",
    "explanation": "The initial vector of the RAT infection was an \"email that was forwarded to multiple recipients.\" This is a classic social engineering attack, likely a phishing or spear-phishing attempt, where users are tricked into executing malicious content. To minimize this type of threat in the future, the most foundational and proactive step is to **implement a security awareness program in the organization (D)**. Effective security awareness training educates users about recognizing and reporting suspicious emails, links, and attachments, making them the first line of defense against such attacks. While other technical controls (A, B, C) are important, addressing the human element of the initial infection vector is critical for preventing similar future incidents.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 124,
    "question_text": "A cloud engineer needs to identify appropriate solutions to:\n• Provide secure access to internal and external cloud resources.\n• Eliminate split-tunnel traffic flows.\n• Enable identity and access management capabilities.\nWhich of the following solutions is the most appropriate?",
    "options": {
      "A": "Microsegmentation",
      "B": "PAM",
      "C": "SD-WAN",
      "D": "SASE"
    },
    "correct_answer": "D",
    "explanation": "The requirements perfectly describe the capabilities of a **Secure Access Service Edge (SASE) (D)** architecture:\n\n*   **Provide secure access to internal and external cloud resources:** SASE integrates various security functions (e.g., SWG, CASB, ZTNA, FWaaS) to secure access to both on-premises and cloud applications and data.\n*   **Eliminate split-tunnel traffic flows:** SASE directs all traffic through a centralized security stack in the cloud, preventing split-tunneling (where some traffic bypasses corporate security) and ensuring consistent policy enforcement.\n*   **Enable identity and access management capabilities:** Identity is a core component of SASE, allowing access policies to be based on user and device identity, location, and other contextual factors. This aligns with Zero Trust principles.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 125,
    "question_text": "A security engineer is building a solution to disable weak CBC configurations for remote access connections to Linux systems. Which of the following should the security engineer modify?",
    "options": {
      "A": "The /etc/openssl.conf file, updating the virtual site parameter",
      "B": "The /etc/nsswitch.conf file, updating the name server",
      "C": "The /etc/hosts file, updating the IP parameter",
      "D": "The /etc/sshd/ssh_config file, updating the ciphers"
    },
    "correct_answer": "D",
    "explanation": "The objective is to \"disable weak CBC configurations for remote access connections to Linux systems.\" Remote access to Linux systems is commonly done via SSH (Secure Shell). The configuration of SSH servers (including which ciphers and MAC algorithms are allowed) is controlled by the `sshd_config` file.\n\n**D. The /etc/sshd/ssh_config file, updating the ciphers:** The `/etc/sshd/sshd_config` file is the primary configuration file for the SSH daemon (`sshd`). Within this file, the `Ciphers` directive is used to specify the encryption algorithms (ciphers) that `sshd` will use. By editing this file, the engineer can remove or comment out entries for weak CBC-mode ciphers (e.g., `aes128-cbc`, `3des-cbc`) and ensure that only strong, recommended ciphers (e.g., AES-GCM modes) are enabled. This directly addresses the requirement to disable weak CBC configurations for remote access.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 126,
    "question_text": "A security engineer is reviewing the results of an annual penetration test. The report lists one of the results as \"critical severity\" on several domain-joined workstations:\nSSL/TLS Weak Protocols Supported TLS 1.0, TLS 1.1\nWhich of the following should the security engineer implement to remediate this finding in the most centralized manner?",
    "options": {
      "A": "An SCCM patch to disable weak protocols in the Schannel hive",
      "B": "A GPO to disable weak protocols in the Schannel hive",
      "C": "A PowerShell script to disable weak protocols in the HKLM Schannel hive",
      "D": "A registry script to disable weak protocols in the Schannel hive"
    },
    "correct_answer": "B",
    "explanation": "The finding is \"SSL/TLS Weak Protocols Supported TLS 1.0, TLS 1.1\" on \"several domain-joined workstations.\" To remediate this in the \"most centralized manner,\" the best approach is to use Group Policy Objects (GPOs) in an Active Directory environment.\n\n**B. A GPO to disable weak protocols in the Schannel hive:** The `Schannel` (Secure Channel) is a Security Support Provider (SSP) that implements SSL, TLS, and DTLS in Windows. TLS protocol versions and cipher suites are configured in the Windows Registry under the Schannel key (`HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\SecurityProviders\\Schannel\\Protocols`). A Group Policy Object (GPO) can be used to centrally manage and deploy registry settings across all domain-joined workstations, effectively disabling TLS 1.0 and TLS 1.1 (and other weak protocols/ciphers) by setting appropriate registry values within the Schannel hive. This provides a scalable and centralized way to enforce security configurations across an entire domain.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 127,
    "question_text": "An analyst reviews a SIEM and generates the following report:\nHost | Rule | Offense trigger\nVM002 | Network connection | TCP connection generated to web.corp.local\nVM002 | Network connection | Web navigation to comptia.org\nHOST002 | File download | File download from web.corp.local\nHOST002 | Network connection | Web navigation to web.corp.local\nVM002 | Network connection | Web navigation to comptia.org/files\nHOST002 | Log-in activity | Log-in successful after two attempts\nOnly HOST002 is authorized for internet traffic. Which of the following statements is accurate?",
    "options": {
      "A": "The VM002 host is misconfigured and needs to be revised by the network team.",
      "B": "The HOST002 host is under attack, and a security incident should be declared.",
      "C": "The SIEM platform is reporting multiple false positives on the alerts.",
      "D": "The network connection activity is unusual, and a network infection is highly possible."
    },
    "correct_answer": "A",
    "explanation": "The report states that \"Only HOST002 is authorized for internet traffic.\" However, the SIEM report shows multiple instances where `VM002` (which is not HOST002) has `Network connection` activity with rules triggering offenses, for example, `TCP connection generated to web.corp.local` and `Web navigation to comptia.org`. Since VM002 is *not* authorized for internet traffic, and its attempts are generating offenses, this indicates that `VM002` is attempting network activities it's not authorized to perform. This is a **misconfiguration on VM002 (A)**, as it's attempting network activities it's not authorized to perform, and thus needs to be revised by the network team to align with policy. It's not necessarily an attack on HOST002 (B), and the SIEM is correctly identifying the non-compliant traffic, so it's not false positives (C) or just unusual activity (D) without a clear misconfiguration.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 128,
    "question_text": "A company wants to implement a three-tier approach to separate the web, database, and application servers. A security administrator must harden the environment. Which of the following is the best solution?",
    "options": {
      "A": "Deploying a VPN to prevent remote locations from accessing server VLANs",
      "B": "Configuring a SASE solution to restrict users to server communication",
      "C": "Implementing microsegmentation on the server VLANs",
      "D": "Installing a firewall and making it the network core"
    },
    "correct_answer": "C",
    "explanation": "The goal is to implement a \"three-tier approach to separate the web, database, and application servers\" and \"harden the environment.\" This describes the need for strong internal network isolation between these tiers. **Microsegmentation (C)** is the best solution. Microsegmentation involves dividing a network into granular, isolated segments, down to individual workloads or applications. In a three-tier architecture, this means creating security policies that strictly control traffic flows between the web, application, and database tiers, ensuring that only necessary communication is allowed (e.g., web server only talks to app server on specific ports, app server only talks to database on specific ports). This significantly reduces the attack surface and prevents lateral movement if one tier is compromised, effectively hardening the environment as desired.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 129,
    "question_text": "A systems administrator wants to use existing resources to automate reporting from disparate security appliances that do not currently communicate. Which of the following is the best way to meet this objective?",
    "options": {
      "A": "Configuring an API integration to aggregate the different data sets",
      "B": "Combining back-end application storage into a single, relational database",
      "C": "Purchasing and deploying commercial off-the-shelf aggregation software",
      "D": "Migrating application usage logs to on-premises storage"
    },
    "correct_answer": "A",
    "explanation": "The problem states that disparate security appliances do not currently communicate, and the objective is to \"automate reporting\" using \"existing resources.\" The best way to achieve this is by **configuring an API integration to aggregate the different data sets (A)**. Many modern security appliances provide Application Programming Interfaces (APIs) that allow external systems to programmatically extract data. By writing custom scripts or using existing connectors that leverage these APIs, the systems administrator can pull data from each appliance and aggregate it into a central repository or reporting tool. This utilizes existing capabilities (the APIs) and automates the data collection for reporting, without necessarily requiring new commercial software or a fundamental change to storage.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 130,
    "question_text": "A vulnerability scan on a web server identified the following:\nTLS 1.2 Cipher Suites:\nThe server supported the following 4 cipher_suites:\nTLS_RSA_WITH_AES_128_CBC_SHA\nTLS_RSA_WITH_AES_256_CBC_SHA\nTLS_DHE_RSA_WITH_AES_128_CBC_SHA\nWhich of the following actions would most likely eliminate on-path decryption attacks? (Choose two.)",
    "options": {
      "A": "Disallowing cipher suites that use ephemeral modes of operation for key agreement",
      "B": "Removing support for CBC-based key exchange and signing algorithms",
      "C": "Adding TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA256",
      "D": "Implementing HIPS rules to identify and block BEAST attack attempts",
      "E": "Restricting cipher suites to only allow TLS_RSA_WITH_AES_128_CBC_SHA",
      "F": "Increasing the key length to 256 for TLS_RSA_WITH_AES_128_CBC_SHA"
    },
    "correct_answer": "B C",
    "explanation": "The vulnerability scan output lists several TLS 1.2 cipher suites, including those using `RSA_WITH_AES_128_CBC_SHA`, `RSA_WITH_AES_256_CBC_SHA`, and `DHE_RSA_WITH_AES_128_CBC_SHA`. The goal is to eliminate \"on-path decryption attacks.\"\n\n1.  **B. Removing support for CBC-based key exchange and signing algorithms:** Cipher Block Chaining (CBC) mode, when used with TLS 1.0/1.1 and certain implementation flaws, is vulnerable to attacks like BEAST and POODLE, which can enable on-path (man-in-the-middle) decryption of data. By removing support for these CBC-based ciphers, the server forces the use of more secure modes.\n2.  **C. Adding TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA256:** This cipher suite is a strong, modern choice. `ECDHE` (Elliptic Curve Diffie-Hellman Ephemeral) provides perfect forward secrecy, meaning a compromise of the server's long-term private key won't compromise past session keys, thus protecting against retroactive decryption. `AES_256_GCM` uses Galois/Counter Mode, which is an authenticated encryption mode that provides both confidentiality and integrity, and is not susceptible to the same padding oracle attacks as CBC. Adding and prioritizing such a robust cipher suite is essential for preventing on-path decryption attacks.",
    "is_simulation": false,
    "question_type": "multiple_choice_multiple_answer"
  },
  {
    "question_number": 131,
    "question_text": "A company’s help desk is experiencing a large number of calls from the finance department stating access issues to www.bank.com. The security operations center reviewed the following security logs:\nUser | User IP & Subnet | Location | Website | DNS Resolved IP (public) | HTTP Status Code\nUser12 | 10.200.2.52/24 | Finance | www.bank.com | 65.146.76.34 | 495\nUser31 | 10.200.2.213/24 | Finance | www.bank.com | 65.146.76.33 | 495\nUser46 | 10.200.5.76/24 | IT | www.bank.com | 98.17.62.79 | 200\nUser23 | 10.200.2.156/24 | Finance | www.bank.com | 65.146.76.34 | 495\nUser51 | 10.200.4.135/24 | Legal | www.bank.com | 98.17.62.79 | 200\nWhich of the following is most likely the cause of the issue?",
    "options": {
      "A": "Recursive DNS resolution is failing.",
      "B": "The DNS record has been poisoned.",
      "C": "DNS traffic is being sinkholed.",
      "D": "The DNS was set up incorrectly."
    },
    "correct_answer": "B",
    "explanation": "The security logs show multiple users attempting to access `www.bank.com` but their DNS lookups (`DNS Resolved IP`) are resolving to `65.146.76.34` or `65.146.76.33` (external IPs often associated with malicious activity), while the `HTTP Status Code` for these connections is `495` or `200` (which implies some response, not a complete failure to connect to *any* IP). For User3 and User4, the `DNS Resolved IP` is `98.17.62.79`, but their `HTTP Status Code` is `200`, suggesting access is allowed, perhaps to a different, potentially malicious, site.\n\nThis pattern, where a legitimate domain (`www.bank.com`) resolves to unusual or suspicious IP addresses that are not the genuine banking site's IPs, is a classic symptom of **DNS record poisoning (B)**. DNS poisoning (or cache poisoning) involves corrupting a DNS server's cache or a client's DNS resolution process, making it return an incorrect IP address for a domain. This can redirect users to a malicious site (phishing) or a sinkhole. The fact that the finance department, which deals with banking, is experiencing access issues is highly suspicious and consistent with DNS poisoning leading them to the wrong site.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 132,
    "question_text": "A financial services organization is using AI to fully automate the process of deciding client loan rates. Which of the following should the organization be most concerned about from a regulatory perspective?",
    "options": {
      "A": "Model explain ability",
      "B": "Credential theft",
      "C": "Possible prompt injections",
      "D": "Exposure to social engineering"
    },
    "correct_answer": "A",
    "explanation": "When an AI system automates a critical process like deciding client loan rates, especially in a regulated industry like financial services, **model explainability (A)** becomes a significant regulatory concern. Regulators (and ethics guidelines) often require transparency and the ability to understand *why* an AI model made a particular decision. If the AI model is a 'black box' and its decisions cannot be adequately explained, it becomes difficult to: \n*   Ensure fairness and prevent discrimination (e.g., if the model inadvertently biases against certain demographics).\n*   Comply with audit requirements.\n*   Challenge or appeal decisions (e.g., a denied loan application).\n*   Identify and correct errors or biases in the model. This lack of explainability can lead to significant compliance and legal risks.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 133,
    "question_text": "A security analyst is reviewing the following log:\nTime | File | Size | Antivirus status | Location\n11:25 | txt | 25mb | block | c:\\\n11:26 | dll | 10mb | allow | c:\\temp\\\n11:29 | docx | 2mb | block | c:\\users\\user1\\Desktop\n11:32 | pdf | 13mb | allow | c:\\users\\user2\\Downloads\n11:35 | txt | 40mb | allow | c:\\users\\user3\\Documents\nWhich of the following possible events should the security analyst investigate further?",
    "options": {
      "A": "A macro that was prevented from running",
      "B": "A text file containing passwords that were leaked",
      "C": "A malicious file that was run in this environment",
      "D": "A PDF that exposed sensitive information improperly"
    },
    "correct_answer": "C",
    "explanation": "The log entries show: `11:25 txt 25mb block c:\\` (text file blocked), `11:26 dll 10mb allow c:\\temp\\` (DLL allowed in temp), `11:29 docx 2mb block c:\\users\\user1\\Desktop` (docx blocked), `11:32 pdf 13mb allow c:\\users\\user2\\Downloads` (PDF allowed), `11:35 txt 40mb allow c:\\users\\user3\\Documents` (text file allowed). \n\nThe most concerning event to investigate further is `11:26 dll 10mb allow c:\\temp\\`. A `DLL` (Dynamic Link Library) file is a type of executable code. Allowing a DLL to run from a temporary directory (`c:\\temp\\`) is highly suspicious. Attackers often drop malicious DLLs into temporary or user-writable directories and then load them to execute malicious code, achieve persistence, or escalate privileges. The large size (10MB) for a DLL further adds to the suspicion. This strongly indicates that **a malicious file that was run in this environment (C)** is a possibility, as DLLs contain executable code.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 134,
    "question_text": "A security operations engineer needs to prevent inadvertent data disclosure when encrypted SSDs are reused within an enterprise. Which of the following is the most secure way to achieve this goal?",
    "options": {
      "A": "Executing a script that deletes and overwrites all data on the SSD three times",
      "B": "Wiping the SSD through degaussing",
      "C": "Securely deleting the encryption keys used by the SSD",
      "D": "Writing non-zero, random data to all cells of the SSD"
    },
    "correct_answer": "C",
    "explanation": "For **encrypted SSDs**, the most secure and effective way to prevent inadvertent data disclosure upon reuse is through **crypto-shredding**, which involves **securely deleting the encryption keys used by the SSD (C)**. SSDs are fundamentally different from HDDs in how data is stored and overwritten, making traditional data wiping (like overwriting multiple times or degaussing) less effective or even damaging to the drive. Many modern SSDs support built-in hardware encryption (often referred to as self-encrypting drives - SEDs). If data is encrypted at rest using a drive-specific encryption key, destroying that key makes the encrypted data mathematically unrecoverable, even if the data blocks themselves are still physically present on the drive. This is considered the most secure and efficient method for sanitizing encrypted SSDs.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 135,
    "question_text": "A security professional is investigating a trend in vulnerability findings for newly deployed cloud systems. Given the following output:\nDate | IP address | System | Finding | Criticality\n10/13/2023 | 10.123.34.98 | System1 | OpenSSL version 1.01 | Medium\n10/13/2023 | 10.3.114.72 | System6 | OpenSSL version 1.01 | Medium\n10/13/2023 | 10.12.134.45 | System12 | Old or unpatched version found | Medium\n10/13/2023 | 10.69.65.11 | System36 | OpenSSL version 1.01 | Medium\n10/13/2023 | 10.23.74.9 | System37 | Old or unpatched version found | Medium\n10/13/2023 | 10.13.124.3 | System45 | OpenSSL version 1.01 | Medium\nWhich of the following actions would address the root cause of this issue?",
    "options": {
      "A": "Automating the patching system to update base images",
      "B": "Recompiling the affected programs with the most current patches",
      "C": "Disabling unused/unneeded ports on all servers",
      "D": "Deploying a WAF with virtual patching upstream of the affected systems."
    },
    "correct_answer": "A",
    "explanation": "The output shows a trend of vulnerabilities in \"newly deployed cloud systems\" related to `OpenSSL version 1.01`, which is an \"old or unpatched version.\" This indicates that the base images or templates used to deploy these new cloud systems are outdated. To address the root cause and prevent future deployments from having these vulnerabilities, the most effective action is to **automate the patching system to update base images (A)**. This means ensuring that the golden images, AMIs, or container images used for deploying new cloud instances are regularly updated with the latest patches and secure configurations. When a new system is spun up from an updated base image, it will already have the necessary patches, preventing the recurrence of these \"old or unpatched version\" vulnerabilities upon deployment.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 136,
    "question_text": "A company established a new process for business analysts to receive emails that contain links for purchase requests. The new process requires links to be submitted through new emails. Which of the following is the best way to secure this process without disrupting order fulfillment?",
    "options": {
      "A": "Deploying a browser isolation solution",
      "B": "Blocking all potentially malicious links",
      "C": "Enforcing security awareness training",
      "D": "Implementing DNS filtering"
    },
    "correct_answer": "A",
    "explanation": "The challenge is to secure a process where business analysts receive emails with links for purchase requests, with a new requirement that links come via new emails, *without disrupting order fulfillment*. This implies that links must be accessible, but securely.\n\n**A. Deploying a browser isolation solution:** Browser isolation (also known as remote browser isolation - RBI) executes web browsing activity in an isolated environment (e.g., in a cloud-hosted container) separate from the user's local endpoint. Only a safe rendering of the web page (e.g., pixels or safe vector graphics) is streamed to the user's browser. If a malicious link is clicked, any malware or exploit code would execute within the isolated environment, not on the user's device, thus protecting the endpoint. This allows analysts to open and review purchase request links from emails securely, even if they lead to malicious content, minimizing disruption to their workflow while enhancing security.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 137,
    "question_text": "An organization receives OSINT reports about an increase in ransomware targeting fileshares at peer companies. The organization wants to deploy hardening policies to its servers and workstations in order to contain potential ransomware. Which of the following should an engineer do to best achieve this goal?",
    "options": {
      "A": "Allow only interactive log-in for users on workstations and restrict port 445 traffic to fileshares.",
      "B": "Enable biometric authentication mechanisms on user workstations and block port 53 traffic.",
      "C": "Instruct users to use a password manager when generating new credentials and secure port 443 traffic.",
      "D": "Give users permission to rotate administrator passwords and deny port 80 traffic."
    },
    "correct_answer": "A",
    "explanation": "The goal is to contain potential ransomware, especially targeting fileshares, by deploying hardening policies. Ransomware often spreads laterally through networks by exploiting vulnerabilities related to file sharing protocols. \n\n**A. Allow only interactive log-in for users on workstations and restrict port 445 traffic to fileshares.**\n*   **Restrict port 445 traffic to fileshares:** Port 445 is used by Server Message Block (SMB), the primary protocol for Windows file sharing. By restricting SMB traffic (port 445) only to authorized file shares and blocking it between other workstations (e.g., workstation-to-workstation SMB), the organization can significantly limit ransomware's ability to spread laterally once it infects an endpoint. This is a crucial network segmentation control for containing ransomware.\n*   **Allow only interactive log-in for users on workstations:** This implies users should not be logging into workstations as administrators or with highly privileged service accounts that have broad access to file shares, which ransomware often targets for wider impact.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 138,
    "question_text": "A malicious actor exploited firmware vulnerabilities and used rootkits in an attack on an organization. After the organization recovered from the incident, an engineer needs to recommend a solution that reduces the likelihood of the same type of attack in the future. Which of the following is the most relevant solution?",
    "options": {
      "A": "Enabling software integrity checks",
      "B": "Installing self-encrypting drives",
      "C": "Implementing measured boot",
      "D": "Configuring host-based encryption"
    },
    "correct_answer": "C",
    "explanation": "The attack involved \"firmware vulnerabilities\" and the use of \"rootkits.\" Rootkits are designed to hide their presence and maintain persistence at low levels of the operating system or even in firmware. To combat this type of attack, especially at the firmware level, **implementing measured boot (C)** is the most relevant solution.\n\nMeasured boot, often leveraging a Trusted Platform Module (TPM), creates a cryptographic measurement (hash) of each component in the boot process (firmware, bootloader, OS kernel, drivers) before it executes. These measurements are stored in the TPM and can be remotely attested (verified) by a trusted server. If any component in the boot chain, including firmware or OS loaders, has been tampered with by a rootkit, the measurements will differ, and the system or a monitoring service can detect the unauthorized modification. This provides a strong defense against firmware-level rootkits and ensures system integrity from boot-up.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 139,
    "question_text": "Which of the following enables the meaningful manipulation of encrypted data when the processor does not know the encryption key?",
    "options": {
      "A": "Simultaneous authentication of equals",
      "B": "Envelope encryption",
      "C": "Authenticated encryption with associated data",
      "D": "Homomorphic encryption"
    },
    "correct_answer": "D",
    "explanation": "**Homomorphic encryption (D)** is a form of encryption that allows computations to be performed on ciphertext (encrypted data) without decrypting it first. The results of these computations are encrypted and, when decrypted, match the results of computations performed on the plaintext. This property means that data can be meaningfully manipulated by a processor (or a cloud service, for example) *without* that processor ever having access to the decryption key or the plaintext data. This is a highly advanced cryptographic technique that enables privacy-preserving computation.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 140,
    "question_text": "Emails that the marketing department is sending to customers are going to the customers’ spam folders. The security team is investigating the issue and discovers that the certificates used by the email server were reissued, but DNS records had not been updated. Which of the following should the security team update in order to fix this issue? (Choose three.)",
    "options": {
      "A": "DMARC",
      "B": "SPF",
      "C": "DKIM",
      "D": "DNSSEC",
      "E": "SASE",
      "F": "SAN",
      "G": "SOA",
      "H": "MX"
    },
    "correct_answer": "A B C",
    "explanation": "Emails going to spam folders, especially after an email server change (implied by reissued certificates), is a classic symptom of misconfigured email authentication DNS records. The key protocols to address this are SPF, DKIM, and DMARC.\n\n1.  **B. SPF (Sender Policy Framework):** An SPF record (a TXT record in DNS) specifies which mail servers are authorized to send email on behalf of a domain. If the email server certificates were reissued, it often implies a change in the sending infrastructure. An outdated SPF record that doesn't include the new sending server's IP address or hostname will cause recipient mail servers to flag legitimate emails as spam.\n2.  **C. DKIM (DomainKeys Identified Mail):** DKIM allows the sender to cryptographically sign outgoing emails, and the recipient's mail server uses a public key (published in a DNS TXT record) to verify the signature. This confirms that the email has not been tampered with in transit and truly originates from the domain. If certificates were reissued, the DKIM keys or related DNS records might need updating.\n3.  **A. DMARC (Domain-based Message Authentication, Reporting, and Conformance):** DMARC builds upon SPF and DKIM. It tells receiving mail servers how to handle emails that fail SPF or DKIM checks (e.g., quarantine, reject, or allow) and provides reporting on email authentication failures. A DMARC record (also a DNS TXT record) relies on SPF and DKIM to be correctly configured. If SPF or DKIM is broken, DMARC will cause emails to be flagged as spam or rejected. Updating SPF and DKIM is necessary for DMARC to function correctly.",
    "is_simulation": false,
    "question_type": "multiple_choice_multiple_answer"
  },
  {
    "question_number": 141,
    "question_text": "A security engineer performed a code scan that resulted in many false positives. The security engineer must find a solution that improves the quality of scanning results before application deployment. Which of the following is the best solution?",
    "options": {
      "A": "Limiting the tool to a specific coding language and tuning the rule set",
      "B": "Configuring branch protection rules and dependency checks",
      "C": "Using an application vulnerability scanner to identify coding flaws in production",
      "D": "Performing updates on code libraries before code development"
    },
    "correct_answer": "A",
    "explanation": "The problem is that a code scan resulted in \"many false positives,\" and the goal is to \"improve the quality of scanning results.\" \n\n**A. Limiting the tool to a specific coding language and tuning the rule set:** Code scanning tools (like SAST) can be complex and may generate many false positives if not properly configured. \n*   **Limiting to a specific coding language:** Ensuring the scanner is configured for the exact language(s) used in the application reduces false positives that might arise from misinterpreting code written in other languages or general rules.\n*   **Tuning the rule set:** This involves customizing the scanner's rules to be more relevant to the specific application's context, development standards, and known frameworks. It means disabling rules that are not applicable, adjusting sensitivity, or creating custom rules, thereby significantly reducing the number of irrelevant or inaccurate findings (false positives) and improving the overall quality and actionable nature of the scan results.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 142,
    "question_text": "A global company with a remote workforce implemented a new VPN solution. After deploying the VPN solution to several hundred users, the help desk starts receiving reports of slow access to both internally and externally available applications. A security analyst reviews the following:\nVPN client routing:\n0.0.0.0/0 eth1\nWhich of the following solutions should the analyst use to fix this issue?",
    "options": {
      "A": "Move the servers to a screened subnet.",
      "B": "Enable split tunneling.",
      "C": "Configure an NAC solution.",
      "D": "Implement DNS over HTTPS."
    },
    "correct_answer": "B",
    "explanation": "The key information is \"slow access to both internally and externally available applications\" for remote VPN users, and the routing configuration `0.0.0.0/0 eth1`. This indicates that all traffic from the VPN client is being forced through the VPN tunnel (`0.0.0.0/0` means all traffic, and `eth1` is likely the VPN tunnel interface). This is known as **full tunneling**. While full tunneling provides strong security (all traffic is inspected by corporate defenses), it can severely degrade performance for internet-bound traffic, as it has to travel to the corporate network and then out to the internet, adding latency and potentially saturating the VPN concentrator's bandwidth.\n\nTo fix the slow access, especially for externally available applications, the analyst should **enable split tunneling (B)**. Split tunneling allows certain traffic (typically internet-bound traffic) to bypass the VPN tunnel and go directly to its destination, while corporate network traffic continues to go through the VPN. This improves performance for users accessing external resources without compromising security for internal corporate resources.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 143,
    "question_text": "A security analyst is reviewing suspicious log-in activity and sees the following data in the SIEM:\nAccount | Application | Authorization source | Status | Risk\nSALES1 | Customer manager | LDAP-US | Success | Low\nSALES1 | Payroll | LDAP-US | Failure | High\nADMIN1 | Email | LDAP-US | Unknown | Unknown\nSALES1 | Email | LDAP-US | Success | Low\nMARKET1 | Customer manager | LDAP-US | Success | Low\nFINANCE1 | Payroll | LDAP-EU | Unknown | Unknown\nWhich of the following is the most appropriate action for the analyst to take?",
    "options": {
      "A": "Update the log configuration settings on the directory server that is not being captured properly.",
      "B": "Have the admin account owner change their password to avoid credential stuffing.",
      "C": "Block employees from logging in to applications that are not part of their business area.",
      "D": "Implement automation to disable accounts that have been associated with high-risk activity."
    },
    "correct_answer": "D",
    "explanation": "The SIEM data shows highly suspicious login activity:\n\n*   `SALES1`, `ADMIN1`, `FINANCE1` accounts show a mix of Success, Failure, and `Unknown` statuses.\n*   The `ADMIN1` account attempting to log into `Email` with `Unknown` status and `Unknown` risk is concerning.\n*   `FINANCE1` accessing `Payroll` from `LDAP-EU` with `Unknown` status and `Unknown` risk is also very suspicious, especially if `LDAP-EU` is unusual or the user is not typically assigned to `Payroll`.\n\nThe presence of `Unknown` status and `Unknown` risk for sensitive applications (Email, Payroll) often indicates a problem with data parsing or a non-standard event, but in the context of \"suspicious log-in activity,\" it warrants immediate action. The most appropriate proactive and automated action to mitigate the immediate threat and prevent further damage is to **implement automation to disable accounts that have been associated with high-risk activity (D)**. This prevents attackers from continuously leveraging potentially compromised accounts. Such automation, often part of SOAR or identity governance solutions, is critical for rapid response in large environments.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 144,
    "question_text": "An organization determined its preparedness for a ransomware attack is inadequate. A security administrator is working on ways to improve and monitor the organization's response to ransomware attacks. Which of the following is the best action for the administrator to take?",
    "options": {
      "A": "Conduct backup testing.",
      "B": "Define the recovery point objective.",
      "C": "Perform a business impact analysis.",
      "D": "Verify the encryption key length."
    },
    "correct_answer": "A",
    "explanation": "If an organization's preparedness for a ransomware attack is inadequate, and the administrator wants to improve and monitor the *response*, a crucial element of ransomware recovery is reliable data restoration. The best action to ensure this is **conduct backup testing (A)**. Regular testing of backups verifies that data can indeed be restored successfully and within acceptable recovery time objectives (RTOs). In a ransomware attack, often the only way to recover data without paying the ransom is from clean backups. Without testing, backups are just an assumption, not a reliable recovery method. Options B, C, and D are important for planning and prevention, but testing backups directly addresses the response capability for recovery.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 145,
    "question_text": "A security engineer receives an alert from the SIEM platform indicating a possible malicious action on the internal network. The engineer generates a report that outputs the logs associated with the incident:\nDate | Time | Action | Details\n01/23/2024 | 08:02:41 | Login success | JohnS login attempt into VM001\n01/24/2024 | 08:01:32 | Login success | JohnS login attempt into SV002\n01/25/2024 | 08:02:12 | Login success | JohnS login attempt into VM001\n01/26/2024 | 08:03:21 | Login success | JohnS login attempt into VM001\n01/26/2024 | 23:52:41 | Login success | JohnS login attempt into SV002\n01/27/2024 | 08:02:54 | Login success | JohnS login attempt into SV002\nWhich of the following actions best enables the engineer to investigate further?",
    "options": {
      "A": "Consulting logs from the enterprise password manager",
      "B": "Searching dark web monitoring resources for exposure",
      "C": "Reviewing audit logs from privileged actions",
      "D": "Querying user behavior analytics data"
    },
    "correct_answer": "C",
    "explanation": "The logs show repeated successful logins by user `JohnS` on `VM001` and `SV002` at various times, including off-hours. The SIEM alert indicates a \"possible malicious action.\" To investigate what specific malicious action `JohnS` might have performed after these successful logins, the most direct and fruitful source of information would be **reviewing audit logs from privileged actions (C)**. \n\nIf `JohnS` performed a malicious action on the internal network, these actions would likely involve access to sensitive resources, configuration changes, or data manipulation, all of which are often logged as privileged actions. Reviewing these detailed audit logs would reveal the specific commands executed, files accessed, or system changes made by the user, providing concrete evidence for the investigation. User behavior analytics (D) could indicate suspicious patterns, but audit logs of privileged actions provide the specific 'what' that occurred.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 146,
    "question_text": "A security engineer must integrate device attestation into user authentication and authorization workflows for mobile devices. Which of the following best meets the requirements?",
    "options": {
      "A": "Enforcing a security boundary for all devices outside the perimeter network",
      "B": "Enabling multifactor authentication using biometrics on access attempts",
      "C": "Implementing single sign-on to centralize access control enforcement",
      "D": "Configuring device profiling for patch level and jailbreak status"
    },
    "correct_answer": "D",
    "explanation": "Device attestation is the process of verifying the integrity and trustworthiness of a device before granting it access to resources. To integrate device attestation into user authentication and authorization workflows for mobile devices, it requires assessing the device's security posture. **Configuring device profiling for patch level and jailbreak status (D)** directly aligns with this. Device profiling involves collecting information about the device's attributes, such as:\n\n*   **Patch level:** Ensuring the device has critical security updates installed.\n*   **Jailbreak/root status:** Detecting if the device has been tampered with or compromised, which would reduce its trustworthiness.\n\nThis collected profile data can then be used by the authentication/authorization system to make contextual access decisions – only allowing devices that meet the required security baseline to access sensitive resources. Options A, B, and C are broader security controls that don't directly implement device attestation details.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 147,
    "question_text": "An organization is developing an AI-enabled digital worker to help employees complete common tasks, such as template development, editing, research, and scheduling. As part of the AI workload, the organization wants to implement guardrails within the platform. Which of the following should the company do to secure the AI environment?",
    "options": {
      "A": "Limit the platform's abilities to only non-sensitive functions.",
      "B": "Enhance the training model's effectiveness.",
      "C": "Grant the system the ability to self-govern.",
      "D": "Require end-user acknowledgement of organizational policies."
    },
    "correct_answer": "A",
    "explanation": "When deploying an AI-enabled digital worker, especially one assisting with tasks like research (which might involve sensitive data) and scheduling (which might interact with sensitive calendars/contacts), implementing \"guardrails\" is crucial for security. The most effective security measure is to **limit the platform's abilities to only non-sensitive functions (A)**. This aligns with the principle of least privilege. By restricting the AI's access and capabilities to only what is absolutely necessary for its defined non-sensitive tasks, the potential impact of a compromise (e.g., if the AI is tricked via a prompt injection) or an AI error is minimized. If the AI does not have access to sensitive data or the ability to perform sensitive actions, the risk of it inadvertently or maliciously exposing or manipulating sensitive information is greatly reduced.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 148,
    "question_text": "A security analyst discovered requests associated with IP addresses known for both legitimate and bot-related traffic. Which of the following should the analyst use to determine whether the requests are malicious?",
    "options": {
      "A": "User-agent string",
      "B": "Byte length of the request",
      "C": "Web application headers",
      "D": "HTML encoding field"
    },
    "correct_answer": "A",
    "explanation": "The analyst needs to differentiate between legitimate and bot-related traffic, especially from IPs that could be associated with either. The **User-Agent string (A)** in HTTP requests is often the most useful piece of information for this. A User-Agent string identifies the client software (e.g., browser, bot, application) making the request. Malicious bots often use distinct or easily identifiable User-Agent strings (e.g., generic or missing User-Agents, or specific strings associated with known botnets/scanners) that differ from typical web browsers. While legitimate bots (like search engine crawlers) also have unique User-Agent strings, the analyst can use this field to filter, identify, and potentially block or flag suspicious bot activity, helping determine if the traffic is malicious.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 149,
    "question_text": "A security analyst received a report that an internal web page is down after a company-wide update to the web browser. Given the following error message:\nYour connection is not private.\nAttackers might be trying to steal your information for www. internalwebsite.company.com.\nNET::ERR_CERT_WEAK_SIGNATURE_ALGORITHM\nWhich of the following is the best way to fix this issue?",
    "options": {
      "A": "Rewriting any legacy web functions",
      "B": "Disabling all deprecated ciphers",
      "C": "Blocking all non-essential ports",
      "D": "Discontinuing the use of self-signed certificates"
    },
    "correct_answer": "B",
    "explanation": "The error message `NET::ERR_CERT_WEAK_SIGNATURE_ALGORITHM` is the key indicator. This error occurs when the web browser encounters an SSL/TLS certificate that is signed using a cryptographic algorithm (like SHA-1) that is considered weak or deprecated. Modern browsers have stopped trusting certificates signed with these algorithms due to security vulnerabilities. \n\nThe best way to fix this is to **disable all deprecated ciphers (B)** and cryptographic algorithms on the web server and ensure it is configured to use only strong, modern algorithms for its SSL/TLS certificates and connections. While reissuing the certificate with a stronger algorithm is implied, the root cause is the server supporting or using deprecated ciphers/algorithms. Disabling these ensures that the server can only negotiate secure connections that modern browsers will trust.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 150,
    "question_text": "A company receives reports about misconfigurations and vulnerabilities in a third-party hardware device that is part of its released products. Which of the following solutions is the best way for the company to identify possible issues at an earlier stage?",
    "options": {
      "A": "Performing vulnerability tests on each device delivered by the providers",
      "B": "Performing regular red-team exercises on the vendor production line",
      "C": "Implementing a monitoring process for the integration between the application and the vendor appliance",
      "D": "Implementing a proper supply chain risk management program"
    },
    "correct_answer": "D",
    "explanation": "The problem describes issues (misconfigurations, vulnerabilities) in a \"third-party hardware device that is part of its released products.\" The goal is to identify these issues at an *earlier stage*. This points to the need for a comprehensive approach to managing risks from external components.\n\n**D. Implementing a proper supply chain risk management program:** This is the most comprehensive and effective solution. A supply chain risk management program focuses on identifying, assessing, and mitigating risks introduced by third-party vendors and components throughout the entire product lifecycle, from design and manufacturing to delivery and maintenance. This program would involve due diligence on vendors, security assessments of components, contractual requirements for security, and continuous monitoring, allowing the company to identify and address misconfigurations and vulnerabilities much earlier than when they are already part of the released products.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 151,
    "question_text": "A company implemented a new NAC solution based on 802.1X. However, the IT support team notices that some devices are not being enrolled in the new policies, causing access disruptions for key users. Which of the following solutions will most likely solve this issue and prevent reoccurrence?",
    "options": {
      "A": "Include the monitoring agent and digital certificate as part of the patching/updating program, keeping all the corporate devices updated and enrolled.",
      "B": "Check whether the certificate is signed by a certification authority and manually deployed to each device.",
      "C": "Check all the devices without proper access, enrolling via the solution agent and authenticating to the network.",
      "D": "Implement default credentials to automate RADIUS authentication and grant access to the network if the device owner is an employee."
    },
    "correct_answer": "A",
    "explanation": "The problem states that a new 802.1X NAC solution is causing \"access disruptions for key users\" because \"some devices are not being enrolled in the new policies.\" This means these devices are failing to meet the NAC's requirements, likely due to missing agents or certificates required for 802.1X authentication. To solve this and prevent reoccurrence in a scalable and automated way:\n\n**A. Include the monitoring agent and digital certificate as part of the patching/updating program, keeping all the corporate devices updated and enrolled.** This ensures that: \n*   The necessary NAC monitoring agent (if required) is automatically deployed and updated.\n*   The digital certificates required for 802.1X authentication (e.g., machine certificates) are automatically provisioned, renewed, and maintained on all corporate devices.\nBy integrating these components into the existing, presumably robust, patching and updating program, the organization ensures that devices remain compliant with NAC policies automatically, preventing future enrollment issues and access disruptions. This is a proactive and automated management approach.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 152,
    "question_text": "While performing threat-hunting functions, an analyst is using the Diamond Model of Intrusion Analysis. The analyst identifies the likely adversary, the infrastructure involved, and the target. Which of the following must the threat hunter document to use the model effectively?",
    "options": {
      "A": "Knowledge",
      "B": "Capabilities",
      "C": "Phase",
      "D": "Methodologies"
    },
    "correct_answer": "B",
    "explanation": "The Diamond Model of Intrusion Analysis has four core features (or facets) that describe any intrusion event: **Adversary, Infrastructure, Victim, and Capabilities**. The analyst has already identified the Adversary, Infrastructure, and Target (Victim). To use the model effectively, the threat hunter must also document the **Capabilities (B)**. Capabilities refer to the adversary's tools and techniques used to execute the intrusion. This includes the adversary's software (malware, tools), hardware (custom devices), and methods (TTPs - tactics, techniques, and procedures). Understanding these capabilities is essential for a complete intrusion analysis within the Diamond Model framework.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 153,
    "question_text": "A systems administrator needs to improve the security assurance in a company's cloud storage environment. The administrator determines that the best approach is to identify whether any data has been maliciously or inadvertently modified. Which of the following techniques should the systems administrator periodically use?",
    "options": {
      "A": "Interference",
      "B": "Antitampering",
      "C": "Hashing",
      "D": "Journaling"
    },
    "correct_answer": "C",
    "explanation": "To identify whether any data has been maliciously or inadvertently modified, the most direct and effective technique is **hashing (C)**. Hashing involves computing a fixed-size unique digital fingerprint (hash value) of a data set. If even a single bit of the data is changed, the computed hash will be completely different. By periodically computing and comparing hashes of data in the cloud storage environment against a known good baseline, the administrator can detect any unauthorized modifications, thereby ensuring data integrity and improving security assurance. This is a fundamental principle of file integrity monitoring.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 154,
    "question_text": "A security engineer wants to enhance the security posture of end-user systems in a zero trust environment. Given the following requirements:\n• Reduce the ability for potentially compromised endpoints to contact C2 infrastructure.\n• Track the requests that the malware makes to the IPs.\n• Avoid the download of additional payloads.\nWhich of the following should the engineer deploy to meet these requirements?",
    "options": {
      "A": "DNS sinkholing",
      "B": "Browser isolation",
      "C": "Zone transfer protection",
      "D": "HIDS"
    },
    "correct_answer": "A",
    "explanation": "The requirements revolve around controlling malicious network communications, specifically command-and-control (C2) traffic and additional payload downloads, and tracking malware requests.\n\n**A. DNS sinkholing:** A DNS sinkhole redirects malicious DNS queries (e.g., for C2 domains or malware distribution sites) to a non-existent or controlled IP address. This directly addresses:\n*   **Reduce the ability for potentially compromised endpoints to contact C2 infrastructure:** By redirecting malicious domains, the compromised endpoint cannot reach the actual C2 server.\n*   **Track the requests that the malware makes to the IPs:** The sinkhole logs all attempts to resolve the malicious domains, providing a record of infected endpoints and their C2 attempts.\n*   **Avoid the download of additional payloads:** If the malware tries to resolve a domain to download further components, the sinkhole prevents it from reaching the legitimate download source.\n\nDNS sinkholing is a powerful and effective technique for disrupting and monitoring malicious network communications.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 155,
    "question_text": "Developers have been creating and managing cryptographic material on their personal laptops for use in the production environment. A security engineer needs to initiate a more secure process. Which of the following is the best strategy for the engineer to use?",
    "options": {
      "A": "Disabling the BIOS and moving to UEFI",
      "B": "Managing secrets on the vTPM hardware",
      "C": "Employing shielding to prevent EMI",
      "D": "Managing key material on a HSM"
    },
    "correct_answer": "D",
    "explanation": "Storing and managing cryptographic material (like private keys or certificates) on personal laptops is highly insecure for production use. The best strategy to initiate a more secure process is to use a dedicated hardware security solution. **Managing key material on an HSM (Hardware Security Module) (D)** is the industry best practice. An HSM is a physical computing device that safeguards and manages digital keys, performs cryptographic operations (like encryption, decryption, signing), and provides a tamper-resistant environment. HSMs are designed to protect keys from unauthorized access, use, and extraction, and they often meet high-security certifications (e.g., FIPS 140-2). This significantly improves the security of cryptographic material compared to storing it on developer laptops.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 156,
    "question_text": "A nation-state actor is exposed for attacking large corporations by establishing persistence in smaller companies that are likely to be acquired by these large corporations. The actor then provisions user accounts in the companies for use post-acquisition. Before an upcoming acquisition, a security officer conducts threat modeling with this attack vector. Which of the following practices is the best way to investigate this threat?",
    "options": {
      "A": "Restricting internet traffic originating from countries in which the nation-state actor is known to operate",
      "B": "Comparing all existing credentials to personnel and services",
      "C": "Auditing vendors to mitigate supply chain risk during the acquisition",
      "D": "Placing a hold on all information about corporate interest in acquisitions"
    },
    "correct_answer": "B",
    "explanation": "The nation-state actor's specific technique is to \"provision user accounts in the companies for use post-acquisition.\" This means they are creating backdoor accounts or compromising legitimate accounts to maintain access. Before an acquisition, to investigate this threat effectively, the security officer should **compare all existing credentials to personnel and services (B)**. This involves a comprehensive audit of all user accounts (both human and service accounts) against known legitimate personnel records and approved services. This process can help identify:\n\n*   Accounts that do not correspond to any current employee or service.\n*   Accounts with unusual permissions or activity patterns.\n*   Accounts that might be duplicates or have suspicious creation dates.\n\nThis kind of detailed credential analysis is crucial for detecting unauthorized persistent access mechanisms like rogue user accounts that a nation-state actor might deploy.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 157,
    "question_text": "After an incident response exercise, a security administrator reviews the following table:\nService | Risk rating | Criticality rating | Alert severity\nPublic website | Medium | High | High\nEmail | High | High | High\nHuman resources system | High | Medium | Medium\nPhone system | High | Critical | Critical\nIntranet | Low | Low | Low\nWhich of the following should the administrator do to best support rapid incident response in the future?",
    "options": {
      "A": "Automate alerting to IT support for phone system outages.",
      "B": "Enable dashboards for service status monitoring.",
      "C": "Send emails for failed log-in attempts on the public website.",
      "D": "Configure automated isolation of human resources systems."
    },
    "correct_answer": "B",
    "explanation": "The table provides a breakdown of various services, their risk rating, criticality rating, and alert severity. The goal is to \"best support rapid incident response in the future.\" \n\n**B. Enable dashboards for service status monitoring:** Dashboards provide real-time, aggregated visibility into the health and status of critical services, often color-coded by severity or criticality (as indicated in the table). By having clear dashboards, security and operations teams can quickly identify which services are experiencing issues, understand their criticality and associated risk, and therefore prioritize response efforts effectively. This centralized, visual representation enables rapid initial assessment and facilitates quicker decision-making during an incident, which is fundamental to rapid incident response.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 158,
    "question_text": "An organization is required to:\n• Respond to internal and external inquiries in a timely manner.\n• Provide transparency.\n• Comply with regulatory requirements.\nThe organization has not experienced any reportable breaches but wants to be prepared if a breach occurs in the future. Which of the following is the best way for the organization to prepare?",
    "options": {
      "A": "Outsourcing the handling of necessary regulatory filings to an external consultant",
      "B": "Integrating automated response mechanisms into the data subject access request process",
      "C": "Developing communication templates that have been vetted by internal and external counsel",
      "D": "Conducting lessons-learned activities and integrating observations into the crisis management plan"
    },
    "correct_answer": "C",
    "explanation": "The organization's requirements revolve around timely, transparent, and compliant responses to breaches. The best way to prepare *before* a breach occurs, especially concerning communication and regulatory compliance, is to **develop communication templates that have been vetted by internal and external counsel (C)**. \n\nHaving pre-approved templates for various scenarios (e.g., data breach notifications, public statements, internal communications) ensures that when a breach occurs:\n*   Communications are legally sound and comply with regulatory requirements.\n*   Transparency is maintained through consistent messaging.\n*   Responses can be issued quickly, addressing the 'timely manner' requirement.\n\nThis proactive step avoids delays and potential legal pitfalls that can arise from scrambling to draft communications during a crisis. While other options are also important aspects of preparedness, templates directly address the communication and compliance needs.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 159,
    "question_text": "An incident response team is analyzing malware and observes the following:\n• Does not execute in a sandbox\n• No network IoCs\n• No publicly known hash match\n• No process injection method detected\nWhich of the following should the team do next to proceed with further analysis?",
    "options": {
      "A": "Use an online virus analysis tool to analyze the sample.",
      "B": "Check for an anti-virtualization code in the sample.",
      "C": "Utilize a new deployed machine to run the sample.",
      "D": "Search other internal sources for a new sample."
    },
    "correct_answer": "B",
    "explanation": "The most significant observation is that the malware \"Does not execute in a sandbox.\" This is a common tactic used by sophisticated malware to evade detection and analysis. Malware often includes **anti-virtualization (or anti-sandbox) code (B)** that detects if it is running in a virtualized environment or a sandbox. If such an environment is detected, the malware will cease execution, behave benignly, or self-destruct, preventing analysts from observing its true malicious behavior. Therefore, the next logical step for the incident response team is to investigate the sample for the presence of anti-virtualization techniques to understand why it isn't executing in the sandbox.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 160,
    "question_text": "Which of the following best explains the business requirement a healthcare provider fulfills by encrypting patient data at rest?",
    "options": {
      "A": "Securing data transfer between hospitals",
      "B": "Providing for non-repudiation of data",
      "C": "Reducing liability from identity theft",
      "D": "Protecting privacy while supporting portability"
    },
    "correct_answer": "D",
    "explanation": "Encrypting patient data at rest primarily fulfills the business requirement of **protecting privacy while supporting portability (D)**. \n\n*   **Protecting privacy:** Encryption is a fundamental control for maintaining the confidentiality and privacy of sensitive patient data (PHI). If the storage media is lost or stolen, the encrypted data remains unreadable to unauthorized parties.\n*   **Supporting portability:** Encrypting data at rest (e.g., on a laptop, USB drive, or cloud storage) makes it safer to store and transport that data without risking exposure in case of physical loss. This allows the organization to store and move patient data as needed for business operations (portability) while ensuring that it remains secure and private. This directly addresses regulatory requirements like HIPAA which mandate protection of PHI.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 161,
    "question_text": "A security engineer is implementing security measures on new hardware in preparation for its launch. During the development phase, a risk related to protections at the UEFI level was found. Which of the following should the engineer recommend to reduce this risk?",
    "options": {
      "A": "Configuring paravirtualization protection",
      "B": "Enabling Secure Boot",
      "C": "Installing cryptography at the operational system level",
      "D": "Implementing hardware root of trust"
    },
    "correct_answer": "B",
    "explanation": "The risk is specifically related to \"protections at the UEFI level.\" UEFI (Unified Extensible Firmware Interface) is the modern firmware interface for computers. To reduce risk at this level, **enabling Secure Boot (B)** is the most direct and effective measure. Secure Boot is a feature of UEFI firmware that ensures only authenticated (signed) operating systems and drivers can load during the boot process. It prevents malicious rootkits or unauthorized software from loading before the operating system, thereby protecting the integrity of the boot chain from the firmware up.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 162,
    "question_text": "A development team must create a website to share indicators of compromise. The team wants to use APIs between industry peers to aid in configuring SIEM and SOAR. The team needs to create a free tier of service, and the senior developer insists on configuring rate limiting. Which of the following best describes the senior developer's reasoning?",
    "options": {
      "A": "To prevent password-spraying attacks on the services hosting the API",
      "B": "To limit the likelihood of resource exhaustion occurring on the API server",
      "C": "To address concerns the team has about API bandwidth utilization",
      "D": "To reduce attack surface exposure of the API endpoints connecting peers"
    },
    "correct_answer": "B",
    "explanation": "The senior developer's insistence on configuring rate limiting for an API, especially one offering a \"free tier of service,\" is primarily aimed at protecting the API server's stability and availability. **To limit the likelihood of resource exhaustion occurring on the API server (B)** is the best description of this reasoning. Rate limiting controls the number of requests a client can make to an API within a given time frame. Without it, a single client (malicious or even a misconfigured legitimate client) could make an excessive number of requests, consuming all available server resources (CPU, memory, network bandwidth), leading to a Denial of Service (DoS) for other users and potentially crashing the server. This is critical for maintaining service availability, especially for a free tier where usage might be unpredictable.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 163,
    "question_text": "A hotel chain wants to use point-of-sale systems to allow customers to check in and out of their rooms without employee assistance. These systems should limit access to a specific set of programs approved to run, with all other programs blocked. Which of the following should the company configure to best support this goal?",
    "options": {
      "A": "Application control using a fresh image, with the applications fully configured as a baseline to build and block other applications from execution",
      "B": "A host-based intrusion detection system to monitor and block all suspicious activities if they occur on the systems",
      "C": "Anti-malware on these systems and only approved application file locations can be bypassed",
      "D": "Event logs to be collected from the systems for all security events and some custom application logs"
    },
    "correct_answer": "A",
    "explanation": "The core requirement is to \"limit access to a specific set of programs approved to run, with all other programs blocked.\" This is a classic definition of **application control** (also known as application whitelisting). \n\n**A. Application control using a fresh image, with the applications fully configured as a baseline to build and block other applications from execution:** This option describes the implementation of application control. By creating a \"fresh image\" with only the approved POS applications, this forms a trusted baseline. Application control then ensures that only executables explicitly defined in this baseline (or through subsequent authorized updates) are allowed to run, blocking everything else. This approach is highly effective for securing fixed-function devices like POS systems against unauthorized software, malware, and tampering.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 164,
    "question_text": "A user reports application access issues to the help desk. The help desk reviews the logs for the user:\nTime | Internal IP | Public IP | Geolocation | Application | Action\n8:47 p.m. | 192.168.1.5 | 104.18.16.29 | Toronto | VPN | Allow\n8:48 p.m. | 10.10.2.21 | 95.67.137.12 | Los Angeles | Email | Allow\n8:48 p.m. | 10.10.2.21 | 95.67.137.12 | Los Angeles | Human resources system | Deny\n8:49 p.m. | 10.10.2.21 | 95.67.137.12 | Los Angeles | Email | Allow\n8:52 p.m. | 192.168.1.5 | 104.18.16.29 | Toronto | Human resources system | Deny\nWhich of the following is most likely the reason for the issue?",
    "options": {
      "A": "The user inadvertently tripped the geoblock rule in NGFW.",
      "B": "A threat actor has compromised the user's account and attempted to log in.",
      "C": "The user is not allowed to access the human resources system outside of business hours.",
      "D": "The user did not attempt to connect from an approved subnet."
    },
    "correct_answer": "B",
    "explanation": "The logs show suspicious activity:\n\n*   An internal IP (`192.168.1.5`) accessing `VPN` and `Human resources system` from `Toronto` (some allowed, some denied).\n*   Crucially, `10.10.2.21` (internal IP) accessing `Email` from `Los Angeles` (`95.67.137.12` public IP) is `Allow`ed.\n*   However, the *same internal IP* (`10.10.2.21`) attempting to access the `Human resources system` from the *same external Los Angeles IP* (`95.67.137.12`) is `Deny`ed.\n\nThis pattern, where an internal user's credentials (implied by internal IP being the source) are used from an external public IP to access some applications (Email) but denied for a more sensitive one (HR System), strongly suggests that **a threat actor has compromised the user's account and attempted to log in (B)**. The threat actor is likely using the compromised credentials from an external location. The denial to the HR system could be due to stricter policies for that application, but the activity from an external IP for an internal user's account is the primary indicator of compromise. If it were a simple geoblock (A), the email access might have been denied as well. Business hours restriction (C) is possible, but less likely given the email access. Non-approved subnet (D) is not clearly supported as the IPs are distinct.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 165,
    "question_text": "A company's security team is notified about vulnerabilities in the company's application. The security team determined these vulnerabilities were previously disclosed in third-party libraries. Which of the following solutions best allows the company to identify third-party vulnerabilities in the future?",
    "options": {
      "A": "Using IaC to include the newest dependencies",
      "B": "Creating a bug bounty program",
      "C": "Implementing a continuous security assessment program",
      "D": "Integrating a SCA tool as part of the pipeline"
    },
    "correct_answer": "D",
    "explanation": "The problem specifically identifies vulnerabilities in \"third-party libraries\" that were \"previously disclosed.\" To identify these types of vulnerabilities in the future, particularly those stemming from open-source or commercial components, the most effective solution is to **integrate a SCA (Software Composition Analysis) tool as part of the pipeline (D)**.\n\nSCA tools analyze an application's codebase to identify all third-party components and their dependencies. They then compare these components against databases of known vulnerabilities (e.g., CVEs). Integrating SCA into the CI/CD pipeline (e.g., during build or commit) ensures that vulnerabilities in third-party libraries are automatically detected early in the development lifecycle, before deployment, addressing the root cause of the issue.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 166,
    "question_text": "A global organization wants to manage all endpoint and user telemetry. The organization also needs to differentiate this data based on which office it is correlated to. Which of the following strategies best aligns with this goal?",
    "options": {
      "A": "Sensor placement",
      "B": "Data labeling",
      "C": "Continuous monitoring",
      "D": "Centralized logging"
    },
    "correct_answer": "B",
    "explanation": "The goal is to \"manage all endpoint and user telemetry\" and to \"differentiate this data based on which office it is correlated to.\" This points to the need for clear categorization and attribution of data.\n\n**B. Data labeling (B)** is the best strategy. Data labeling involves attaching metadata (tags, labels) to telemetry data points that indicate their origin, context, or classification. In this scenario, telemetry could be labeled with attributes like `office_location: [OfficeName]`, `endpoint_group: [Group]`, or `user_department: [Department]`. Once labeled, the data can be easily filtered, correlated, and analyzed based on the office it came from, allowing for efficient management and differentiation of telemetry from various locations. While centralized logging (D) is necessary for collection, labeling enables the desired differentiation and correlation.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 167,
    "question_text": "A security architect must make sure that the least number of services as possible is exposed in order to limit an adversary's ability to access the systems. Which of the following should the architect do first?",
    "options": {
      "A": "Enforce Secure Boot.",
      "B": "Perform attack surface reduction.",
      "C": "Disable third-party integrations.",
      "D": "Limit access to the systems."
    },
    "correct_answer": "B",
    "explanation": "The objective is to \"limit an adversary's ability to access the systems\" by ensuring \"the least number of services as possible is exposed.\" This directly describes the concept of **attack surface reduction (B)**. Attack surface reduction involves minimizing the ways an attacker can interact with a system, which includes disabling unnecessary services, closing unused ports, removing unnecessary software, and restricting access points. It's a foundational security principle that should be prioritized because a smaller attack surface means fewer opportunities for exploitation. While limiting access (D) is related, attack surface reduction is a broader strategy that encompasses reducing the available access points in the first place.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 168,
    "question_text": "A security officer performs due diligence activities before implementing a third-party solution into the enterprise environment. The security officer needs evidence from the third party that a data subject access request handling process is in place. Which of the following is the security officer most likely seeking to maintain compliance?",
    "options": {
      "A": "Information security standards",
      "B": "E-discovery requirements",
      "C": "Privacy regulations",
      "D": "Certification requirements",
      "E": "Reporting frameworks"
    },
    "correct_answer": "C",
    "explanation": "The security officer is seeking evidence of a \"data subject access request handling process\" from a third-party solution. A data subject access request (DSAR) is a right granted to individuals under various **privacy regulations (C)**, such as GDPR (General Data Protection Regulation) or CCPA (California Consumer Privacy Act). These regulations give individuals the right to request access to, correction of, or deletion of their personal data held by an organization (and its processors). Therefore, the security officer is most likely performing due diligence to ensure the third-party solution (which will likely process personal data) can help the organization maintain compliance with relevant privacy regulations.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 169,
    "question_text": "An administrator needs to craft a single certificate-signing request for a web-server certificate. The server should be able to use the following identities to mutually authenticate other resources over TLS:\n• www.int.comptia.org\n• webserver01 .int.comptia.org\n• 10.5.100.10\nWhich of the following certificate fields must be set properly to support this objective?",
    "options": {
      "A": "Subject alternative name",
      "B": "Organizational unit",
      "C": "Extended key usage",
      "D": "Certificate extension"
    },
    "correct_answer": "A",
    "explanation": "The requirement is to include \"multiple identities\" (multiple hostnames and an IP address) in a single certificate so the server can mutually authenticate other resources over TLS. The certificate field designed for this purpose is the **Subject Alternative Name (SAN) (A)** extension. A SAN certificate allows you to specify multiple hostnames (DNS entries) and/or IP addresses that the certificate should cover, in addition to or instead of the Common Name (CN). This is essential for web servers that host multiple domains or need to be accessed by both IP address and hostname, all under one certificate.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 170,
    "question_text": "A security analyst reviews the following event timeline from an EDR solution:\nTime | File name | File action | Action verdict\n4:08 p.m. | hr-reporting.docx | File save | Allowed\n4:09 p.m. | hr-reporting.docx | Scan initiated | Pending\n4:10 p.m. | hr-reporting.docx | File execute | Allowed\n4:16 p.m. | paychecks.xlsx | File save | Allowed\n4:16 p.m. | paychecks.xlsx | File shared | Allowed\n4:17 p.m. | hr-reporting.docx | Script launched | Malware found\n4:19 p.m. | hr-reporting.docx | Scan complete | Allowed\n4:20 p.m. | paychecks.xlsx | File edit | Allowed\nWhich of the following has most likely occurred and needs to be fixed?",
    "options": {
      "A": "The DLP has failed to block malicious exfiltration, and data tagging is not being utilized properly.",
      "B": "A NIDS bypass was utilized by a threat actor, and updates must be installed by the administrator.",
      "C": "A logic flaw has introduced a TOCTOU vulnerability and must be addressed by the vendor.",
      "D": "A potential insider threat is being investigated and will be addressed by the senior management team."
    },
    "correct_answer": "C",
    "explanation": "Let's analyze the timeline:\n*   `4:09 p.m. hr-reporting.docx Scan initiated Pending` followed immediately by `4:10 p.m. hr-reporting.docx File execute Allowed`. The scan was initiated but still pending when the file was allowed to execute. Later, at `4:17 p.m.`, a `Script launched` from `hr-reporting.docx` is identified as `Malware found`. This indicates a **Time-of-Check to Time-of-Use (TOCTOU) vulnerability (C)**. The system performed a security check (scan initiated) on the file, but before the check completed or its results were acted upon, the file was allowed to execute, and it turned out to be malicious. This is a classic race condition where the security decision (allow/deny execution) is made based on the state of the file at one point in time, but the file's nature (or its associated script/payload) changes or is exploited before the system can fully enforce a security measure. This needs to be addressed by the vendor (application or EDR vendor) to ensure checks complete and block decisions are enforced before execution.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 171,
    "question_text": "A hospital provides tablets to its medical staff to enable them to more quickly access and edit patients' charts. The hospital wants to ensure that if a tablet is identified as lost or stolen and a remote command is issued, the risk of data loss can be mitigated within seconds. The tablets are configured as follows to meet hospital policy:\n• Full disk encryption is enabled.\n• \"Always On\" corporate VPN is enabled.\n• eFuse-backed keystore is enabled/ready.\n• Wi-Fi 6 is configured with SAE.\n• Location services is disabled.\n• Application allow list is unconfigured.\nAssuming the hospital policy cannot be changed, which of the following is the best way to meet the hospital's objective?",
    "options": {
      "A": "Revoke the user VPN and Wi-Fi certificates",
      "B": "Cryptographically erase FDE volumes",
      "C": "Issue new MFA credentials to all users",
      "D": "Configure the application allow list"
    },
    "correct_answer": "B",
    "explanation": "The core objective is to \"mitigate the risk of data loss within seconds\" if a lost or stolen tablet has a remote command issued. The tablets already have \"Full disk encryption (FDE) is enabled\" and \"eFuse-backed keystore is enabled/ready.\" This setup is ideal for **crypto-shredding**.\n\n**B. Cryptographically erase FDE volumes:** This method involves securely erasing the encryption key that protects the Full Disk Encryption (FDE) volume. Since the data is encrypted, destroying the key immediately renders all data on the disk cryptographically unrecoverable, even if the disk remains physically intact. This action can be triggered remotely and occurs almost instantly, effectively meeting the requirement to mitigate data loss within seconds. The eFuse-backed keystore ensures that the keys are hardware-protected and can be securely erased or rendered unusable if tampering occurs.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 172,
    "question_text": "A compliance officer is facilitating a business impact analysis and wants business unit leaders to collect meaningful data. Several business unit leaders want more information about the types of data the officer needs. Which of the following data types would be the most beneficial for the compliance officer? (Choose two.)",
    "options": {
      "A": "Inventory details",
      "B": "Applicable contract obligations",
      "C": "Costs associated with downtime",
      "D": "Network diagrams",
      "E": "Contingency plans",
      "F": "Critical processes"
    },
    "correct_answer": "C F",
    "explanation": "A Business Impact Analysis (BIA) aims to identify the critical functions, systems, and data within an organization and assess the impact of their disruption. To collect \"meaningful data\" from business unit leaders:\n\n1.  **C. Costs associated with downtime:** This is a crucial output of a BIA. Understanding the financial (and non-financial) costs of downtime for critical business functions allows the organization to quantify the potential losses from a disruption. This metric helps prioritize recovery efforts and justify investments in resilience.\n2.  **F. Critical processes:** The BIA's first step is usually to identify the organization's core business processes and determine which ones are critical for its operation. Business unit leaders are the best source for this information, as they understand which processes are essential, their interdependencies, and the impact of their interruption. Identifying critical processes is foundational to understanding what needs to be protected and recovered first.",
    "is_simulation": false,
    "question_type": "multiple_choice_multiple_answer"
  },
  {
    "question_number": 173,
    "question_text": "An ISAC supplied recent threat intelligence information about pictures used on social media that provide reconnaissance of systems in use in secure facilities. In response, the Chief Information Security Officer (CISO) wants several configuration changes implemented via the MDM to ensure the following:\n• Camera functions and location services are blocked for corporate mobile devices.\n• All social media is blocked on the corporate and guest wireless networks.\nWhich of the following is the CISO practicing to safeguard against the threat?",
    "options": {
      "A": "Adversary emulation",
      "B": "Operational security",
      "C": "Open-source intelligence",
      "D": "Social engineering"
    },
    "correct_answer": "B",
    "explanation": "The threat intelligence is about \"pictures used on social media that provide reconnaissance of systems in use in secure facilities.\" This means adversaries are using publicly available information (social media images) to gain intelligence about the organization's physical or digital assets. The CISO's response to block camera functions and location services on corporate devices, and block social media on networks, is a direct effort to prevent the leakage of sensitive operational information. This is a classic example of **Operational Security (OPSEC) (B)**. OPSEC is the process of identifying critical information to determine if friendly actions can be observed by enemy intelligence, determining if information can be exploited by the enemy, and then taking actions to eliminate or reduce vulnerabilities. The CISO is taking steps to protect the organization's operational details from being inadvertently revealed through employee social media or device usage.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 174,
    "question_text": "A company needs to define a new road map for improving secure coding practices in the software development life cycle and implementing better security standards. Which of the following is the best way for the company to achieve this goal?",
    "options": {
      "A": "Performing a Software Assurance Maturity Model assessment and generating a road map as a final result",
      "B": "Conducting a threat-modeling exercise for the main applications and developing a road map based on the necessary security implementations",
      "C": "Developing a new road map, including secure coding best practices, based on the security area road map and annual goals defined by the Chief Information Security Officer",
      "D": "Using the best practices in the OWASP secure coding manual to define a new road map"
    },
    "correct_answer": "A",
    "explanation": "The objective is to \"define a new road map for improving secure coding practices... and implementing better security standards.\" This is a strategic initiative for application security.\n\n**A. Performing a Software Assurance Maturity Model (SAMM) assessment and generating a road map as a final result:** SAMM is an open framework that helps organizations formulate and implement a strategy for software security that is tailored to the specific risks faced by the organization. It provides a structured way to assess current security practices across various security domains (including secure coding, design, testing, and governance), identify gaps, and define a phased roadmap for improvement. This approach directly results in a customized roadmap for improving secure coding and overall security standards.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 175,
    "question_text": "A security architect wants to develop a baseline of security configurations. These configurations automatically will be utilized every time a new virtual machine is created. Which of the following technologies should the security architect deploy to accomplish this goal?",
    "options": {
      "A": "Snort",
      "B": "CASB",
      "C": "Ansible",
      "D": "CMDB"
    },
    "correct_answer": "C",
    "explanation": "The goal is to \"develop a baseline of security configurations\" that will be \"automatically utilized every time a new virtual machine is created.\" This points to an Infrastructure as Code (IaC) or configuration management tool that can define and enforce desired states.\n\n**Ansible (C)** is an open-source automation engine that automates provisioning, configuration management, application deployment, orchestration, and other IT needs. It uses YAML playbooks to define desired configurations, which can then be automatically applied to newly created virtual machines (or other infrastructure). This allows for consistent and compliant deployment of security baselines without manual intervention, ensuring that every new VM starts with the approved secure configuration.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 176,
    "question_text": "A company wants to modify its process to comply with privacy requirements after an incident involving PII data in a development environment. In order to perform functionality tests, the QA team still needs to use valid data in the specified format. Which of the following best addresses the risk without impacting the development life cycle?",
    "options": {
      "A": "Encrypting the data before moving Into the QA environment",
      "B": "Truncating the data to make it not personally identifiable",
      "C": "Using a large language model to generate synthetic data",
      "D": "Utilizing tokenization for sensitive fields"
    },
    "correct_answer": "D",
    "explanation": "The problem involves sensitive PII data in a development environment, privacy compliance, and the need for the QA team to use \"valid data in the specified format\" for functionality tests without disrupting the development lifecycle. This is a common challenge when dealing with sensitive data in non-production environments. \n\n**D. Utilizing tokenization for sensitive fields:** Tokenization involves replacing sensitive data elements (like credit card numbers, social security numbers, or other PII) with a non-sensitive substitute called a token. The token retains the format and type of the original data but has no intrinsic value or meaning. The original sensitive data is stored securely in a separate, highly protected vault. The QA team can use the tokens for functionality testing, as they maintain the data's format and allow the application to function correctly, but the actual PII is never exposed in the development environment. This approach is highly effective for reducing privacy risk while enabling realistic testing.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 177,
    "question_text": "A global organization is reviewing potential vendors to outsource a critical payroll function. Each vendor's plan includes using local resources in multiple regions to ensure compliance with all regulations. The organization's Chief Information Security Officer is conducting a risk assessment on the potential outsourcing vendors' subprocessors. Which of the following best explains the need for this risk assessment?",
    "options": {
      "A": "Risk mitigations must be more comprehensive than the existing payroll provider.",
      "B": "Due care must be exercised during all procurement activities.",
      "C": "The responsibility of protecting PII remains with the organization.",
      "D": "Specific regulatory requirements must be met in each jurisdiction."
    },
    "correct_answer": "C",
    "explanation": "When an organization outsources a function involving sensitive data (like payroll, which includes PII), it remains ultimately accountable for the protection of that data, regardless of who is processing it. This is a fundamental principle in data privacy regulations (e.g., GDPR, HIPAA). Therefore, the CISO is conducting a risk assessment on the outsourcing vendors' *subprocessors* because **the responsibility of protecting PII remains with the organization (C)**. Even if the immediate vendor is compliant, the organization must ensure that any sub-contractors or subprocessors also adhere to adequate security and privacy standards to fulfill its own regulatory obligations and liability for the data. This extends the organization's risk management to the entire supply chain involved in data processing.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 178,
    "question_text": "An organization plans to deploy new software. The project manager compiles a list of roles that will be involved in different phases of the deployment life cycle. Which of the following should the project manager use to track these roles?",
    "options": {
      "A": "CMDB",
      "B": "Recall tree",
      "C": "ITIL",
      "D": "RACI matrix"
    },
    "correct_answer": "D",
    "explanation": "To track roles and responsibilities in a project, especially across different phases of a deployment lifecycle, the **RACI matrix (D)** is the most appropriate tool. RACI stands for:\n\n*   **R**esponsible: The person who does the work.\n*   **A**ccountable: The person who is ultimately answerable for the correct and thorough completion of the deliverable or task.\n*   **C**onsulted: Those whose opinions are sought, typically subject matter experts.\n*   **I**nformed: Those who are kept up-to-date on progress or decisions.\n\nA RACI matrix clearly defines who is responsible, accountable, consulted, and informed for each task or deliverable in a project, ensuring clarity of roles and efficient communication, which is exactly what a project manager needs to track involved roles.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 179,
    "question_text": "An organization decides to move to a distributed workforce model. Several legacy systems exist on premises and cannot be migrated because of existing compliance requirements. However, all new systems are required to be cloud-based. Which of the following would best ensure network access security?",
    "options": {
      "A": "Utilizing a VPN for all users who require legacy system access",
      "B": "Shifting all legacy systems to the existing public cloud infrastructure",
      "C": "Configuring an SDN to block malicious traffic to on-premises networks",
      "D": "Deploying microsegmentation with a firewall acting as the core router"
    },
    "correct_answer": "A",
    "explanation": "The scenario describes a hybrid environment: distributed workforce, new cloud-based systems, and legacy on-premises systems that *cannot* be migrated. The goal is to ensure \"network access security.\" \n\n**A. Utilizing a VPN for all users who require legacy system access:** Since the legacy systems must remain on-premises and the workforce is distributed, a VPN (Virtual Private Network) provides a secure, encrypted tunnel for remote users to access these internal resources. It extends the corporate network to the remote user, ensuring that traffic to the legacy systems is secured and authenticated, thus best ensuring network access security for the non-migrated systems.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 180,
    "question_text": "A web application server that provides services to hybrid modern and legacy financial applications recently underwent a scheduled upgrade to update common libraries, including OpenSSL. Multiple users are now reporting failed connection attempts to the server. The technician performing initial triage identified the following:\n• Client applications more than five years old appear to be the most affected\n• Web server logs show initial connection attempts by affected hosts.\n• For the failed connections, logs indicate \"cipher unavailable. \"\nWhich of the following is most likely to safely remediate this situation?",
    "options": {
      "A": "The server needs to be configured for backward compatibility to SSL 3.0 applications.",
      "B": "The client applications need to be modified to support AES in Galois/Counter Mode or equivalent",
      "C": "The client TLS configuration must be set to enforce electronic codebook modes of operation",
      "D": "The server-side digital signature algorithm needs to be modified to support elliptic curve cryptography"
    },
    "correct_answer": "B",
    "explanation": "The problem states: \"failed connection attempts to the server,\" \"client applications more than five years old appear to be the most affected,\" and \"logs indicate 'cipher unavailable'.\" This strongly suggests a mismatch in supported cryptographic algorithms or protocols. The server was recently upgraded to update OpenSSL, which likely means it now defaults to or strictly enforces more modern, secure cipher suites.\n\nLegacy clients often only support older, less secure (and sometimes deprecated) cipher suites. The \"cipher unavailable\" error means the client and server cannot agree on a mutually acceptable cryptographic algorithm for the TLS handshake. The safest remediation, given that the applications are \"legacy financial applications\" which usually implies critical and stable operations, is to **B. The client applications need to be modified to support AES in Galois/Counter Mode or equivalent**. GCM (Galois/Counter Mode) is a modern, authenticated encryption mode (like AES-GCM) that provides both confidentiality and integrity. If the old clients are updated to support modern, strong ciphers like AES-GCM (which the updated OpenSSL server likely supports and prefers), the `cipher unavailable` error will be resolved, and secure connections can be established. This is a safer approach than downgrading the server's security (e.g., enabling SSL 3.0) or using insecure modes like ECB.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 181,
    "question_text": "An organization recently migrated data to a new file management system. The architect decides to use a discretionary authorization model on the new system. Which of the following best explains the architect’s choice?",
    "options": {
      "A": "The responsibility of migrating data to the new file management system was outsourced to the vendor providing the platform.",
      "B": "The permissions were not able to be migrated to the new system, and several stakeholders were made responsible for granting appropriate access.",
      "C": "The legacy file management system did not support modern authentication techniques despite the business requirements.",
      "D": "The data custodians were selected by business stakeholders to ensure backups of the file management system are maintained off site."
    },
    "correct_answer": "B",
    "explanation": "In a Discretionary Access Control (DAC) model, resource owners (or creators) typically have the discretion to grant or revoke access to their resources. The architect's choice to use DAC on the new file management system is best explained by: **B. The permissions were not able to be migrated to the new system, and several stakeholders were made responsible for granting appropriate access.**\n\nIf existing permissions could not be directly migrated, a manual or a new, flexible system for assigning permissions is needed. DAC allows the 'several stakeholders' (who are likely the owners of the data or responsible for specific files/folders) to define and manage access to their respective data, which aligns with the discretionary nature of DAC. This avoids a rigid, top-down approach and gives control to those closest to the data, which is often chosen when a complex migration makes a strict, centralized model impractical initially.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 182,
    "question_text": "An organization recently acquired another company that is running a different EDR solution. A SOC analyst wants to automate the isolation of endpoints that are found to be compromised. Which of the following workflows best mitigates the risk of false positives and reduces the spread of malicious code?",
    "options": {
      "A": "Using a SOAR solution to look up entities via a TIP platform and isolate endpoints via APIs",
      "B": "Setting a policy on each EDR management console to isolate all endpoints that trigger any alerts",
      "C": "Reviewing all alerts manually in the various portals and taking action to isolate them",
      "D": "Automating the suppression of all alerts that are not critical and sending an email asking SOC analysts to review these alerts"
    },
    "correct_answer": "A",
    "explanation": "The goal is to \"automate the isolation of endpoints\" while \"mitigating the risk of false positives\" and \"reducing the spread of malicious code.\" This requires intelligent, automated response capabilities.\n\n**A. Using a SOAR (Security Orchestration, Automation, and Response) solution to look up entities via a TIP (Threat Intelligence Platform) platform and isolate endpoints via APIs:** This workflow is ideal:\n*   **SOAR:** Provides the automation and orchestration capabilities to take immediate action.\n*   **TIP:** Integrates with the SOAR to provide enriched context and validate potential threats. Before isolating, the SOAR can check the detected entity (e.g., file hash, IP address) against the TIP's database of known bad indicators. This verification step significantly reduces false positives.\n*   **Isolate endpoints via APIs:** Both EDR solutions (from the acquiring and acquired company) typically have APIs that SOAR can use to programmatically issue isolation commands, providing rapid containment and reducing the spread of malicious code.\n\nThis approach combines automation with intelligence to make informed, quick decisions.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 183,
    "question_text": "While reviewing recent incident reports a security officer discovers that several employees were contacted by the same individual who impersonated a recruiter. Which of the following best describes this type of correlation?",
    "options": {
      "A": "Spear-phishing campaign",
      "B": "Threat modeling",
      "C": "Red-team assessment",
      "D": "Attack pattern analysis"
    },
    "correct_answer": "D",
    "explanation": "The security officer is observing that \"several employees were contacted by the same individual who impersonated a recruiter.\" This is an observation of a recurring method or technique used by an adversary. **Attack pattern analysis (D)** is the process of identifying and understanding the common methods, tactics, and procedures (TTPs) that attackers use. In this case, the specific attack pattern is 'impersonating a recruiter to contact employees', which is a social engineering technique. By identifying this pattern, the organization can better understand the threat and develop targeted defenses or awareness campaigns against this specific modus operandi.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 184,
    "question_text": "A news organization wants to implement workflows that allow users to request that untruthful data be retraced and scrubbed from online publications to comply with the right to be forgotten. Which of the following regulations is the organization most likely trying to address?",
    "options": {
      "A": "GDPR",
      "B": "COPPA",
      "C": "CCPA",
      "D": "DORA"
    },
    "correct_answer": "A",
    "explanation": "The key phrase is \"right to be forgotten.\" This fundamental right is a cornerstone of the **General Data Protection Regulation (GDPR) (A)**, a comprehensive data privacy law enacted by the European Union. GDPR Article 17, 'Right to erasure ('right to be forgotten')', grants individuals the right to have their personal data erased by a data controller under certain conditions. For a news organization publishing online, this would involve workflows to remove or 'scrub' personal data if requested and compliant with legal requirements. While other regulations like CCPA (C) have similar rights (right to delete), the term 'right to be forgotten' is most strongly associated with GDPR.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 185,
    "question_text": "An analyst wants to conduct a risk assessment on a new application that is being deployed. Given the following information:\n• Total budget allocation for the new application is unavailable.\n• Recovery time objectives have not been set\n• Downtime loss calculations cannot be provided\nWhich of the following statements describes the reason a qualitative assessment is the best option?",
    "options": {
      "A": "The analyst has previous work experience in application development",
      "B": "Sufficient metrics are not available to conduct other risk assessment types",
      "C": "An organizational risk register tracks all risks and mitigations across business units",
      "D": "The organization wants to find the monetary value of any outages"
    },
    "correct_answer": "B",
    "explanation": "The problem states that key quantitative metrics are unavailable: \"Total budget allocation for the new application is unavailable,\" \"Recovery time objectives have not been set,\" and \"Downtime loss calculations cannot be provided.\" \n\n**B. Sufficient metrics are not available to conduct other risk assessment types:** This is the precise reason why a qualitative assessment is the best option. Quantitative risk assessments require specific numerical data (e.g., asset values, annual loss expectancy, cost of controls) to calculate risk in monetary terms. Since these financial and operational metrics are missing, it's impossible to perform a quantitative assessment. A qualitative assessment, on the other hand, relies on descriptive categories (e.g., high, medium, low) for impact and likelihood, making it suitable when precise numerical data is lacking.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 186,
    "question_text": "A software company deployed a new application based on its internal code repository. Several customers are reporting anti-malware alerts on workstations used to test the application. Which of the following is the most likely cause of the alerts?",
    "options": {
      "A": "Misconfigured code commit",
      "B": "Unsecure bundled libraries",
      "C": "Invalid code signing certificate",
      "D": "Data leakage"
    },
    "correct_answer": "B",
    "explanation": "The key information is that customers are reporting \"anti-malware alerts on workstations used to test the application\" after a new application deployment. The application is based on an \"internal code repository.\" \n\nThe most likely cause of anti-malware alerts is **unsecure bundled libraries (B)**. Many modern applications bundle third-party libraries (open-source or commercial) as part of their distribution. If these libraries contain known vulnerabilities or are simply flagged by anti-malware software for suspicious behavior (e.g., packed executables, certain network functions that are common in malware), they can trigger alerts even if the company's own code is clean. This is a common supply chain security issue where vulnerabilities or suspicious components within dependencies trigger security software.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 187,
    "question_text": "A security engineer is reviewing the following vulnerability scan report:\nHostname | IP address | Description | Public facing | CVSS 3.0 score\nweb.example.com | 192.168.7.1 | Apache HTTP Server < 2.4 | Yes | 9.7\ncomptia-rhel01 | 192.168.8.131 | OpenSSH < 9.0/9.1 | No | 9.8\ncomptia-rhel02 | 192.168.7.2 | Google Chrome Update < 10.0.0.131 | No | 8.5\nweb1.example.com | 192.168.8.132 | SSL/TLS 1.0 Weak Protocols Support | Yes | 8.5\nWhich of the following should the engineer prioritize for remediation?",
    "options": {
      "A": "Apache HTTP Server",
      "B": "OpenSSH",
      "C": "Google Chrome",
      "D": "Migration to TLS 1.3"
    },
    "correct_answer": "B",
    "explanation": "To prioritize remediation, the security engineer should look for vulnerabilities with high CVSS scores and public-facing exposure.\n\n*   `web.example.com` (Apache HTTP Server) has CVSS 3.0 score 9.7 (High) and is Public-facing.\n*   `comptia-rhel01` (OpenSSH < 9.0/9.1) has CVSS 3.0 score 9.8 (Critical) and is Not Public-facing.\n*   `comptia-rhel02` (Google Chrome Update < 10.0.0.131) has CVSS 3.0 score 8.5 (High) and is Not Public-facing.\n*   `web1.example.com` (SSL/TLS 1.0 Weak Protocols Support) has CVSS 3.0 score 8.5 (High) and is Public-facing.\n\nComparing the criticalities, `OpenSSH < 9.0/9.1` has a CVSS score of 9.8, which is the highest score listed. Even though it's not public-facing, a CVSS score of 9.8 indicates a critical vulnerability, likely easily exploitable, and potentially leading to full system compromise once accessed internally. Given its extremely high severity, **OpenSSH (B)** should be prioritized for remediation. While Apache HTTP Server is public-facing and high severity, the OpenSSH vulnerability is even higher and could lead to compromise of internal systems.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 188,
    "question_text": "A company notices that cloud environment costs increased after using a new serverless solution based on API requests. Many invalid requests from unknown IPs were found, often within a short time. Which of the following solutions would most likely solve this issue, reduce cost, and improve security?",
    "options": {
      "A": "Using digital certificates for known customers and performing API authorization through those certificates",
      "B": "Defining request rate limits and comparing new requests from unknown IPs with a list of known-malicious IPs",
      "C": "Setting authentication processes for the API requests as well as proper rate limits according to regular usage",
      "D": "Only allowing API requests coming from regions with known customers"
    },
    "correct_answer": "C",
    "explanation": "The problem indicates increased cloud costs due to \"many invalid requests from unknown IPs\" to a serverless API solution. This suggests that the API is being abused or hit by non-legitimate traffic (e.g., bots, scanning attempts, or misconfigured clients). This drives up compute costs in a serverless model where billing is often based on execution. To solve this, reduce cost, and improve security:\n\n**C. Setting authentication processes for the API requests as well as proper rate limits according to regular usage:**\n*   **Authentication processes:** Requiring authentication (e.g., API keys, OAuth tokens) ensures that only legitimate and identified users/applications can make requests. This immediately blocks \"invalid requests from unknown IPs.\"\n*   **Proper rate limits:** This prevents abuse even from authenticated users. Rate limits cap the number of requests a single client can make within a time period, protecting against excessive usage and potential DoS attacks. By combining both, the company ensures that only authorized requests are processed, and even authorized requests are capped, directly addressing the cost increase from invalid requests and improving overall security and cost efficiency.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 189,
    "question_text": "A large organization deployed a generative AI platform for its global user population to use. Based on feedback received during beta testing, engineers have identified issues with user interface latency and page-loading performance for international users. The infrastructure is currently maintained within two separate data centers, which are connected using high-availability networking and load balancers. Which of the following is the best way to address the performance issues?",
    "options": {
      "A": "Configuring the application to use a CDN",
      "B": "Implementing RASP to enable large language models queuing",
      "C": "Remote journaling within a third data center",
      "D": "Traffic shaping through the use of a SASE"
    },
    "correct_answer": "A",
    "explanation": "The core problem is \"user interface latency and page-loading performance for international users\" of a global AI platform. The platform is currently hosted in two data centers. To improve performance for geographically dispersed users, the best solution is to bring content closer to them.\n\n**A. Configuring the application to use a CDN (Content Delivery Network):** A CDN geographically distributes static and dynamic content (e.g., UI elements, images, scripts, AI model outputs that can be cached) to edge servers closer to the end-users. When an international user requests content, it's served from the nearest CDN edge node rather than from the main data centers, significantly reducing latency and improving page-loading performance. This is a standard solution for optimizing global web application performance.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 190,
    "question_text": "A company reduced its staff 60 days ago, and applications are now starting to fail. The security analyst is investigating to determine if there is malicious intent for the application failures. The security analyst reviews the following logs:\nMar 5 22:09:50 ak:3 sshd[21502]: Success login for user01 from 192.168.2.5\nMar 5 22:10:00 ak:3 sshd[21502]: Failed login for user10 from 192.168.2.5\nMar 5 22:10:45 ak:3 sshd[21502]: Success login for user01 from 192.168.2.58\nMar 5 22:11:00 ak:3 sshd[21502]: Failed login for user10 from 192.168.2.5\nMar 5 22:12:00 ak:3 sshd[21502]: Failed login for user10 from 192.168.2.5\nMar 5 22:12:30 ak:3 sshd[21502]: Success login for user12 from 192.168.2.27\nMar 5 22:13:00 ak:3 sshd[21502]: Failed login for user10 from 192.168.2.5\nWhich of the following is the most likely reason for the application failures?",
    "options": {
      "A": "The user’s account was set as a service account",
      "B": "The user's home directory was deleted",
      "C": "The user does not have sudo access.",
      "D": "The root password has been changed"
    },
    "correct_answer": "A",
    "explanation": "The logs show repeated \"Failed login\" attempts and \"Success login\" attempts for various users (`user01`, `user10`, `user12`) from `192.168.2.5` to `sshd[21502]` (likely an SSH daemon process). The problem states that the company reduced staff 60 days ago and applications are now failing.\n\nThe most likely reason for applications failing due to staff reduction is that **the user’s account was set as a service account (A)**. If an application relies on a specific user account (e.g., `user01` or `user10`) to run as a service, and that account was associated with a departed employee and then disabled or de-provisioned, the application relying on it would fail to start or operate correctly. The repeated failed logins for `user10` from the same source IP (`192.168.2.5`) could be the application attempting to authenticate as this account, failing because it is now disabled or removed. This is a common oversight during staff reduction when service accounts are not properly identified and transferred.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 191,
    "question_text": "A security analyst detected unusual network traffic related to program updating processes. The analyst collected artifacts from compromised user workstations. The discovered artifacts were binary files with the same name as existing valid binaries but with different hashes. Which of the following solutions would most likely prevent this situation from reoccurring?",
    "options": {
      "A": "Improving patching processes",
      "B": "Implementing digital signature",
      "C": "Performing manual updates via USB ports",
      "D": "Allowing only files from internal sources"
    },
    "correct_answer": "B",
    "explanation": "The core problem is that malicious binary files with the \"same name as existing valid binaries but with different hashes\" were found, indicating that legitimate program files were replaced or tampered with. This is a form of software supply chain attack or file integrity compromise. To prevent this situation from reoccurring, **implementing digital signatures (B)** for all executables and updates is the most effective solution.\n\nDigital signatures (code signing) bind the identity of the software publisher to the code and ensure its integrity. If a binary file is modified or replaced (even with the same name), its digital signature will become invalid. Systems configured to verify these signatures before execution will detect the tampering and prevent the unauthorized binary from running, thus ensuring the authenticity and integrity of program updates.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 192,
    "question_text": "Source code snippets for two separate malware samples are shown below:\nSample 1:\n...\nknockEmDown(String e)\n{\n  if(target.isAccessed())\n  {\n    System.out.println(e.toString());\n    target.sendTelemetry(target.hostname.toString() + \" is \" + e.toString());\n  }\n}\nSample 2:\n...\ntargetSys(address a)\n{\n  if(address.isIPv4())\n  {\n    address.connect(1337);\n    String status = knockEmDown(address.current);\n    System.out.println(address.current + \" is \" + status);\n  }\n  else\n  {\n    throw Exception e;\n  }\n}\n...\nWhich of the following describes the most important observation about the two samples?",
    "options": {
      "A": "Telemetry is first buffered and then transmitted in paranoid mode",
      "B": "The samples were probably written by the same developer.",
      "C": "Both samples use IP connectivity for command and control",
      "D": "Sample 1 is the target agent while Sample 2 is the C2 server."
    },
    "correct_answer": "B",
    "explanation": "Comparing the two source code snippets, the most striking observation suggesting they were written by the same developer is the reuse of distinctive variable names and function calls.\n\n*   **Sample 1** has `knockEmDown(String e)` and `target.sendTelemetry(target.hostname.toString() + \" is \" + e.toString());`\n*   **Sample 2** has `String status = knockEmDown(address.current);` and `target.sendC2(address.current + \" is \" + status);`\n\nThe presence of the unique function name `knockEmDown` in both samples and the very similar structure and purpose of the `sendTelemetry` and `sendC2` calls (both involving `address.current` or `target.hostname` and a status/error message) are strong indicators of common authorship. Developers often have distinct coding styles, naming conventions, and reusable code blocks that reveal their identity across different projects.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 193,
    "question_text": "A systems engineer is configuring a system baseline for servers that will provide email services. As part of the architecture design, the engineer needs to improve performance of the systems by using an access vector cache, facilitating mandatory access control, and protecting against:\n• Unauthorized reading and modification of data and programs\n• Bypassing application security mechanisms\n• Privilege escalation\n• Interference with other processes\nWhich of the following is the most appropriate for the engineer to deploy?",
    "options": {
      "A": "SELinux",
      "B": "Privileged access management",
      "C": "Self-encrypting disks",
      "D": "NIPS"
    },
    "correct_answer": "A",
    "explanation": "The requirements describe a need for fine-grained access control and system hardening that goes beyond traditional discretionary access controls, specifically protecting against unauthorized data modification, bypassing security mechanisms, privilege escalation, and inter-process interference. It also mentions \"facilitating mandatory access control\" and \"using an access vector cache.\"\n\n**A. SELinux (Security-Enhanced Linux):** SELinux is a Linux kernel security module that provides a mechanism for supporting access control security policies, including Mandatory Access Control (MAC). It enforces these policies on processes, files, and other system resources. SELinux uses an Access Vector Cache (AVC) to speed up access decisions. It directly addresses all the listed protections: unauthorized reading/modification (through MAC), bypassing application security (by enforcing policy on all processes), privilege escalation (by tightly controlling process capabilities), and interference between processes (through strict process isolation).",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 194,
    "question_text": "A company migrated a critical workload from its data center to the cloud. The workload uses a very large data set that requires computational-intensive data processing. The business unit that uses the workload is projecting the following growth pattern:\n• Storage requirements will double every six months.\n• Computational requirements will fluctuate throughout the year\n• Average computational requirements will double every year.\nWhich of the following should the company do to address the business unit's requirements?",
    "options": {
      "A": "Deploy a cloud-based CDN for storage and a load balancer for compute",
      "B": "Combine compute and storage in vertically autoscaling mode",
      "C": "Implement a load balancer for computing and storage resources",
      "D": "Plan for a horizontally scaling computing and storage infrastructure"
    },
    "correct_answer": "D",
    "explanation": "The key challenges are: \"very large data set,\" \"computational-intensive data processing,\" \"storage requirements will double every six months,\" \"computational requirements will fluctuate throughout the year,\" and \"average computational requirements will double every year.\" This indicates rapid and variable growth in both data volume and processing needs, which is a perfect fit for cloud elasticity. \n\n**D. Plan for a horizontally scaling computing and storage infrastructure:** Horizontal scaling (scaling out) involves adding more machines (servers, storage nodes) to a system as demand increases, rather than increasing the capacity of existing machines (vertical scaling). This approach is ideal for handling high-growth, fluctuating workloads because cloud environments are designed for easy horizontal scaling. As storage and computational needs double, new instances or storage units can be automatically provisioned and de-provisioned (elasticity), allowing the infrastructure to scale seamlessly and cost-effectively to meet demand without over-provisioning.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 195,
    "question_text": "A security analyst received a notification from a cloud service provider regarding an attack detected on a web server. The cloud service provider shared the following information about the attack:\n• The attack came from inside the network.\n• The attacking source IP was from the internal vulnerability scanners\n• The scanner is not configured to target the cloud servers.\nWhich of the following actions should the security analyst take first?",
    "options": {
      "A": "Create an allow list for the vulnerability scanner IPs in order to avoid false positives",
      "B": "Configure the scan policy to avoid targeting an out-of-scope host",
      "C": "Set network behavior analysis rules.",
      "D": "Quarantine the scanner sensor to perform a forensic analysis"
    },
    "correct_answer": "B",
    "explanation": "The core problem is that an \"attack detected on a web server\" was actually caused by \"internal vulnerability scanners\" that were \"not configured to target the cloud servers.\" This means the vulnerability scanner inadvertently attacked the web server because it was scanning assets it wasn't supposed to. This is a configuration error resulting in a false positive/unintended behavior from the scanner. \n\nThe most appropriate first action is to **configure the scan policy to avoid targeting an out-of-scope host (B)**. This immediately stops the unintended scanning of the web server by updating the scanner's configuration to exclude the cloud servers from its target list. This prevents further unnecessary alerts and potential disruption to the web server, addressing the root cause of the immediate issue.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 196,
    "question_text": "A company implemented a NIDS and a NIPS on the most critical environments. Since this implementation the company has been experiencing network connectivity issues. Which of the following should the security architect recommend for a new NIDS/NIPS implementation?",
    "options": {
      "A": "Implementing the NIDS with a port mirror in the core switch and the NIPS in the main firewall",
      "B": "Implementing the NIDS and the NIPS together with the main firewall",
      "C": "Implementing a NIDS without a NIPS to increase the detection capability",
      "D": "Implementing the NIDS in the bastion host and the NIPS in the branch network router"
    },
    "correct_answer": "A",
    "explanation": "The problem states that after implementing NIDS (Network Intrusion Detection System) and NIPS (Network Intrusion Prevention System), the company is experiencing \"network connectivity issues.\" This often happens when NIPS is incorrectly deployed or misconfigured, as NIPS is an in-line device that can block traffic. \n\nTo resolve connectivity issues while maintaining security:\n\n**A. Implementing the NIDS with a port mirror in the core switch and the NIPS in the main firewall:**\n*   **NIDS with a port mirror:** A NIDS is a passive device. Deploying it with a port mirror (SPAN port) means it receives a copy of the network traffic without being in the direct path. This allows it to monitor traffic without introducing any latency or connectivity issues, fulfilling its detection role without disruption.\n*   **NIPS in the main firewall:** A NIPS is an active, in-line device. Placing it within the main firewall allows for centralized policy enforcement and leverages the firewall's existing traffic control capabilities. This placement ensures that any blocking actions by the NIPS are managed by a critical network device, providing consistent policy application and minimizing the chance of unintended blocking that causes connectivity issues. This separation of NIDS (passive monitoring) and NIPS (active prevention) roles is a common best practice to balance security and network performance.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 197,
    "question_text": "The material findings from a recent compliance audit indicate a company has an issue with excessive permissions. The findings show that employees changing roles or departments results in privilege creep. Which of the following solutions are the best ways to mitigate this issue? (Choose two.)",
    "options": {
      "A": "Setting different access controls defined by business area",
      "B": "Implementing a role-based access policy",
      "C": "Designing a least-needed privilege policy",
      "D": "Establishing a mandatory vacation policy",
      "E": "Performing periodic access reviews",
      "F": "Requiring periodic job rotation"
    },
    "correct_answer": "B E",
    "explanation": "The core problem is \"excessive permissions\" and \"privilege creep\" resulting from employees changing roles or departments. This means users accumulate permissions over time that they no longer need.\n\n1.  **B. Implementing a role-based access policy:** Role-Based Access Control (RBAC) assigns permissions to roles, and users are assigned roles based on their job functions. When an employee changes roles, their previous role assignments can be revoked, and new ones granted, helping to ensure they only have permissions relevant to their *current* role, thus preventing privilege creep more effectively than individual permission assignments.\n2.  **E. Performing periodic access reviews:** Regular access reviews (also known as access certifications or recertifications) involve systematically reviewing user access rights to ensure they are still appropriate and necessary for their current job functions. This directly identifies and allows for the removal of excessive permissions that have accumulated due to role changes or other factors, actively mitigating privilege creep.",
    "is_simulation": false,
    "question_type": "multiple_choice_multiple_answer"
  },
  {
    "question_number": 198,
    "question_text": "A security analyst is reviewing a SIEM and generates the following report:\nDEV001 | 192.168.1.2 | 192.168.2.2 | VM001 | 9920 | Deny connection | 4:55:28\nDEV001 | 192.168.2.0 | 192.168.2.2 | VM001 | 1912 | IPS Alert | 7:21:41\nDEV001 | 10.1.1.1 | 192.168.2.2 | VM001 | Malware detection, 811112 | \nDEV001 | 10.1.1.1 | 192.168.2.2 | VM001 | 9927 | Allow connection | 8:15:32\nLater, the incident response team notices an attack was executed on the VM001 host. Which of the following should the security analyst do to enhance the alerting process on the SIEM platform?",
    "options": {
      "A": "Include the EDR solution on the SIEM as a new log source",
      "B": "Perform a log correlation on the SIEM solution",
      "C": "Improve parsing of data on the SIEM",
      "D": "Create a new rule set to detect malware"
    },
    "correct_answer": "B",
    "explanation": "The report shows various events on `VM001`: `Network connection` (denied), `Malware detection` (blocked), `IPS Alert`, and `Allow connection`. The problem states that an attack was executed on `VM001`, implying that despite these individual events, a full picture or alert for the attack wasn't generated or wasn't clear. To enhance the alerting process and gain better visibility into complex attacks, the security analyst should **perform a log correlation on the SIEM solution (B)**.\n\nLog correlation involves analyzing multiple events across different log sources to identify patterns, sequences, or relationships that indicate a larger security incident. For example, a denied connection, followed by an IPS alert, then malware detection, and finally an allowed connection (which might be C2 traffic after malware execution) on the same host, might individually trigger low-severity alerts. But when correlated, they could trigger a high-severity alert for a potential compromise. This helps the SIEM move from reporting individual events to identifying the actual attack being executed, improving the quality and actionability of alerts.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 199,
    "question_text": "A security administrator is performing a gap assessment against a specific OS benchmark. The benchmark requires the following configurations be applied to endpoints:\n• Full disk encryption\n• Host-based firewall\n• Time synchronization\n• Password policies\n• Application allow listing\n• Zero Trust application access\nWhich of the following solutions best addresses the requirements? (Choose two.)",
    "options": {
      "A": "MDM",
      "B": "CASB",
      "C": "SBoM",
      "D": "SCAP",
      "E": "SASE",
      "F": "HIDS"
    },
    "correct_answer": "A D",
    "explanation": "The security administrator needs to apply specific configurations to endpoints (Full disk encryption, Host-based firewall, Time synchronization, Password policies, Application allow listing, Zero Trust application access).\n\n1.  **A. MDM (Mobile Device Management):** While MDM is primarily for mobile devices, modern MDM/UEM (Unified Endpoint Management) solutions often manage configurations and policies for laptops and desktops as well, especially in the context of corporate-owned devices. MDM can enforce full disk encryption, deploy host-based firewall rules, manage password policies, and ensure compliance with various security settings on endpoints.\n2.  **D. SCAP (Security Content Automation Protocol):** SCAP is a suite of specifications that standardize how security automation tools assess and report on the security posture of systems. It is commonly used for defining and checking compliance against security benchmarks and baselines. An SCAP-compliant tool can audit endpoints for adherence to configurations like full disk encryption, firewall settings, password policies, and application allow listing. This directly addresses the need to apply and verify configurations against an OS benchmark. The 'Zero Trust application access' is typically handled by identity and access management solutions often integrated with MDM/UEM and conditional access.",
    "is_simulation": false,
    "question_type": "multiple_choice_multiple_answer"
  },
  {
    "question_number": 200,
    "question_text": "A security analyst is reviewing the following authentication logs:\nDate | Time | Computer | Account | Log-in success?\n12/15 | 8:01:23AM | VM01 | User1 | No\n12/15 | 8:01:23AM | VM01 | User1 | No\n12/15 | 8:01:23AM | VM08 | User8 | No\n12/15 | 8:01:23AM | VM12 | User12 | No\n12/15 | 8:01:23AM | VM12 | User12 | Yes\n12/15 | 8:01:23AM | VM01 | User1 | Yes\n12/15 | 8:01:24AM | VM01 | User1 | No\n12/15 | 8:01:24AM | VM01 | User2 | No\n12/15 | 8:01:25AM | VM01 | User2 | No\n12/15 | 8:01:25AM | VM08 | User8 | Yes\nWhich of the following should the analyst do first?",
    "options": {
      "A": "Disable User2’s account.",
      "B": "Disable User12’s account",
      "C": "Disable User8’s account",
      "D": "Disable User1’s account"
    },
    "correct_answer": "D",
    "explanation": "The authentication logs show `User1` with multiple failed login attempts on `VM01` at `8:01:23AM` and `8:01:24AM`, but also a successful login (`Yes`) at `8:01:23AM` on `VM01`. This pattern of repeated failures followed by a success for the *same account on the same machine within the same minute* strongly suggests a successful brute-force attack or credential stuffing, or a more sophisticated attack technique where initial login attempts are failed, but then the attacker finds a way to succeed. This indicates a compromise of `User1`'s account. While other users like `User8` and `User12` also have successful logins, the combination of multiple failures and a subsequent success for `User1` makes their account the highest priority for immediate action to stop potential ongoing malicious activity. Therefore, **disabling User1’s account (D)** is the most urgent step.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 201,
    "question_text": "A game developer wants to reach new markets and is advised by legal counsel to include specific age-related sign-up requirements. Which of the following best describes the legal counsel's concerns?",
    "options": {
      "A": "GDPR",
      "B": "LGPD",
      "C": "PCI DSS",
      "D": "COPPA"
    },
    "correct_answer": "D",
    "explanation": "The legal counsel's advice to include \"specific age-related sign-up requirements\" for a game developer aiming for \"new markets\" is directly related to regulations concerning children's online privacy. The **Children's Online Privacy Protection Act (COPPA) (D)** is a U.S. federal law that imposes certain requirements on operators of websites or online services directed to children under 13 years of age, or on operators that have actual knowledge that they are collecting personal information online from children under 13. This includes requirements for verifiable parental consent, privacy policies, and limitations on data collection. Game developers, due to their user base, are often directly impacted by COPPA.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 202,
    "question_text": "Which of the following AI concerns is most adequately addressed by input sanitization?",
    "options": {
      "A": "Model inversion",
      "B": "Prompt injection",
      "C": "Data poisoning",
      "D": "Non-explainable model"
    },
    "correct_answer": "B",
    "explanation": "Input sanitization is the process of cleaning, filtering, or validating user-supplied data before it is processed by a system. In the context of AI, particularly with large language models (LLMs) and other generative AI, **prompt injection (B)** is the concern most directly addressed by input sanitization. Prompt injection attacks involve crafting malicious inputs (prompts) to manipulate the AI model into performing unintended actions, revealing sensitive information, or bypassing its safety guidelines. By sanitizing user inputs, the system can attempt to detect and neutralize malicious instructions or unexpected characters before they reach the AI model, thereby mitigating prompt injection attempts.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 203,
    "question_text": "A company that operates in different countries has local email infrastructure for each of its business units. A breach occurred in which email communications were intercepted between the headquarters and one of the overseas business units.\nDuring an investigation, the security analyst finds the following email log:\n250 2.0.0 STARTTLS\n220 2.0.0 SMTP server ready\n(Email content)\nTLS Negotiation: No common cipher suite, SockError\nWhich of the following actions should the security analyst take to best address the issue?",
    "options": {
      "A": "Revoke the expired TLS certificate and replace it with a valid one",
      "B": "Disable the NTLM authentication and replace it with TLS 1.2",
      "C": "Change the TLS configuration from opportunistic to enforced",
      "D": "Create a new TLS certificate using a stronger algorithm and larger key"
    },
    "correct_answer": "C",
    "explanation": "The email log shows: `250 2.0.0 STARTTLS`, then `220 2.0.0 SMTP server ready` followed by the email being sent. The critical line is `TLS Negotiation: No common cipher suite, SockError`. This indicates that a secure TLS connection could not be established between the sending and receiving email servers because they couldn't agree on a common cipher suite, leading to the email being sent in plaintext (or through a less secure fallback). This is common when TLS is configured in **opportunistic** mode, meaning it will attempt TLS but fall back to plaintext if TLS negotiation fails. Since email communications were intercepted, this vulnerability was exploited. \n\nTo address this, the security analyst should **change the TLS configuration from opportunistic to enforced (C)**. Enforced TLS (also known as mandatory TLS) means that email servers will *only* send or receive mail if a secure TLS connection can be established. If TLS negotiation fails, the email will not be sent, preventing plaintext transmission and the risk of interception. This prioritizes security over delivery in situations where confidentiality is paramount.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 204,
    "question_text": "During a recent audit, a company's systems were assessed Given the following information:\nDepartment | System | Status | Notes\nAccounting | TaxReporting | OK |\nHuman resources | HRIS | OK |\nManufacturing | ProductionControl | WARNING | EOL software detected\nSupport | ServiceDesk | WARNING | Patches available\nWhich of the following is the best way to reduce the attack surface?",
    "options": {
      "A": "Deploying an EDR solution to all impacted machines in manufacturing",
      "B": "Segmenting the manufacturing network with a firewall and placing the rules in monitor mode",
      "C": "Setting up an IDS inline to monitor and detect any threats to the software",
      "D": "Implementing an application-aware firewall and writing strict rules for the application access"
    },
    "correct_answer": "D",
    "explanation": "The provided table indicates the status of different departments/systems. The issue is about reducing the attack surface. An attack surface is the sum of the different points where an unauthorized user can try to enter data to or extract data from an environment. Reducing it means minimizing the potential entry points. \n\n**D. Implementing an application-aware firewall and writing strict rules for the application access:** An application-aware firewall can inspect traffic at the application layer (Layer 7) and make decisions based on the application protocol, rather than just IP addresses and ports. By writing *strict rules for application access*, it ensures that only legitimate application traffic is allowed, and only between necessary components. This significantly reduces the attack surface by blocking all other unauthorized application-layer communication, making it much harder for attackers to exploit vulnerabilities or move laterally by abusing unintended communication paths. This is a very granular way to control network access and reduce the attack surface for specific applications.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 205,
    "question_text": "A global manufacturing company has an internal application that is critical to making products. This application cannot be updated and must be available in the production area. A security architect is implementing security for the application. Which of the following best describes the action the architect should take?",
    "options": {
      "A": "Disallow wireless access to the application.",
      "B": "Deploy intrusion detection capabilities using a network tap",
      "C": "Create an acceptable use policy for the use of the application",
      "D": "Create a separate network for users who need access to the application"
    },
    "correct_answer": "D",
    "explanation": "The critical constraints are: an \"internal application that is critical to making products,\" it \"cannot be updated,\" and it \"must be available in the production area.\" Since it cannot be updated (implying potential vulnerabilities) and must remain available, the primary security strategy shifts from patching to isolation and access control. \n\n**D. Create a separate network for users who need access to the application:** This refers to network segmentation or isolation. By placing the critical, unpatchable application on a dedicated, isolated network segment, access can be tightly controlled. Only authorized users or systems (and only those truly needing access) would be able to connect to this segment, and traffic within and out of this segment would be strictly regulated by firewall rules. This significantly reduces the attack surface and potential for lateral movement, effectively containing the risk of the vulnerable application without disrupting its availability. Disallowing wireless access (A) might be a step, but is part of broader network segmentation. Acceptable Use Policy (C) is administrative, not a technical control. IDS (B) detects but doesn't prevent access to the vulnerable application.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 206,
    "question_text": "A company wants to perform threat modeling on an internally developed, business-critical application. The Chief Information Security Officer (CISO) is most concerned that the application should maintain 99.999% availability and authorized users should only be able to gain access to data they are explicitly authorized to view. Which of the following threat-modeling frameworks directly addresses the CISO’s concerns about this system?",
    "options": {
      "A": "CAPEC",
      "B": "STRIDE",
      "C": "ATT&CK",
      "D": "TAXII"
    },
    "correct_answer": "B",
    "explanation": "The CISO's concerns are explicitly about: \"99.999% availability\" and \"authorized users should only be able to gain access to data they are explicitly authorized to view\" (confidentiality and authorization). \n\n**STRIDE (B)** is a threat modeling methodology developed by Microsoft that categorizes threats into six categories, each directly corresponding to a security property:\n\n*   **S**poofing (Authentication)\n*   **T**ampering (Integrity)\n*   **R**epudiation (Non-repudiation)\n*   **I**nformation Disclosure (**Confidentiality**) - directly addresses \"authorized users should only be able to gain access to data they are explicitly authorized to view.\"\n*   **D**enial of Service (**Availability**) - directly addresses \"99.999% availability.\"\n*   **E**levation of Privilege (Authorization) - indirectly addresses authorization by ensuring users don't gain unauthorized higher privileges.\n\nSTRIDE is specifically designed to identify threats against these core security properties, making it the most suitable framework for the CISO's stated concerns.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 207,
    "question_text": "A company's internal network is experiencing a security breach and the threat actor is still active Due to business requirements, users in this environment are allowed to utilize multiple machines at the same time. Given the following log snippet:\nTime | User | Process | Action | Machine\n10:11 | user-a | mnc.exe | blocked | machine02\n10:12 | user-a | mnc.exe | blocked | machine01\n10:13 | user-b | appwiz.exe | allowed | machine03\n10:14 | user-b | appwiz.exe | allowed | machine04\n10:15 | user-c | cmd.exe | allowed | machine03\n10:16 | user-c | sysconfig.exe | allowed | machine03\n10:17 | user-c | winver.exe | allowed | machine01\n10:18 | user-d | firefox.exe | blocked | machine04\n10:19 | user-d | cmd.com | blocked | machine01\nWhich of the following accounts should a security analyst disable to best contain the incident without impacting valid users?",
    "options": {
      "A": "user-a",
      "B": "user-b",
      "C": "user-c",
      "D": "user-d"
    },
    "correct_answer": "C",
    "explanation": "The goal is to contain the incident without impacting valid users, given that users can utilize multiple machines. We need to identify an account that is likely compromised and its activity is definitively malicious.\n\n*   **user-a:** Logs show `mnc.exe` blocked on `machine02` and `machine01`. These are blocked activities, so no current compromise.\n*   **user-b:** Logs show `appwiz.exe` allowed on `machine03` and `machine04`. `appwiz.exe` is a legitimate Windows process (Add/Remove Programs). While allowed, these activities are not inherently malicious.\n*   **user-c:** Logs show `cmd.exe` allowed on `machine03`, and crucially, `sysconfig.exe` allowed on `machine03`. `sysconfig.exe` is a highly suspicious or non-standard executable name, often associated with malware, system manipulation tools, or custom backdoor components. Allowing this to execute on a machine during an active breach is a strong indicator of compromise. This is the most definite malicious activity compared to others.\n*   **user-d:** Logs show `firefox.exe` blocked on `machine04` and `cmd.com` blocked on `machine01`. These are blocked activities, so no current compromise.\n\nGiven the explicit \"security breach\" and \"threat actor still active\" context, the allowed execution of `sysconfig.exe` by `user-c` on `machine03` is the strongest indicator of compromise among the options provided, making `user-c` the account to disable first for containment.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 208,
    "question_text": "A security team is responding to malicious activity and needs to determine the scope of impact. The malicious activity appears to affect a certain version of an application used by the organization. Which of the following actions best enables the team to determine the scope of impact?",
    "options": {
      "A": "Performing a port scan",
      "B": "Inspecting egress network traffic",
      "C": "Reviewing the asset inventory",
      "D": "Analyzing user behavior"
    },
    "correct_answer": "C",
    "explanation": "When responding to malicious activity that affects a \"certain version of an application,\" determining the scope of impact means identifying all instances of that application version within the organization's environment that might be affected or compromised. The best action to achieve this is **reviewing the asset inventory (C)**. An accurate asset inventory (often stored in a CMDB or asset management system) contains records of all deployed applications, their versions, and the systems they run on. By querying the inventory for the specific vulnerable application version, the security team can quickly pinpoint all potentially impacted systems and understand the full scope of the compromise, which is critical for effective containment and eradication.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 209,
    "question_text": "An organization recently implemented a policy that requires all passwords to be rotated every 90 days. An administrator sees a large volume of failed sign-on logs from multiple servers that are often accessed by users. The administrator determines users are disconnecting from the RDP session but not logging off. Which of the following should the administrator do to prevent account lockouts?",
    "options": {
      "A": "Increase the account lockout threshold",
      "B": "Enforce password complexity",
      "C": "Automate logout of inactive sessions",
      "D": "Extend the allowed session length"
    },
    "correct_answer": "C",
    "explanation": "The core problem is that users are \"disconnecting from the RDP session but not logging off,\" leading to \"failed sign-on logs\" and presumably account lockouts due to the 90-day password rotation policy. When a user disconnects but doesn't log off, their session remains active. If their password has rotated, but the disconnected session is still using the old credentials (or if the RDP client tries to reauthenticate with old credentials), it can lead to repeated failed login attempts against the account, triggering lockout policies. \n\nTo prevent account lockouts in this scenario, the administrator should **automate logout of inactive sessions (C)**. By configuring RDP or session management settings to automatically log off (not just disconnect) user sessions after a period of inactivity, it ensures that old, potentially stale credentials in disconnected sessions do not continuously attempt to authenticate and trigger lockout policies. This cleans up inactive sessions and prevents the accumulation of failed login attempts.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 210,
    "question_text": "A security review revealed that not all of the client proxy traffic is being captured. Which of the following architectural changes best enables the capture of traffic for analysis?",
    "options": {
      "A": "Adding an additional proxy server to each segmented VLAN",
      "B": "Setting up a reverse proxy for client logging at the gateway",
      "C": "Configuring a span port on the perimeter firewall to ingest logs",
      "D": "Enabling client device logging and system event auditing"
    },
    "correct_answer": "A",
    "explanation": "The problem states that \"not all of the client proxy traffic is being captured.\" This implies a gap in the current proxy deployment or configuration. To ensure all client proxy traffic is captured for analysis, the most direct solution is to ensure all clients are indeed using a proxy that logs their activity. \n\n**A. Adding an additional proxy server to each segmented VLAN:** This approach ensures that all network segments where clients reside have a dedicated proxy server through which their traffic must pass. If traffic from a VLAN is not currently being captured, deploying a proxy server within that segmented VLAN (and configuring clients in that VLAN to use it) ensures that their outbound traffic flows through a controlled point where it can be captured and analyzed. This is a common strategy to enforce proxy usage and ensure comprehensive traffic logging.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 211,
    "question_text": "A subcontractor develops safety critical avionics software for a major aircraft manufacturer. After an incident, a third-party investigator recommends the company begin to employ formal methods in the development life cycle. Which of the following findings from the investigation most directly supports the investigator's recommendation?",
    "options": {
      "A": "The system’s bill of materials failed to include commercial and open-source libraries.",
      "B": "The company lacks dynamic and interactive application security testing standards.",
      "C": "The codebase lacks traceability to functional and non-functional requirements.",
      "D": "The implemented software inefficiently manages compute and memory resources."
    },
    "correct_answer": "C",
    "explanation": "Formal methods are mathematically-based techniques for the specification, design, and verification of software and hardware systems. They are typically used in safety-critical systems where absolute correctness is paramount. The finding that most directly supports a recommendation for formal methods is: **C. The codebase lacks traceability to functional and non-functional requirements.**\n\nFormal methods involve rigorously defining system requirements and then mathematically proving that the design and implementation meet those requirements. If the codebase lacks clear traceability (i.e., it's hard to tell which part of the code implements which requirement or if all requirements are met), it indicates a fundamental problem with ensuring the system behaves as intended. Formal methods would impose a strict, verifiable link between requirements and code, which is essential for safety-critical software where a single missed or incorrectly implemented requirement can have catastrophic consequences.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 212,
    "question_text": "A security architect is onboarding a new EDR agent on servers that traditionally do not have internet access. In order for the agent to receive updates and report back to the management console, some changes must be made. Which of the following should the architect do to best accomplish this requirement? (Choose two.)",
    "options": {
      "A": "Create a firewall rule to only allow traffic from the subnet to the internet via a proxy.",
      "B": "Configure a proxy policy that blocks all traffic on port 443",
      "C": "Configure a proxy policy that allows only fully qualified domain names needed to communicate to a portal",
      "D": "Create a firewall rule to only allow traffic from the subnet to the internet via port 443.",
      "E": "Create a firewall rule to only allow traffic from the subnet to the internet to fully qualified names that are not identified as malicious by the firewall vendor",
      "F": "Configure a proxy policy that blocks only lists of known-bad fully qualified domain names"
    },
    "correct_answer": "A C",
    "explanation": "The problem is that EDR agents on servers without internet access need to receive updates and report to a management console, which implies internet connectivity. This needs to be done securely.\n\n1.  **A. Create a firewall rule to only allow traffic from the subnet to the internet via a proxy.** This ensures that internet access for these servers is strictly controlled and monitored. By forcing all outbound internet traffic through a proxy, the organization can inspect, filter, and log all communication, preventing unauthorized access and exfiltration, while allowing the EDR agent to communicate securely.\n2.  **C. Configure a proxy policy that allows only fully qualified domain names needed to communicate to a portal.** This works in conjunction with the proxy. Instead of allowing broad access, the proxy policy should be configured with a whitelist of specific Fully Qualified Domain Names (FQDNs) that the EDR agent needs to communicate with (e.g., `updates.edr-vendor.com`, `console.edr-vendor.com`). This ensures that the servers only communicate with the legitimate EDR vendor's services for updates and reporting, greatly reducing the attack surface compared to allowing general internet access.",
    "is_simulation": false,
    "question_type": "multiple_choice_multiple_answer"
  },
  {
    "question_number": 213,
    "question_text": "Due to an infrastructure optimization plan, a company has moved from a unified architecture to a federated architecture divided by region. Long-term employees now have a better experience, but new employees are experiencing major performance issues when traveling between regions.\nThe company is reviewing the following information:\nDate and time | Region | Employee | System | Status\n1/25/2024 8:00 a.m. | Americas | 1 | Building | Access granted\n1/25/2024 8:05 a.m. | Americas | 1 | EMP1-LT | Log-in success\n1/25/2024 8:55 a.m. | Americas | 1 | EMP1-LT | Log-out success\n1/26/2024 9:00 a.m. | Europe | 1 | Building | Access granted\n1/26/2024 9:15 a.m. | Europe | 1 | EMP1-LT | Log-in success\n1/26/2024 4:55 p.m. | Europe | 1 | EMP1-LT | Log-out success\nDate and time | Region | Employee | System | Status\n1/25/2024 8:00 a.m. | Americas | 2 | Building | Access granted\n1/25/2024 8:05 a.m. | Americas | 2 | EMP1-LT | Log-in success\n1/25/2024 8:55 a.m. | Americas | 2 | EMP1-LT | Log-out success\n1/26/2024 9:00 a.m. | Europe | 2 | Building | Access denied\n1/26/2024 9:01 a.m. | Europe | 2 | Building | Access denied\n1/26/2024 9:02 a.m. | Europe | 2 | Building | Access denied\nWhich of the following is the most effective action to remediate the issue?",
    "options": {
      "A": "Creating a new user entry in the affected region for the affected employee",
      "B": "Synchronizing all regions' user identities and ensuring ongoing synchronization",
      "C": "Restarting European region physical access control systems",
      "D": "Resyncing single sign-on application with connected security appliances"
    },
    "correct_answer": "B",
    "explanation": "The problem states that the company moved to a \"federated architecture divided by region\" and \"new employees are experiencing major performance issues when traveling between regions.\" The table shows various access attempts, including `Log-in success`, `Log-out success`, and `Access denied` across different regions and systems for different employees. Specifically, Employee 2, when traveling to Europe, experiences `Access denied` for 'Building' system, despite seemingly legitimate login attempts in Americas. This suggests an issue with identity recognition or access rights when moving between federated regions.\n\n**B. Synchronizing all regions' user identities and ensuring ongoing synchronization:** In a federated architecture, especially one divided by region, inconsistent or unsynchronized user identities (usernames, passwords, group memberships, etc.) can lead to access failures and performance issues when users try to authenticate from a region where their identity is not yet fully propagated or synchronized. Ensuring all user identities are synchronized across all regions and maintaining this synchronization continuously (ongoing synchronization) will resolve access issues for new employees traveling between regions, leading to a better user experience.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 214,
    "question_text": "A company hosts a platform-as-a-service solution with a web-based front end, through which customers interact with data sets. A security administrator needs to deploy controls to prevent application-focused attacks. Which of the following most directly supports the administrator’s objective?",
    "options": {
      "A": "Improving security dashboard visualization on SIEM",
      "B": "Rotating API access and authorization keys every two months",
      "C": "Implementing application load balancing and cross-region availability",
      "D": "Creating WAF policies for relevant programming languages"
    },
    "correct_answer": "D",
    "explanation": "The objective is to \"deploy controls to prevent application-focused attacks\" on a \"web-based front end\" of a PaaS solution. Application-focused attacks include SQL injection, Cross-Site Scripting (XSS), command injection, etc.\n\n**D. Creating WAF policies for relevant programming languages:** A Web Application Firewall (WAF) is specifically designed to protect web applications from common application-layer attacks. WAFs sit in front of web applications and inspect HTTP traffic. By configuring WAF policies that are tailored to the programming languages used in the application (e.g., rules specific to PHP, Java, Node.js vulnerabilities), the WAF can effectively detect and block known attack patterns (like those for SQL injection, XSS) before they reach the application, thus directly supporting the objective of preventing application-focused attacks.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 215,
    "question_text": "A software engineer is creating a CI/CD pipeline to support the development of a web application. The DevSecOps team is required to identify syntax errors. Which of the following is the most relevant to the DevSecOps team's task?",
    "options": {
      "A": "Static application security testing",
      "B": "Software composition analysis",
      "C": "Runtime application self-protection",
      "D": "Web application vulnerability scanning"
    },
    "correct_answer": "A",
    "explanation": "The DevSecOps team's requirement is to \"identify syntax errors\" within a CI/CD pipeline for a web application. While syntax errors are primarily a code quality issue rather than a security vulnerability, within the context of security testing in DevSecOps, Static Application Security Testing (SAST) tools are the closest match. SAST tools analyze source code (or bytecode/binary code) without executing the application. While their primary function is to find security vulnerabilities, many SAST tools also have capabilities to detect code quality issues, programmatic errors, and indeed, some syntax-related issues that could lead to vulnerabilities or poor performance. It's the 'static' nature that allows for early detection of code-level flaws, including syntactic ones.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 216,
    "question_text": "A security officer received several complaints from users about excessive MFA push notifications at night. The security team investigates and suspects malicious activities regarding user account authentication. Which of the following is the best way for the security officer to restrict MFA notifications?",
    "options": {
      "A": "Provisioning FIDO2 devices",
      "B": "Deploying a text message based on MFA",
      "C": "Enabling OTP via email",
      "D": "Configuring prompt-driven MFA"
    },
    "correct_answer": "D",
    "explanation": "The problem is \"excessive MFA push notifications at night\" leading to suspicion of \"malicious activities regarding user account authentication.\" This often indicates a credential stuffing attack, where attackers use stolen credentials to try and log into accounts, triggering MFA prompts. \n\nTo restrict MFA notifications effectively and securely without blocking legitimate users (which could happen with simple time-based restrictions), the best approach is **configuring prompt-driven MFA (D)**. This means that instead of pushing an MFA notification *automatically* upon a login attempt, the MFA prompt is only triggered *after* the user has successfully entered their primary credentials *and* explicitly initiated the MFA verification (e.g., by clicking a button on the login page or a specific prompt in the authenticator app). This significantly reduces the nuisance of push notifications from failed or malicious login attempts, as the MFA challenge is not sent unless the user is truly interacting with the legitimate login portal.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 217,
    "question_text": "A security analyst is troubleshooting the reason a specific user is having difficulty accessing company resources. The analyst reviews the following information:\nUser | Source IP | Source location | User assigned location | MFA satisfied? | Sign-in status\nSALES1 | 8.11.4.16 | Germany | France | Yes | Blocked\nSALES1 | 8.11.4.16 | Germany | France | Yes | Blocked\nACCO1 | 192.168.4.18 | France | France | Yes | Allowed\nSALES1 | 8.11.4.16 | Germany | France | Yes | Blocked\nSALES1 | 8.11.4.16 | Germany | France | Yes | Blocked\nSALES2 | 8.11.4.20 | France | France | Yes | Allowed\nWhich of the following is most likely the cause of the issue?",
    "options": {
      "A": "The local network access has been configured to bypass MFA requirements.",
      "B": "A network geolocation is being misidentified by the authentication server.",
      "C": "Administrator access from an alternate location is blocked by company policy.",
      "D": "Several users have not configured their mobile devices to receive OTP codes."
    },
    "correct_answer": "B",
    "explanation": "The table shows that `SALES1` attempts to log in from `Source IP 8.11.4.16`, which is geolocated to `Germany`. However, the `User assigned location` for `SALES1` is `France`. Despite `MFA satisfied?` being `Yes`, the `Sign-in status` is `Blocked`. This pattern indicates that the user is successfully authenticating (including MFA) but is being denied access because the authentication system perceives their current location (Germany) as different from their assigned/expected location (France). This is most likely caused by **a network geolocation being misidentified by the authentication server (B)**, leading to a conditional access policy blocking the login based on the perceived location mismatch.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 218,
    "question_text": "An organization is looking for gaps in its detection capabilities based on the APTs that may target the industry. Which of the following should the security analyst use to perform threat modeling?",
    "options": {
      "A": "ATT&CK",
      "B": "OWASP",
      "C": "CAPEC",
      "D": "STRIDE"
    },
    "correct_answer": "A",
    "explanation": "The problem asks for a framework to identify \"gaps in its detection capabilities based on the APTs (Advanced Persistent Threats) that may target the industry.\" \n\n**MITRE ATT&CK (A)** is the most appropriate framework for this. ATT&CK is a globally accessible knowledge base of adversary tactics and techniques based on real-world observations. It provides detailed descriptions of how adversaries (including APTs) operate, their TTPs (Tactics, Techniques, and Procedures), and associated detection and mitigation strategies. By mapping an organization's existing security controls and detection capabilities against ATT&CK, a security analyst can systematically identify where gaps exist in detecting specific APT techniques relevant to their industry.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 219,
    "question_text": "A security analyst needs to ensure email domains that send phishing attempts without previous communications are not delivered to mailboxes.\nThe following email headers are being reviewed:\nDate | Sending domain | Reply-to domain | Subject\nApril 16 | sales.com | sales-mail.com | Updated Security Questions\nApril 18 | vendor.com | vendor.com | New Sales Catalog\nApril 18 | partner.com | partner.com | B2B Sales Increase\nApril 19 | hr-saas.com | hr-saas.com | Employee Payroll Update Request\nApril 19 | vendor.com | vendor.com | Password Requirements Not Met\nWhich of the following is the best action for the security analyst to take?",
    "options": {
      "A": "Block messages from hr-saas.com because it is not a recognized domain",
      "B": "Reroute all messages with unusual security warning notices to the IT administrator",
      "C": "Quarantine all messages with sales-mail com in the email header",
      "D": "Block vendor com for repeated attempts to send suspicious messages"
    },
    "correct_answer": "C",
    "explanation": "The objective is to prevent \"email domains that send phishing attempts without previous communications\" from being delivered. The email headers show suspicious activity, particularly with `sales-mail.com` and `hr-saas.com` having subjects like \"Updated Security Questions\" and \"Employee Payroll Update Request\", which are common phishing lures. These domains are likely typosquatted or newly registered for phishing purposes.\n\n**C. Quarantine all messages with sales-mail.com in the email header:** While multiple suspicious domains are present, `sales-mail.com` is a clear example of a domain used for phishing (impersonating a legitimate sales domain to phish for credentials). Quarantining messages based on specific suspicious domains (like `sales-mail.com` or others identified as phishing sources) that have no legitimate prior communication with the organization is a direct and effective way to prevent phishing attempts from reaching user mailboxes. This is a more targeted and immediate action than general rerouting or broad blocking.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 220,
    "question_text": "A systems administrator needs to identify new attacks that could be carried out against the environment. The administrator plans to proactively seek out and observe new attacks. Which of the following is the best way to accomplish this goal?",
    "options": {
      "A": "Configuring an IPS",
      "B": "Implementing sandboxing",
      "C": "Scanning for IoCs",
      "D": "Deploying a honeypot"
    },
    "correct_answer": "D",
    "explanation": "The objective is to \"identify new attacks\" and \"proactively seek out and observe new attacks.\" This means creating an environment that attracts and allows observation of malicious activity without risking the production network. \n\n**D. Deploying a honeypot (D)** is the best way to accomplish this. A honeypot is a security mechanism that is configured to mimic legitimate systems and applications but contains no real data or production value. Its sole purpose is to lure attackers, detect their activities, and allow security teams to learn about new attack techniques, tools, and methodologies without putting real assets at risk. This directly aligns with the goal of proactively observing new attacks.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 221,
    "question_text": "A company experienced a data breach, resulting in the disclosure of extremely sensitive data regarding a merger. As a regulated entity, the company must comply with reporting and disclosure requirements. The company is concerned about its public image and shareholder values. Which of the following best supports the organization in addressing its concerns?",
    "options": {
      "A": "Data subject access request",
      "B": "Business impact analysis",
      "C": "Supply chain management program",
      "D": "Crisis management plan"
    },
    "correct_answer": "D",
    "explanation": "The company has experienced a data breach, involving sensitive data, with regulatory compliance obligations, and concerns about public image and shareholder values. This is a full-blown crisis. To address these broad concerns, a **crisis management plan (D)** is the best support. \n\nA crisis management plan outlines an organization's strategy for responding to and recovering from disruptive events that threaten its reputation, operations, or financial stability. It encompasses not only technical incident response but also communication strategies (public image, shareholder values), legal and regulatory compliance (reporting and disclosure), and business continuity. It provides a holistic framework for managing the entire fallout of a significant breach.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 222,
    "question_text": "Company A acquired Company B and needs to determine how the acquisition will impact the attack surface of the organization as a whole. Which of the following is the best way to achieve this goal? (Choose two.)",
    "options": {
      "A": "Implementing DLP controls preventing sensitive data from leaving Company B's network",
      "B": "Documenting third-party connections used by Company B",
      "C": "Reviewing the privacy policies currently adopted by Company B",
      "D": "Requiring data sensitivity labeling for all files shared with Company B",
      "E": "Forcing a password reset requiring more stringent passwords for users on Company B's network",
      "F": "Performing an architectural review of Company B's network"
    },
    "correct_answer": "B F",
    "explanation": "To determine the impact of an acquisition on the overall attack surface, it's crucial to understand the acquired company's technical environment and external relationships.\n\n1.  **F. Performing an architectural review of Company B's network:** An architectural review involves examining the network topology, system designs, applications, and security controls of the acquired company (Company B). This deep dive helps identify existing vulnerabilities, exposed services, misconfigurations, and potential entry points that would become part of the combined organization's attack surface. It provides a comprehensive technical understanding of the risks being integrated.\n2.  **B. Documenting third-party connections used by Company B:** Third-party connections (e.g., VPNs to vendors, cloud service integrations, API access to partners) are common vectors for supply chain attacks and can significantly expand an organization's attack surface. Understanding and documenting all external interfaces and data flows of Company B is crucial for assessing new exposure points and potential risks that the acquisition introduces to the entire organization.",
    "is_simulation": false,
    "question_type": "multiple_choice_multiple_answer"
  },
  {
    "question_number": 223,
    "question_text": "After an incident occurred, a team reported during the lessons-learned review that the team:\n• Lost important information for further analysis.\n• Did not utilize the chain of communication\n• Did not follow the right steps for a proper response.\nWhich of the following solutions is the best way to address these findings?",
    "options": {
      "A": "Requesting budget for better forensic tools to improve technical capabilities for incident response operations",
      "B": "Building playbooks for different scenarios and performing regular table-top exercises",
      "C": "Requiring professional incident response certifications for each new team member",
      "D": "Publishing the incident response policy and enforcing it as part of the security awareness program"
    },
    "correct_answer": "B",
    "explanation": "The lessons learned identify several process and training gaps: lost information (implying poor evidence handling/documentation), failed communication chain, and not following proper response steps. These are all addressed by:\n\n**B. Building playbooks for different scenarios and performing regular table-top exercises:**\n*   **Building playbooks:** Playbooks provide step-by-step instructions for specific incident scenarios, ensuring that the 'right steps for a proper response' are defined and documented. They include guidelines for communication paths ('chain of communication') and data collection/preservation to avoid 'lost important information.'\n*   **Performing regular table-top exercises:** These exercises simulate incidents and allow the team to practice the playbooks, identify ambiguities, reinforce communication protocols, and train personnel on their roles and responsibilities. This practical training helps improve adherence to documented procedures and addresses the 'did not follow the right steps' finding.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 224,
    "question_text": "A security analyst is reviewing the following code in the public repository for potential risk concerns:\npublic class BoundaryCastle {\n  public static String access_token = \"spat-hfeiw-segur-werdb-werib\";\n  public static void main() { ... }\n  public static int retrieveMemory() { ... }\n  public static void state() { ... }\n}\nWhich of the following should the security analyst recommend first to remediate the vulnerability?",
    "options": {
      "A": "Developing role-based security awareness training",
      "B": "Revoking the secret used in the solution",
      "C": "Purging code from public view",
      "D": "Scanning the application with SAST"
    },
    "correct_answer": "B",
    "explanation": "The code snippet includes a hardcoded secret: `public static String access_token = \"spat-hfeiw-segur-werdb-werib\";`. This access token is directly visible in the public repository. If this token is used for authentication or authorization to sensitive resources, its exposure represents an immediate and critical security vulnerability. An attacker can easily find and misuse this token to gain unauthorized access.\n\nThe most urgent remediation for a hardcoded secret found in a public repository is to **revoke the secret used in the solution (B)**. This immediately invalidates the compromised token, preventing any further unauthorized use by attackers who may have already discovered it. After revocation, the secret should be replaced with a new, secure one and managed using a secrets management solution, and the code should be updated to retrieve the secret securely at runtime.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 225,
    "question_text": "After remote desktop capabilities were deployed in the environment various vulnerabilities were noticed:\n• Exfiltration of intellectual property\n• Unencrypted files\n• Weak user passwords\nWhich of the following is the best way to mitigate these vulnerabilities? (Choose two.)",
    "options": {
      "A": "Implementing data loss prevention",
      "B": "Deploying file integrity monitoring",
      "C": "Restricting access to critical file services only",
      "D": "Deploying directory-based group policies",
      "E": "Enabling modem authentication that supports MFA",
      "F": "Implementing a version control system",
      "G": "Implementing a CMDB platform"
    },
    "correct_answer": "A D",
    "explanation": "The vulnerabilities are \"Exfiltration of intellectual property,\" \"Unencrypted files,\" and \"Weak user passwords\" after remote desktop capabilities were deployed.\n\n1.  **A. Implementing data loss prevention (DLP):** This directly addresses \"Exfiltration of intellectual property\" and indirectly \"Unencrypted files.\" DLP solutions monitor, detect, and block sensitive data from being transferred out of the network or stored in unencrypted forms where it shouldn't be. It can prevent unauthorized sharing or exfiltration of IP, and also help identify and enforce encryption policies for files.\n2.  **D. Deploying directory-based group policies:** Group policies (like Active Directory Group Policy in Windows environments) allow administrators to centrally manage and enforce security configurations, including password policies, folder redirection, and access permissions. This directly addresses \"Weak user passwords\" by enforcing strong password requirements and can contribute to preventing unencrypted files by mandating encryption policies or restricting where files can be saved.",
    "is_simulation": false,
    "question_type": "multiple_choice_multiple_answer"
  },
  {
    "question_number": 226,
    "question_text": "An audit finding reveals that a legacy platform has not retained logs for more than 30 days. The platform has been segmented due to its interoperability with newer technology. As a temporary solution, the IT department changed the log retention to 120 days. Which of the following should the security engineer do to ensure the logs are being properly retained?",
    "options": {
      "A": "Configure a scheduled task nightly to save the logs.",
      "B": "Configure event-based triggers to export the logs at a threshold.",
      "C": "Configure the SIEM to aggregate the logs.",
      "D": "Configure a Python script to move the logs into a SQL database."
    },
    "correct_answer": "C",
    "explanation": "The problem states that a legacy platform has insufficient log retention (less than 30 days, temporarily extended to 120 days) and the security engineer needs to ensure logs are *properly retained*. The best way to achieve long-term, reliable, and compliant log retention, especially for a legacy platform, is to integrate it with a centralized logging solution.\n\n**C. Configure the SIEM to aggregate the logs:** A Security Information and Event Management (SIEM) system is designed for centralized log collection, aggregation, storage, analysis, and retention. By configuring the SIEM to ingest logs from the legacy platform, the organization can leverage the SIEM's robust data retention capabilities (which are typically configured for longer periods and compliance) to ensure the logs are properly retained. The SIEM provides a secure, searchable, and compliant repository for all logs, irrespective of the source platform's native retention capabilities.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 227,
    "question_text": "A company recently experienced an incident in which an advanced threat actor was able to shim malicious code against the hardware stack of a domain controller. The forensic team cryptographically validated that both the underlying firmware of the box and the operating system had not been compromised. However, the attacker was able to exfiltrate information from the server using a steganographic technique within LDAP. Which of the following is the best way to reduce the risk of reoccurrence?",
    "options": {
      "A": "Enforcing allow lists for authorized network ports and protocols",
      "B": "Measuring and attesting to the entire boot chain",
      "C": "Rolling the cryptographic keys used for hardware security modules",
      "D": "Using code signing to verify the source of OS updates"
    },
    "correct_answer": "A",
    "explanation": "The sophisticated attack involved shimming malicious code against the hardware stack and exfiltrating data via a steganographic technique within LDAP. While firmware and OS were not compromised (per cryptographic validation), a malicious shim (likely a form of rootkit or custom low-level code) was present. Data exfiltration via LDAP is an unusual and covert communication channel. \n\nTo reduce the risk of reoccurrence, especially against covert communication and unexpected protocol usage, **enforcing allow lists for authorized network ports and protocols (A)** is the best way. This implements a default-deny security posture for network communications. By allowing only the explicitly necessary ports (e.g., LDAP's standard port 389/636 for legitimate LDAP traffic) and protocols required for the domain controller's normal operation, any unusual or steganographic communication over LDAP (or any other port/protocol) would be blocked. This greatly limits the attacker's ability to use covert channels or unintended network pathways for C2 or exfiltration.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 228,
    "question_text": "A company designs policies and procedures for hardening containers deployed in the production environment. However, a security assessment reveals that deployed containers are not complying with the security baseline. Which of the following solutions best addresses this issue throughout early life-cycle stages?",
    "options": {
      "A": "Installing endpoint agents on each container and setting them to report when configurations drift from the baseline",
      "B": "Finding hardened container images and enforcing them as the baseline for new deployments",
      "C": "Creating a pipeline to check the containers through security gates and validating the baseline controls before the final deployment",
      "D": "Running security assessments regularly and checking for the security baseline on containers already in production"
    },
    "correct_answer": "C",
    "explanation": "The problem is that \"deployed containers are not complying with the security baseline,\" and the goal is to address this \"throughout early life-cycle stages.\" This means shifting security left, embedding checks earlier in the development and deployment process.\n\n**C. Creating a pipeline to check the containers through security gates and validating the baseline controls before the final deployment:** A CI/CD (Continuous Integration/Continuous Delivery) pipeline is designed to automate the build, test, and deployment of software, including containers. By integrating \"security gates\" into the pipeline, the organization can automatically check container images and configurations against the security baseline at various early stages (e.g., after a build, before staging, before production). If a container fails to meet the baseline controls, the pipeline can halt, preventing non-compliant containers from reaching production and ensuring that security is enforced from the start of the lifecycle.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 229,
    "question_text": "A security architect for a global organization with a distributed workforce recently received funding to deploy a CASB solution. Which of the following most likely explains the choice to use a proxy-based CASB?",
    "options": {
      "A": "The capability to block unapproved applications and services is possible.",
      "B": "Privacy compliance obligations are bypassed when using a user-based deployment.",
      "C": "Protecting and regularly rotating API secret keys requires a significant time commitment.",
      "D": "Corporate devices cannot receive certificates when not connected to on-premises devices."
    },
    "correct_answer": "A",
    "explanation": "CASB (Cloud Access Security Broker) solutions are used to extend security controls from on-premises infrastructure to cloud services. There are different CASB deployment models, including proxy-based (forward or reverse proxy) and API-based. The question asks why a *proxy-based* CASB was chosen.\n\n**A. The capability to block unapproved applications and services is possible:** Proxy-based CASBs sit in the data path between users and cloud services. This allows them to inspect traffic in real-time, enforce policies, and perform actions like blocking access to unapproved or unsanctioned cloud applications (shadow IT) or specific risky functionalities within approved apps. This real-time enforcement and blocking capability over any cloud service accessed through the proxy is a key advantage of proxy-based CASBs that API-based CASBs (which rely on API integration with cloud providers) often cannot provide for all services.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 230,
    "question_text": "A user tried to access a web page at http://10.1.11. Previously the web page did not require authentication, and now the browser is prompting for credentials. Which of the following actions would best prevent the issue from reoccurring and reduce the likelihood of credential exposure?",
    "options": {
      "A": "Implementing 802.1x EAP-TTLS on access points to reduce the risk of evil twins",
      "B": "Transitioning internal services to use DNS security",
      "C": "Modifying web server configuration and utilizing X509 certificates for authentication",
      "D": "Installing new rules for the IDS to detect impersonation attacks"
    },
    "correct_answer": "C",
    "explanation": "The problem states that a web page at an *internal* IP (`http://10.1.11`) previously did not require authentication but now prompts for credentials, and the goal is to prevent this from reoccurring and \"reduce the likelihood of credential exposure.\" The use of `http` indicates an unencrypted connection, which is a major risk for credential exposure. \n\n**C. Modifying web server configuration and utilizing X.509 certificates for authentication:**\n*   **Modify web server configuration:** The first step is to switch from `http` to `https` (HTTP over TLS/SSL). This requires reconfiguring the web server to use TLS.\n*   **Utilizing X.509 certificates for authentication:** This is critical. Instead of relying on insecure basic authentication over HTTP (which would expose credentials), X.509 certificates provide strong, secure, and often client-certificate based authentication. This can involve client certificates (for mutual TLS) or server certificates for HTTPS to secure the communication channel. This ensures that any credentials transmitted are encrypted and that authentication is cryptographically strong, significantly reducing the likelihood of credential exposure and preventing the issue from reoccurring by enforcing secure access.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 231,
    "question_text": "A compliance officer is reviewing the data sovereignty laws in several countries where the organization has no presence. Which of the following is the most likely reason for reviewing these laws?",
    "options": {
      "A": "The organization is performing due diligence of potential tax issues.",
      "B": "The organization has been subject to legal proceedings in countries where it has a presence.",
      "C": "The organization is concerned with new regulatory enforcement in other countries.",
      "D": "The organization has suffered brand reputation damage from incorrect media coverage."
    },
    "correct_answer": "C",
    "explanation": "A compliance officer reviewing data sovereignty laws in countries where the organization *has no presence* strongly suggests a proactive concern about potential future operations or expansion, or perhaps indirect data processing. The most likely reason is that **the organization is concerned with new regulatory enforcement in other countries (C)**. \n\nData sovereignty laws dictate that data is subject to the laws and regulations of the country in which it is collected, stored, or processed. Even if an organization doesn't have a physical presence, it might still process data of citizens from those countries or intend to expand there. New or stricter regulations (like GDPR, CCPA, or similar emerging laws globally) can impose obligations on organizations worldwide if they handle data belonging to citizens of those countries. Proactively reviewing these laws helps the organization prepare for or comply with potential future legal requirements, especially concerning data storage and processing locations.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 232,
    "question_text": "Audit findings indicate several user endpoints are not utilizing full disk encryption. During the remediation process, a compliance analyst reviews the testing details for the endpoints and notes the endpoint device configuration does not support full disk encryption. Which of the following is the most likely reason the device must be replaced?",
    "options": {
      "A": "The HSM is outdated and no longer supported by the manufacturer",
      "B": "The vTPM was not properly initialized and is corrupt.",
      "C": "The HSM is vulnerable to common exploits and a firmware upgrade is needed",
      "D": "The motherboard was not configured with a TPM from the OEM supplier",
      "E": "The HSM does not support sealing storage"
    },
    "correct_answer": "D",
    "explanation": "Full Disk Encryption (FDE) often relies on a hardware component called a Trusted Platform Module (TPM) for secure key storage and sealing. The problem states that \"the endpoint device configuration does not support full disk encryption\" and implies the device must be replaced. The most likely reason for this is that **the motherboard was not configured with a TPM from the OEM supplier (D)**. Many commercial FDE solutions (like BitLocker in Windows) use a TPM to securely store and protect the encryption keys and verify the integrity of the boot process. If a device lacks a TPM (or a functional, correctly configured one from the manufacturer), it cannot meet the hardware prerequisites for robust FDE, thus necessitating replacement if FDE is a mandatory security requirement.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 233,
    "question_text": "A company's security policy states that any publicly available server must be patched within 12 hours after a patch is released. A recent IIS zero-day vulnerability was discovered that affects all versions of the Windows Server OS:\nHost | OS | Externally available? | Behind WAF? | IIS installed?\nHost 1 | Windows 2019 | Yes | No | Yes\nHost 2 | Windows 2008 R2 | No | N/A | No\nHost 3 | Windows 2012 R2 | Yes | Yes | Yes\nHost 4 | Windows 2022 | Yes | No | Yes\nHost 5 | Windows 2012 R2 | No | N/A | No\nHost 6 | Windows 2019 | Yes | No | No\nWhich of the following hosts should a security analyst patch first once a patch is available?",
    "options": {
      "A": "1",
      "B": "2",
      "C": "3",
      "D": "4",
      "E": "5",
      "F": "6"
    },
    "correct_answer": "D",
    "explanation": "The security policy requires patching public-facing servers within 12 hours for zero-day vulnerabilities. The IIS zero-day vulnerability affects all versions of Windows Server OS. We need to identify the public-facing IIS servers and prioritize.\n\n*   **Host 1:** Windows 2019, Externally available? **Yes**, IIS installed? Yes. **Public-facing IIS server.**\n*   **Host 3:** Windows 2012 R2, Externally available? Yes, IIS installed? Yes. **Public-facing IIS server.**\n*   **Host 4:** Windows 2022, Externally available? **Yes**, IIS installed? Yes. **Public-facing IIS server.**\n\nHosts 1, 3, and 4 are publicly available IIS servers. Among these, Host 4 is running Windows 2022, which is the newest operating system. While Host 3 is behind a WAF (which provides some protection), Host 4 is directly exposed (`Behind WAF? No`). Therefore, Host 4 represents the most direct and modern public-facing exposure to the IIS zero-day vulnerability. Patching the most recent and exposed public-facing IIS server would be the highest priority.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 234,
    "question_text": "A central bank implements strict risk mitigations for the hardware supply chain, including an allow list for specific countries of origin. Which of the following best describes the cyberthreat to the bank?",
    "options": {
      "A": "Ability to obtain components during wartime",
      "B": "Fragility to DDoS and other availability attacks",
      "C": "Physical implants and tampering",
      "D": "Non-conformance to accepted manufacturing standards"
    },
    "correct_answer": "C",
    "explanation": "A central bank implementing \"strict risk mitigations for the hardware supply chain, including an allow list for specific countries of origin,\" is primarily concerned with the integrity and trustworthiness of the hardware components themselves. The cyberthreat this best describes is **physical implants and tampering (C)**. \n\nSupply chain attacks can involve adversaries physically installing malicious hardware components (implants) or tampering with legitimate hardware during manufacturing or transit. These implants could be used to exfiltrate data, create backdoors, or facilitate other cyberattacks. By restricting hardware procurement to an allow list of trusted countries of origin, the bank aims to reduce the risk of receiving hardware that has been maliciously altered or backdoored by state-sponsored actors or other sophisticated threats.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 235,
    "question_text": "An organization has noticed an increase in phishing campaigns utilizing typosquatting. A security analyst needs to enrich the data for commonly used domains against the domains used in phishing campaigns. The analyst uses a log forwarder to forward network logs to the SIEM. Which of the following would allow the security analyst to perform this analysis?",
    "options": {
      "A": "Use a cron job to regularly update and compare domains.",
      "B": "Create a parser that matches domains.",
      "C": "Develop a query that filters out all matching domain names.",
      "D": "Implement a dashboard on the SIEM that shows the percentage of traffic by domain."
    },
    "correct_answer": "B",
    "explanation": "The security analyst needs to \"enrich the data for commonly used domains against the domains used in phishing campaigns\" and has network logs forwarded to a SIEM. To perform this analysis, the SIEM needs to be able to extract and categorize the domain names from the logs.\n\n**B. Create a parser that matches domains:** A parser is a crucial component within a SIEM system that interprets raw log data and extracts relevant information into structured fields (e.g., source IP, destination IP, username, and in this case, domain names). To identify and compare domains (especially for typosquatting), the SIEM needs a parser that can correctly identify and extract domain names from network traffic logs (e.g., DNS queries, HTTP requests). Once parsed, these domains can then be compared against lists of legitimate domains and known typosquatting patterns to detect phishing attempts. Without proper parsing, the raw log data cannot be effectively analyzed.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 236,
    "question_text": "A security analyst wants to use lessons learned from a prior incident response to reduce dwell time in the future. The analyst is using the following data points:\nUser | Site visited | HTTP Method | Filter Status | Traffic Status | Alert\nadmin1 | toolz.com | GET | Allowed | Allowed | No\nadmin1 | hacking.com | GET | Blocked | Blocked | Yes\naccount1 | payroll.com | GET | Allowed | Allowed | No\naccount1 | p4yroll.com | POST | Blocked | Blocked | Blocked\naccount2 | p4yroll.com | POST | Blocked | Blocked | No\naccount3 | payroll.com | GET | Allowed | Allowed | No\nWhich of the following would the analyst most likely recommend?",
    "options": {
      "A": "Adjusting the SIEM to alert on attempts to visit phishing sites",
      "B": "Allowing TRACE method traffic to enable better log correlation",
      "C": "Enabling alerting on all suspicious administrator behavior",
      "D": "Utilizing allow lists on the WAF for all users using GET methods"
    },
    "correct_answer": "A",
    "explanation": "The data points show that `admin1` attempted to visit `hacking.com`, which was `Blocked` and generated an `Alert` (`Yes`). Also, `account1` and `account2` attempted to access `p4yroll.com`, which was `Blocked`. The goal is to \"reduce dwell time\" (the time an attacker is present in a system before detection). \n\nThese logs clearly indicate attempts to visit malicious/phishing sites. To reduce dwell time, the detection and alerting on these types of activities need to be immediate and robust. Therefore, the most likely recommendation is to **adjust the SIEM to alert on attempts to visit phishing sites (A)**. This ensures that any user attempting to access such sites (especially phishing or malicious domains like `hacking.com` or `p4yroll.com` which is likely a typosquatted domain for `payroll.com`) immediately triggers a high-fidelity alert, allowing for faster response and reduced dwell time.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 237,
    "question_text": "After a penetration test on the internal network the following report was generated:\nAttack | Target | Result\nCompromised host | ADMIN01$ CORP.LOCAL | Successful\nHash collected | KRBTGT.CORP.LOCAL | Successful\nPass the hash | SQLSV.CORP.LOCAL | Successful\nDomain control | CORP.LOCAL | Successful\nWhich of the following should be recommended to remediate the attack?",
    "options": {
      "A": "Deleting SQLSV",
      "B": "Reimaging ADMIN01$",
      "C": "Rotating KRBTGT password",
      "D": "Resetting the local domain"
    },
    "correct_answer": "C",
    "explanation": "The report shows a successful `Pass the hash` attack on `ADMIN01$` targeting `KRBTGT.CORP.LOCAL` with a `Successful` result. The `Hash collected` was `SQLSV.CORP.LOCAL`. \n\n*   **Pass the hash:** An attack technique where an attacker captures a user's password hash and uses it to authenticate to a remote system without knowing the plaintext password.\n*   **KRBTGT:** This is a critical account in Active Directory environments. The `KRBTGT` account is the Key Distribution Center (KDC) service account. It's used to encrypt and sign all Kerberos tickets within the domain. If the `KRBTGT` hash is compromised, an attacker can forge Kerberos tickets (Golden Ticket attack), granting themselves arbitrary access to any resource in the domain, effectively owning the entire domain.\n\nTherefore, the most critical remediation for a compromised `KRBTGT` hash is to **rotate the KRBTGT password (C)** *twice*. Rotating it twice invalidates all previously issued Kerberos tickets encrypted with the old key, forcing all systems and users to reauthenticate and obtain new tickets using the new key. This is the primary remediation for a Golden Ticket attack and compromised `KRBTGT` account.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 238,
    "question_text": "A security engineer is given the following requirements:\n• An endpoint must only execute internally signed applications.\n• Administrator accounts cannot install unauthorized software\n• Attempts to run unauthorized software must be logged\nWhich of the following best meets these requirements?",
    "options": {
      "A": "Maintaining appropriate account access through directory management and controls",
      "B": "Implementing a CSPM platform to monitor updates being pushed to applications",
      "C": "Deploying an EDR solution to monitor and respond to software installation attempts",
      "D": "Configuring application control with blocked hashes and enterprise-trusted root certificates"
    },
    "correct_answer": "D",
    "explanation": "The requirements are specific about controlling software execution:\n*   \"An endpoint must only execute internally signed applications.\"\n*   \"Administrator accounts cannot install unauthorized software.\"\n*   \"Attempts to run unauthorized software must be logged.\"\n\n**D. Configuring application control with blocked hashes and enterprise-trusted root certificates:** This option directly addresses all requirements:\n*   **Application control (whitelisting/blacklisting):** This security measure allows only explicitly approved applications to run on a system. It can be configured to block executables that are not on a whitelist or block those matching a blacklist (blocked hashes).\n*   **Enterprise-trusted root certificates:** By allowing only applications signed by certificates issued by the organization's trusted root CA, the system enforces the \"internally signed applications\" requirement. This prevents the execution of unsigned or untrusted software, even by administrators.\n*   **Logging:** Application control solutions inherently log attempts to run unauthorized software, fulfilling the logging requirement.\n\nThis is a highly effective and granular control against unauthorized software execution.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 239,
    "question_text": "After an organization met with its ISAC, the organization decided to test the resiliency of its security controls against a small number of advanced threat actors. Which of the following will enable the security administrator to accomplish this task?",
    "options": {
      "A": "Adversary emulation",
      "B": "Reliability factors",
      "C": "Deployment of a honeypot",
      "D": "Internal reconnaissance"
    },
    "correct_answer": "A",
    "explanation": "The organization wants to \"test the resiliency of its security controls against a small number of advanced threat actors.\" This requires simulating the behavior of specific, known threat groups.\n\n**A. Adversary emulation:** This is a red teaming activity where the red team mimics the tactics, techniques, and procedures (TTPs) of specific, identified advanced threat actors. Instead of just finding vulnerabilities, adversary emulation aims to test how well the organization's defensive capabilities (detection, prevention, response) perform against the realistic behaviors of those specific adversaries. This directly enables the organization to test its resiliency against the actual threats it faces.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 240,
    "question_text": "After a cybersecurity incident, a security analyst was able to collect a binary that the attacker used on the compromised server. Then the analyst ran the following command:\nstrings -n 7 binary.exe\nroot@mail> strings binary.exe\n...s.dfa.ss.d.as\n...2.3.3.1.,5.6.2.,...>@34.4.,..133\n...dhj.hgt.f.d.g.h.>92.4.,..133\n...ipconfig....5.6.2..g..g23.45.56.\nevil.info.\n...h123hsdjfjjf///...\n...c:\\windows\\\\system32\\\\temp<.xml\n1.2.3.4...e.gt.gv....5.6.5.\nMicrfoft Windows bin\n...\nMicrosoft Windows win32\n...\nWhich of the following options describes what the analyst is trying to do?",
    "options": {
      "A": "To reconstruct the timeline of commands executed by the binary",
      "B": "To extract IoCs from the binary used on the attack",
      "C": "To replicate the attack in a secure environment",
      "D": "To debug the binary to analyze low-level instructions"
    },
    "correct_answer": "B",
    "explanation": "The command `strings -n 7 binary.exe` is being run on a collected malware binary. \n\n*   `strings`: This is a command-line utility that extracts printable strings (sequences of printable characters) from binary files.\n*   `-n 7`: This option specifies that only strings of at least 7 characters in length should be extracted.\n\nWhen analyzing malware, extracting strings is a common initial step for identifying **Indicators of Compromise (IoCs) (B)**. These strings can include:\n*   Filenames that the malware interacts with (e.g., `s.dfa.ss.d.as`, `evil.info`).\n*   IP addresses or domains used for Command and Control (C2) communication (e.g., `2.3.3.1`, `5.6.2.`, `g23.45.56`).\n*   Registry keys, mutexes, service names, or other system artifacts.\n*   Error messages or internal function names.\n\nBy extracting these strings, the analyst can quickly identify network destinations, file paths, and other artifacts that can be used to further detect the malware in other systems or block its communication.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 241,
    "question_text": "A senior security engineer flags the following log file snippet as having likely facilitated an attacker's lateral movement in a recent breach:\nqry_source: 19.27.214.22 TCP/53\nqry_dest: 10.105.22.13 TCP/53\nqry_type: AXFR\nDirectoryserver1 A 10.80.8.10\nInternal-dns A 10.80.8.11\nDirectoryserver2 A 10.80.8.12\nfshare A 10.80.9.4\nzip A 10.80.9.5\nmain-apps A 10.81.22.33\nWhich of the following solutions, if implemented, would mitigate the risk of this issue reoccurring?",
    "options": {
      "A": "Disabling DNS zone transfers",
      "B": "Restricting DNS traffic to UDP/53",
      "C": "Implementing DNS masking on internal servers",
      "D": "Permitting only clients from internal networks to query DNS"
    },
    "correct_answer": "A",
    "explanation": "The log file snippet shows `qry_type: AXFR` (AXFR is DNS zone transfer). This is happening from `qry_source: 19.27.214.22` to `qry_dest: 10.105.22.13` for various internal domain names like `Directoryserver1`, `Internal-dns`, `fshare`, `zip`, and `main-apps`. DNS zone transfers are mechanisms used to replicate DNS data between DNS servers. If an unauthorized entity can request a zone transfer, they can obtain a complete list of all hostnames and IP addresses within a domain. This information is invaluable for attackers performing reconnaissance and lateral movement within a network.\n\nTherefore, the issue is that DNS zone transfers are allowed from a potentially unauthorized source, enabling an attacker to map the internal network. The solution is to **disable DNS zone transfers (A)** from being initiated by unauthorized sources. Zone transfers should be strictly limited to authorized secondary DNS servers only, typically secured with TSIG (Transaction Signature) or IP-based access controls.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 242,
    "question_text": "To prevent data breaches, security leaders at a company decide to expand user education to:\n• Create a healthy security culture.\n• Comply with regulatory requirements\n• Improve incident reporting\nWhich of the following would best meet their objective?",
    "options": {
      "A": "Performing a DoS attack",
      "B": "Scheduling regular penetration tests",
      "C": "Simulating a phishing campaign",
      "D": "Deploying fake ransomware"
    },
    "correct_answer": "C",
    "explanation": "The objectives are to \"expand user education\" to \"create a healthy security culture,\" \"comply with regulatory requirements,\" and \"improve incident reporting.\" \n\n**C. Simulating a phishing campaign:** This action directly addresses the objectives:\n\n*   **Expand user education & healthy security culture:** Phishing simulations are a highly effective educational tool. They test users' ability to recognize and report phishing attempts in a controlled environment. The results can inform targeted training, raising overall awareness and fostering a culture where reporting suspicious emails is encouraged (improving incident reporting).\n*   **Improve incident reporting:** Phishing simulations provide a safe way for users to practice reporting. Analyzing the reporting rates and methods during a simulation helps identify gaps in the incident reporting process and provides metrics to improve it.\n*   **Comply with regulatory requirements:** Many regulations and frameworks (e.g., PCI DSS, NIST) either recommend or require security awareness training, including phishing simulations, to mitigate risks related to human error.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  }
]