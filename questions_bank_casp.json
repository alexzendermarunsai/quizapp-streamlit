
[
  {
    "question_number": 1,
    "question_text": "A security administrator is performing a gap assessment against a specific OS benchmark. The benchmark requires the following configurations be applied to endpoints:\n• Full disk encryption\n• Host-based firewall\n• Time synchronization\n• Password policies\n• Application allow listing\n• Zero Trust application access\nWhich of the following solutions best addresses the requirements? (Select two).",
    "options": {
      "A": "CASB",
      "B": "SBoM",
      "C": "SCAP",
      "D": "SASE",
      "E": "HIDS"
    },
    "correct_answer": "C D",
    "explanation": "To address the specific OS benchmark configurations, the following solutions are most appropriate:\nC. SCAP (Security Content Automation Protocol): SCAP helps in automating vulnerability management and policy compliance, including configurations like full disk encryption, host-based firewalls, and password policies.\nD. SASE (Secure Access Service Edge): SASE provides a framework for Zero Trust network access and application allow listing, ensuring secure and compliant access to applications and data.\nThese solutions together cover the comprehensive security requirements specified in the OS benchmark, ensuring a robust security posture for endpoints.",
    "is_simulation": false,
    "question_type": "multiple_choice_multiple_answer"
  },
  {
    "question_number": 2,
    "question_text": "A user reports application access issues to the help desk. The help desk reviews the logs for the user:\n[Image of logs showing Time, Internal IP, Public IP, IP Geolocation, Application, and Action. Entries show access to VPN, Email, HR System from Toronto and Los Angeles at different times, with one HR System access attempt from Toronto being denied.]\nWhich of the following is most likely the reason for the issue?",
    "options": {
      "A": "The user inadvertently tripped the geoblock rule in NGFW.",
      "B": "A threat actor has compromised the user's account and attempted to log in.",
      "C": "The user is not allowed to access the human resources system outside of business hours.",
      "D": "The user did not attempt to connect from an approved subnet."
    },
    "correct_answer": "A",
    "explanation": "The logs show that the user connected from Toronto (104.18.16.29) and Los Angeles (95.67.137.12) within minutes. The sudden location change is a typical trigger for geoblocking in a Next-Generation Firewall (NGFW), leading to the HR System being denied.\nA compromised account (B) would show failed login attempts or unusual activities, but all other access attempts were allowed.\nBusiness hours restriction (C) is unlikely since the user was granted access earlier.\nApproved subnet issues (D) would affect all applications, not just HR System access.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 3,
    "question_text": "A security team is responding to malicious activity and needs to determine the scope of impact the malicious activity appears to affect certain version of an application used by the organization. Which of the following actions best enables the team to determine the scope of Impact?",
    "options": {
      "A": "Performing a port scan",
      "B": "Inspecting egress network traffic",
      "C": "Reviewing the asset inventory",
      "D": "Analyzing user behavior"
    },
    "correct_answer": "C",
    "explanation": "Reviewing the asset inventory allows the security team to identify all instances of the affected application versions within the organization. By knowing which systems are running the vulnerable versions, the team can assess the full scope of the impact, determine which systems might be compromised, and prioritize them for further investigation and remediation.\nPerforming a port scan (Option A) might help identify open ports but does not provide specific information about the application versions. Inspecting egress network traffic (Option B) and analyzing user behavior (Option D) are important steps in the incident response process but do not directly identify which versions of the application are affected.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 4,
    "question_text": "Which of the following is the main reason quantum computing advancements are leading companies and countries to deploy new encryption algorithms?",
    "options": {
      "A": "Encryption systems based on large prime numbers will be vulnerable to exploitation",
      "B": "Zero Trust security architectures will require homomorphic encryption.",
      "C": "Perfect forward secrecy will prevent deployment of advanced firewall monitoring techniques",
      "D": "Quantum computers will enable malicious actors to capture IP traffic in real time"
    },
    "correct_answer": "A",
    "explanation": "Advancements in quantum computing pose a significant threat to current encryption systems, especially those based on the difficulty of factoring large prime numbers, such as RSA. Quantum computers have the potential to solve these problems exponentially faster than classical computers, making current cryptographic systems vulnerable.\nWhy Large Prime Numbers are Vulnerable:\nShor's Algorithm: Quantum computers can use Shor's algorithm to factorize large integers efficiently, which undermines the security of RSA encryption.\nCryptographic Breakthrough: The ability to quickly factor large prime numbers means that encrypted data, which relies on the hardness of this mathematical problem, can be decrypted.\nOther options, while relevant, do not capture the primary reason for the shift towards new encryption algorithms:\nB. Zero Trust security architectures: While important, the shift to homomorphic encryption is not the main driver for new encryption algorithms.\nC. Perfect forward secrecy: It enhances security but is not the main reason for new encryption algorithms.\nD. Real-time IP traffic capture: Quantum computers pose a more significant threat to the underlying cryptographic algorithms than to the real-time capture of traffic.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 5,
    "question_text": "PKI can be used to support security requirements in the change management process. Which of the following capabilities does PKI provide for messages?",
    "options": {
      "A": "Non-repudiation",
      "B": "Confidentiality",
      "C": "Delivery receipts",
      "D": "Attestation"
    },
    "correct_answer": "A",
    "explanation": "Public Key Infrastructure (PKI) supports change management by securing messages (e.g., approvals, updates).\nNon-repudiation, provided via digital signatures, ensures a sender cannot deny sending a message, critical for auditability in change processes.\nOption A: Correct—PKI’s digital signatures ensure non-repudiation.\nOption B: Confidentiality (via encryption) is a PKI feature but less tied to change management’s focus on accountability.\nOption C: Delivery receipts are not a PKI function; they’re protocol-specific (e.g., SMTP).\nOption D: Attestation relates to verifying attributes, not a direct PKI message capability.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 6,
    "question_text": "A company wants to invest in research capabilities with the goal to operationalize the research output. Which of the following is the best option for a security architect to recommend?",
    "options": {
      "A": "Dark web monitoring",
      "B": "Threat intelligence platform",
      "C": "Honeypots",
      "D": "Continuous adversary emulation"
    },
    "correct_answer": "B",
    "explanation": "Investing in a threat intelligence platform is the best option for a company looking to operationalize research output. A threat intelligence platform helps in collecting, processing, and analyzing threat data to provide actionable insights. These platforms integrate data from various sources, including dark web monitoring, honeypots, and other security tools, to offer a comprehensive view of the threat landscape.\nWhy a Threat Intelligence Platform?\nData Integration: It consolidates data from multiple sources, including dark web monitoring and honeypots, making it easier to analyze and derive actionable insights.\nActionable Insights: Provides real-time alerts and reports on potential threats, helping the organization take proactive measures.\nOperational Efficiency: Streamlines the process of threat detection and response, allowing the security team to focus on critical issues.\nResearch and Development: Facilitates the operationalization of research output by providing a platform for continuous monitoring and analysis of emerging threats.\nOther options, while valuable, do not offer the same level of integration and operationalization capabilities:\nA. Dark web monitoring: Useful for specific threat intelligence but lacks comprehensive operationalization.\nC. Honeypots: Effective for detecting and analyzing specific attack vectors but not for broader threat intelligence.\nD. Continuous adversary emulation: Important for testing defenses but not for integrating and operationalizing threat intelligence.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 7,
    "question_text": "A company's help desk is experiencing a large number of calls from the finance department stating access issues to www.bank.com. The security operations center reviewed the following security logs:\n[Image of logs showing user, source, location, website, DNS resolved IP (public), Code.]\nWhich of the following is most likely the cause of the issue?",
    "options": {
      "A": "Recursive DNS resolution is failing",
      "B": "The DNS record has been poisoned.",
      "C": "DNS traffic is being sinkholed.",
      "D": "The DNS was set up incorrectly."
    },
    "correct_answer": "C",
    "explanation": "Sinkholing, or DNS sinkholing, is a method used to redirect malicious traffic to a safe destination. This technique is often employed by security teams to prevent access to malicious domains by substituting a benign destination IP address.\nIn the given logs, users from the finance department are accessing www.bank.com and receiving HTTP status code 495. This status code is typically indicative of a client certificate error, which can occur if the DNS traffic is being manipulated or redirected incorrectly. The consistency in receiving the same HTTP status code across different users suggests a systematic issue rather than an isolated incident.\nRecursive DNS resolution failure (A) would generally lead to inability to resolve DNS at all, not to a specific HTTP error.\nDNS poisoning (B) could result in users being directed to malicious sites, but again, would likely result in a different set of errors or unusual activity.\nIncorrect DNS setup (D) would likely cause broader resolution issues rather than targeted errors like the one seen here.\nBy reviewing the provided data, it is evident that the DNS traffic for www.bank.com is being rerouted improperly, resulting in consistent HTTP 495 errors for the finance department users. Hence, the most likely cause is that the DNS traffic is being sinkholed.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 8,
    "question_text": "A company wants to modify its process to comply with privacy requirements after an incident involving PII data in a development environment. In order to perform functionality tests, the QA team still needs to use valid data in the specified format. Which of the following best addresses the risk without impacting the development life cycle?",
    "options": {
      "A": "Encrypting the data before moving into the QA environment",
      "B": "Truncating the data to make it not personally identifiable",
      "C": "Using a large language model to generate synthetic data",
      "D": "Utilizing tokenization for sensitive fields"
    },
    "correct_answer": "D",
    "explanation": "Tokenization replaces sensitive data (e.g., PII) with non-sensitive placeholders while maintaining format consistency, ensuring compliance without disrupting testing. This method is commonly used for PCI-DSS and GDPR compliance while preserving data structure for functional tests.\nEncryption (A) secures data but does not remove sensitivity or solve testing concerns.\nTruncation (B) removes portions of data but may impact testing if format requirements are strict.\nSynthetic data (C) can be useful but may not always match real-world scenarios perfectly for testing purposes.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 9,
    "question_text": "A software engineer is creating a CI/CD pipeline to support the development of a web application. The DevSecOps team is required to identify syntax errors. Which of the following is the most relevant to the DevSecOps team's task?",
    "options": {
      "A": "Static application security testing",
      "B": "Software composition analysis",
      "C": "Runtime application self-protection",
      "D": "Web application vulnerability scanning"
    },
    "correct_answer": "A",
    "explanation": "Static Application Security Testing (SAST) involves analyzing source code or compiled code for security vulnerabilities without executing the program. This method is well-suited for identifying syntax errors, coding standards violations, and potential security issues early in the development lifecycle.\nA. Static application security testing (SAST): SAST tools analyze the source code to detect syntax errors, vulnerabilities, and other issues before the code is run. This is the most relevant task for the DevSecOps team to identify syntax errors and improve code quality.\nB. Software composition analysis: This focuses on identifying vulnerabilities in open-source components and libraries used in the application but does not address syntax errors directly.\nC. Runtime application self-protection (RASP): RASP involves monitoring and protecting applications during runtime, which does not help in identifying syntax errors during the development phase.\nD. Web application vulnerability scanning: This involves scanning the running application for vulnerabilities but does not address syntax errors in the code.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 10,
    "question_text": "A company recently experienced an incident in which an advanced threat actor was able to shim malicious code against the hardware static of a domain controller. The forensic team cryptographically validated that the underlying firmware of the box and the operating system had not been compromised. However, the attacker was able to exfiltrate information from the server using a steganographic technique within LDAP. Which of the following is the best way to reduce the risk of reoccurrence?",
    "options": {
      "A": "Enforcing allow lists for authorized network ports and protocols",
      "B": "Measuring and attesting to the entire boot chain",
      "C": "Rolling the cryptographic keys used for hardware security modules",
      "D": "Using code signing to verify the source of OS updates"
    },
    "correct_answer": "A",
    "explanation": "The scenario describes a sophisticated attack where the threat actor used steganography within LDAP to exfiltrate data. Given that the hardware and OS firmware were validated and found uncompromised, the attack vector likely exploited a network communication channel. To mitigate such risks, enforcing allow lists for authorized network ports and protocols is the most effective strategy.\nHere’s why this option is optimal:\nPort and Protocol Restrictions: By creating an allow list, the organization can restrict communications to only those ports and protocols that are necessary for legitimate business operations. This reduces the attack surface by preventing unauthorized or unusual traffic.\nNetwork Segmentation: Enforcing such rules helps in segmenting the network and ensuring that only approved communications occur, which is critical in preventing data exfiltration methods like steganography.\nPreventing Unauthorized Access: Allow lists ensure that only predefined, trusted connections are allowed, blocking potential paths that attackers could use to infiltrate or exfiltrate data.\nOther options, while beneficial in different contexts, are not directly addressing the network communication threat:\nB. Measuring and attesting to the entire boot chain: While this improves system integrity, it doesn’t directly mitigate the risk of data exfiltration through network channels.\nC. Rolling the cryptographic keys used for hardware security modules: This is useful for securing data and communications but doesn’t directly address the specific method of exfiltration described.\nD. Using code signing to verify the source of OS updates: Ensures updates are from legitimate sources, but it doesn’t mitigate the risk of network-based data exfiltration.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 11,
    "question_text": "Which of the following best explains the business requirement a healthcare provider fulfills by encrypting patient data at rest?",
    "options": {
      "A": "Securing data transfer between hospitals",
      "B": "Providing for non-repudiation of data",
      "C": "Reducing liability from identity theft",
      "D": "Protecting privacy while supporting portability"
    },
    "correct_answer": "D",
    "explanation": "Encrypting patient data at rest ensures that sensitive information is protected from unauthorized access, thereby maintaining patient privacy. Additionally, encryption supports data portability by allowing secure transfer and storage of data across different systems and devices without compromising confidentiality. This practice is crucial for healthcare providers to comply with regulations such as the Health Insurance Portability and Accountability Act (HIPAA), which mandates the protection of patient information.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 12,
    "question_text": "A company isolated its OT systems from other areas of the corporate network. These systems are required to report usage information over the internet to the vendor. Which of the following best reduces the risk of compromise or sabotage? (Select two).",
    "options": {
      "A": "Implementing allow lists",
      "B": "Monitoring network behavior",
      "C": "Encrypting data at rest",
      "D": "Performing boot Integrity checks",
      "E": "Executing daily health checks",
      "F": "Implementing a site-to-site IPSec VPN"
    },
    "correct_answer": "A F",
    "explanation": "A. Implementing allow lists: Allow lists (whitelisting) restrict network communication to only authorized devices and applications, significantly reducing the attack surface by ensuring that only pre-approved traffic is permitted.\nF. Implementing a site-to-site IPSec VPN: A site-to-site VPN provides a secure, encrypted tunnel for data transmission between the OT systems and the vendor, protecting the data from interception and tampering during transit.\nOther options:\nB. Monitoring network behavior: While useful for detecting anomalies, it does not proactively reduce the risk of compromise or sabotage.\nC. Encrypting data at rest: Important for protecting data stored on devices, but does not address network communication risks.\nD. Performing boot integrity checks: Ensures the integrity of the system at startup but does not protect ongoing network communications.\nE. Executing daily health checks: Useful for maintaining system health but does not directly reduce the risk of network-based compromise or sabotage.",
    "is_simulation": false,
    "question_type": "multiple_choice_multiple_answer"
  },
  {
    "question_number": 13,
    "question_text": "A security analyst is performing a review of a web application. During testing as a standard user, the following error log appears:\nError Message in Database Connection\nConnection to host USA-WebApp-Database failed\nDatabase \"Prod-DB01\" not found\nTable \"CustomerInfo\" not found\nPlease retry your request later\nWhich of the following best describes the analyst’s findings and a potential mitigation technique?",
    "options": {
      "A": "The findings indicate unsecure references. All potential user input needs to be properly sanitized.",
      "B": "The findings indicate unsecure protocols. All cookies should be marked as HttpOnly.",
      "C": "The findings indicate information disclosure. The displayed error message should be modified.",
      "D": "The findings indicate a SQL injection. The database needs to be upgraded."
    },
    "correct_answer": "C",
    "explanation": "The error message reveals sensitive details (hostnames, database names, table names), constituting information disclosure. This aids attackers in reconnaissance. Mitigation involves modifying the application to display generic error messages (e.g., “An error occurred”) instead of specifics.\nOption A: Unsecure references suggest coding flaws, but this is a configuration/output issue, not input sanitization.\nOption B: Unsecure protocols and HttpOnly cookies relate to session security, not error handling.\nOption C: Correct—information disclosure is the issue; generic errors mitigate it.\nOption D: No evidence of SQL injection (e.g., manipulated input); upgrading the database doesn’t address disclosure.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 14,
    "question_text": "A network engineer must ensure that always-on VPN access is enabled and restricted to company assets. Which of the following best describes what the engineer needs to do?",
    "options": {
      "A": "Generate device certificates using the specific template settings needed",
      "B": "Modify signing certificates in order to support IKE version 2",
      "C": "Create a wildcard certificate for connections from public networks",
      "D": "Add the VPN hostname as a SAN entry on the root certificate"
    },
    "correct_answer": "A",
    "explanation": "To ensure always-on VPN access is enabled and restricted to company assets, the network engineer needs to generate device certificates using the specific template settings required for the company's VPN solution. These certificates ensure that only authorized devices can establish a VPN connection.\nWhy Device Certificates are Necessary:\nAuthentication: Device certificates authenticate company assets, ensuring that only authorized devices can access the VPN.\nSecurity: Certificates provide a higher level of security compared to username and password combinations, reducing the risk of unauthorized access.\nCompliance: Certificates help in meeting security policies and compliance requirements by ensuring that only managed devices can connect to the corporate network.\nOther options do not provide the same level of control and security for always-on VPN access:\nB. Modify signing certificates for IKE version 2: While important for VPN protocols, it does not address device-specific authentication.\nC. Create a wildcard certificate: This is not suitable for device-specific authentication and could introduce security risks.\nD. Add the VPN hostname as a SAN entry: This is more related to certificate management and does not ensure device-specific authentication.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 15,
    "question_text": "An audit finding reveals that a legacy platform has not retained logs for more than 30 days. The platform has been segmented due to its interoperability with newer technology. As a temporary solution, the IT department changed the log retention to 120 days. Which of the following should the security engineer do to ensure the logs are being properly retained?",
    "options": {
      "A": "Configure a scheduled task nightly to save the logs",
      "B": "Configure event-based triggers to export the logs at a threshold.",
      "C": "Configure the SIEM to aggregate the logs",
      "D": "Configure a Python script to move the logs into a SQL database."
    },
    "correct_answer": "C",
    "explanation": "To ensure that logs from a legacy platform are properly retained beyond the default retention period, configuring the SIEM to aggregate the logs is the best approach. SIEM solutions are designed to collect, aggregate, and store logs from various sources, providing centralized log management and retention. This setup ensures that logs are retained according to policy and can be easily accessed for analysis and compliance purposes.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 16,
    "question_text": "An external SaaS solution user reports a bug associated with the role-based access control module. This bug allows users to bypass system logic associated with client segmentation in the multitenant deployment model. When assessing the bug report, the developer finds that the same bug was previously identified and addressed in an earlier release. The developer then determines the bug was reintroduced when an existing software component was integrated from a prior version of the platform. Which of the following is the best way to prevent this scenario?",
    "options": {
      "A": "Regression testing",
      "B": "Code signing",
      "C": "Automated test and retest",
      "D": "User acceptance testing",
      "E": "Software composition analysis"
    },
    "correct_answer": "A",
    "explanation": "Regression testing is a software testing practice that ensures that recent code changes have not adversely affected existing functionalities. In this scenario, the reintroduction of a previously fixed bug indicates that changes or integrations brought back the old issue. Implementing comprehensive regression testing would help detect such reintroductions by systematically retesting the existing functionalities whenever changes are made to the codebase. This practice is crucial in maintaining the integrity of the application, especially in complex systems where multiple components interact.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 17,
    "question_text": "A security officer performs due diligence activities before implementing a third-party solution into the enterprise environment. The security officer needs evidence from the third party that a data subject access request handling process is in place. Which of the following is the security officer most likely seeking to maintain compliance?",
    "options": {
      "A": "Information security standards",
      "B": "E-discovery requirements",
      "C": "Privacy regulations",
      "D": "Certification requirements",
      "E": "Reporting frameworks"
    },
    "correct_answer": "C",
    "explanation": "Comprehensive and Detailed Step-by-Step Explanation:\nPrivacy regulations (C), such as GDPR (General Data Protection Regulation) and CCPA (California Consumer Privacy Act), require companies to provide data subject access request (DSAR) handling processes. A DSAR allows individuals to request details about their personal data stored by a company and request modifications or deletions.\nInformation security standards (A) focus on overall security controls, while e-discovery requirements (B) relate to legal investigations rather than ongoing compliance.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 18,
    "question_text": "Previously intercepted communications must remain secure even if a current encryption key is compromised in the future. Which of the following best supports this requirement?",
    "options": {
      "A": "Tokenization",
      "B": "Key stretching",
      "C": "Forward secrecy",
      "D": "Simultaneous authentication of equals"
    },
    "correct_answer": "C",
    "explanation": "Forward secrecy (FS) ensures that past encrypted data remains secure even if encryption keys are compromised in the future. It generates ephemeral session keys that are not reused.\nOther options:\nA (Tokenization) replaces sensitive data with tokens but does not prevent key compromise.\nB (Key stretching) makes brute-force attacks harder but does not ensure secrecy after compromise.\nD (Simultaneous Authentication of Equals – SAE) is used in WPA3 but is not related to past communication security.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 19,
    "question_text": "An organization is planning for disaster recovery and continuity of operations, and has noted the following relevant findings:\n1. A natural disaster may disrupt operations at Site A, which would then cause an evacuation. Users are unable to log into the domain from their workstations after relocating to Site B.\n2. A natural disaster may disrupt operations at Site A, which would then cause the pump room at Site B to become inoperable.\n3. A natural disaster may disrupt operations at Site A, which would then cause unreliable internet connectivity at Site B due to route flapping.\nINSTRUCTIONS\nMatch each relevant finding to the affected host by clicking on the host name and selecting the appropriate number.\nFor findings 1 and 2, select the items that should be replicated to Site B. For finding 3, select the item requiring configuration changes, then select the appropriate corrective action from the drop-down menu.\n[Image of network diagram with Site A (Directory Server, Application Server 01, PLC, HVAC, SCADA Master Controller, Web Server 01, Internet) and Site B (Application Server 03, Application Server 04, File Server, PLC, Pumps, VPN Concentrator, Internet). Finding numbers 1, 2, 3 are shown as drag-and-drop elements.]\n[Image showing a dropdown menu for selecting the appropriate corrective action for Finding 3. Options include: Select corrective action, Modify the BGP configuration, Update the firmware version, Integrate a WAF, Synchronize the SIEM database, Increase the bandwidth at the site, Update the SCADA master controller software, Implement AV Software.]",
    "options": {},
    "correct_answer": null,
    "explanation": "Matching Relevant Findings to the Affected Hosts:\nFinding 1:\nAffected Host: DNS (Implied by \"unable to log into the domain\"). The diagram shows \"Directory Server\" at Site A which would handle DNS/Domain login services.\nReason: Users are unable to log into the domain from their workstations after relocating to Site B, which implies a failure in domain name services that are critical for user authentication and domain login.\nFinding 2:\nAffected Host: Pumps (at Site B).\nReason: The pump room at Site B becoming inoperable directly points to the critical infrastructure components associated with pumping operations.\nFinding 3:\nAffected Host: VPN Concentrator (at Site B).\nReason: Unreliable internet connectivity at Site B due to route flapping indicates issues with network routing, which is often managed by VPN concentrators that handle site-to-site connectivity.\n\nReplication to Site B:\nFor Finding 1 (DNS/Directory Server): The Directory Server at Site A should be replicated to Site B.\nFor Finding 2 (Pumps): The PLC controlling the Pumps at Site B should be replicated to Site B.\n\nCorrective Actions for Finding 3:\nAffected Host: VPN Concentrator.\nAction: Modify the BGP configuration.\nReason: Route flapping is often related to issues with Border Gateway Protocol (BGP) configurations. Adjusting BGP settings can stabilize routes and improve internet connectivity reliability. VPN concentrators, which manage connections between sites, are typically configured with BGP for optimal routing.",
    "is_simulation": true,
    "question_type": "simulation"
  },
  {
    "question_number": 20,
    "question_text": "Which of the following best explains the importance of determining organization risk appetite when operating with a constrained budget?",
    "options": {
      "A": "Risk appetite directly impacts acceptance of high-impact low-likelihood events.",
      "B": "Organizational risk appetite varies from organization to organization",
      "C": "Budgetary pressure drives risk mitigation planning in all companies",
      "D": "Risk appetite directly influences which breaches are disclosed publicly"
    },
    "correct_answer": "A",
    "explanation": "Risk appetite is the amount of risk an organization is willing to accept to achieve its objectives. When operating with a constrained budget, understanding the organization's risk appetite is crucial because:\nIt helps prioritize security investments based on the level of risk the organization is willing to tolerate.\nHigh-impact, low-likelihood events may be deemed acceptable if they fall within the organization's risk appetite, allowing for budget allocation to other critical areas.\nProperly understanding and defining risk appetite ensures that limited resources are used effectively to manage risks that align with the organization's strategic goals.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 21,
    "question_text": "Emails that the marketing department is sending to customers are going to the customers' spam folders. The security team is investigating the issue and discovers that the certificates used by the email server were reissued, but DNS records had not been updated. Which of the following should the security team update in order to fix this issue? (Select three.)",
    "options": {
      "A": "DMARC",
      "B": "SPF",
      "C": "DKIM",
      "D": "DNSSEC",
      "E": "SASC",
      "F": "SAN",
      "G": "SOA",
      "H": "MX"
    },
    "correct_answer": "A B C",
    "explanation": "To prevent emails from being marked as spam, several DNS records related to email authentication need to be properly configured and updated when there are changes to the email server's certificates:\nA. DMARC (Domain-based Message Authentication, Reporting & Conformance): DMARC records help email servers determine how to handle messages that fail SPF or DKIM checks, improving email deliverability and reducing the likelihood of emails being marked as spam.\nB. SPF (Sender Policy Framework): SPF records specify which mail servers are authorized to send email on behalf of your domain. Updating the SPF record ensures that the new email server is recognized as an authorized sender.\nC. DKIM (DomainKeys Identified Mail): DKIM adds a digital signature to email headers, allowing the receiving server to verify that the email has not been tampered with and is from an authorized sender. Updating DKIM records ensures that emails are properly signed and authenticated.\nD. DNSSEC (Domain Name System Security Extensions): DNSSEC adds security to DNS by enabling DNS responses to be verified. While important for DNS security, it does not directly address the issue of emails being marked as spam.\nE. SASC: This is not a relevant standard for this scenario.\nF. SAN (Subject Alternative Name): SAN is used in SSL/TLS certificates for securing multiple domain names, not for email delivery issues.\nG. SOA (Start of Authority): SOA records are used for DNS zone administration and do not directly impact email deliverability.\nH. MX (Mail Exchange): MX records specify the mail servers responsible for receiving email on behalf of a domain. While important, the primary issue here is the authentication of outgoing emails, which is handled by SPF, DKIM, and DMARC.",
    "is_simulation": false,
    "question_type": "multiple_choice_multiple_answer"
  },
  {
    "question_number": 22,
    "question_text": "You are tasked with integrating a new B2B client application with an existing OAuth workflow that must meet the following requirements:\n- The application does not need to know the users' credentials.\n- An approval interaction between the users and the HTTP service must be orchestrated.\n- The application must have limited access to users' data.\nINSTRUCTIONS\nUse the drop-down menus to select the action items for the appropriate locations. All placeholders must be filled.\n[Image showing a diagram of an OAuth workflow with components: Resource owner, Client Application (B2B client application), Authorization server, Resource server. Dropdown menus labeled \"Select Action Item\" are present for Authorization server, Resource server, and B2B client application.]\n[Image showing the same diagram with the dropdown menus open, listing options like: Access issued tokens, Grant access, Authorize access to other applications.]",
    "options": {},
    "correct_answer": null,
    "explanation": "Select the Action Items for the Appropriate Locations:\nAuthorization Server:\nAction Item: Grant access\nThe authorization server's role is to authenticate the user and then issue an authorization code or token that the client application can use to access resources. Granting access involves the server authenticating the resource owner and providing the necessary tokens for the client application.\nResource Server:\nAction Item: Access issued tokens\nThe resource server is responsible for serving the resources requested by the client application. It must verify the issued tokens from the authorization server to ensure the client has the right permissions to access the requested data.\nB2B Client Application:\nAction Item: Authorize access to other applications\nThe B2B client application must handle the OAuth flow to authorize access on behalf of the user without requiring direct knowledge of the user's credentials. This includes obtaining authorization tokens from the authorization server and using them to request access to the resource server.\nDetailed Explanation:\nOAuth 2.0 is designed to provide specific authorization flows for web applications, desktop applications, mobile phones, and living room devices. The integration involves multiple steps and components, including:\nResource Owner (User):\nThe user owns the data and resources that are being accessed.\nClient Application (B2B Client Application):\nRequests access to the resources controlled by the resource owner but does not directly handle the user's credentials. Instead, it uses tokens obtained through the OAuth flow.\nAuthorization Server:\nHandles the authentication of the resource owner and issues the access tokens to the client application upon successful authentication.\nResource Server:\nHosts the resources that the client application wants to access. It verifies the access tokens issued by the authorization server before granting access to the resources.\nOAuth Workflow:\nThe resource owner accesses the client application.\nThe client application redirects the resource owner to the authorization server for authentication.\nThe authorization server authenticates the resource owner and asks for consent to grant access to the client application.\nUpon consent, the authorization server issues an authorization code or token to the client application.\nThe client application uses the authorization code or token to request access to the resources from the resource server.\nThe resource server verifies the token with the authorization server and, if valid, grants access to the requested resources.",
    "is_simulation": true,
    "question_type": "simulation"
  },
  {
    "question_number": 23,
    "question_text": "A financial technology firm works collaboratively with business partners in the industry to share threat intelligence within a central platform. This collaboration gives partner organizations the ability to obtain and share data associated with emerging threats from a variety of adversaries. Which of the following should the organization most likely leverage to facilitate this activity? (Select two).",
    "options": {
      "A": "CWPP",
      "B": "YAKA",
      "C": "ATTACK",
      "D": "STIX",
      "E": "TAXII",
      "F": "JTAG"
    },
    "correct_answer": "D E",
    "explanation": "D. STIX (Structured Threat Information eXpression): STIX is a standardized language for representing threat information in a structured and machine-readable format. It facilitates the sharing of threat intelligence by ensuring that data is consistent and can be easily understood by all parties involved.\nE. TAXII (Trusted Automated eXchange of Indicator Information): TAXII is a transport mechanism that enables the sharing of cyber threat information over a secure and trusted network. It works in conjunction with STIX to automate the exchange of threat intelligence among organizations.\nOther options:\nA. CWPP (Cloud Workload Protection Platform): This focuses on securing cloud workloads and is not directly related to threat intelligence sharing.\nB. YARA: YARA is used for malware research and identifying patterns in files, but it is not a platform for sharing threat intelligence.\nC. ATT&CK: This is a knowledge base of adversary tactics and techniques but does not facilitate the sharing of threat intelligence data.\nF. JTAG: JTAG is a standard for testing and debugging integrated circuits, not related to threat intelligence.",
    "is_simulation": false,
    "question_type": "multiple_choice_multiple_answer"
  },
  {
    "question_number": 24,
    "question_text": "An organization currently has IDS, firewall, and DLP systems in place. The systems administrator needs to integrate the tools in the environment to reduce response time. Which of the following should the administrator use?",
    "options": {
      "A": "SOAR",
      "B": "CWPP",
      "C": "XCCDF",
      "D": "CMDB"
    },
    "correct_answer": "A",
    "explanation": "Comprehensive and Detailed Step-by-Step Explanation:\nIntegrating IDS, firewall, and DLP to reduce response time requires orchestration and automation. Let’s evaluate:\nA. SOAR (Security Orchestration, Automation, and Response): SOAR integrates security tools, automates workflows, and speeds up incident response. It’s the best fit for this scenario, as CAS-005 highlights SOAR for operational efficiency.\nB. CWPP (Cloud Workload Protection Platform): Focused on securing cloud workloads, not integrating on-premises tools.\nC. XCCDF (Extensible Configuration Checklist Description Format): A standard for compliance checklists, not a tool for integration or response.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 25,
    "question_text": "A security engineer is reviewing the following logs:\n[Image of logs showing Account, Host, Log-in date, Log-in time, Location, which is likely used for analysis.]\nWhich of the following is the security engineer most likely doing?",
    "options": {
      "A": "Assessing log in activities using geolocation to tune impossible Travel rate alerts",
      "B": "Reporting on remote log-in activities to track team metrics",
      "C": "Threat hunting for suspicious activity from an insider threat",
      "D": "Baselining user behavior to support advanced analytics"
    },
    "correct_answer": "A",
    "explanation": "In the given scenario, the security engineer is likely examining login activities and their associated geolocations. This type of analysis is aimed at identifying unusual login patterns that might indicate an impossible travel scenario. An impossible travel scenario is when a single user account logs in from geographically distant locations in a short time, which is physically impossible. By assessing login activities using geolocation, the engineer can tune alerts to identify and respond to potential security breaches more effectively.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 26,
    "question_text": "A company wants to install a three-tier approach to separate the web, database, and application servers. A security administrator must harden the environment. Which of the following is the best solution?",
    "options": {
      "A": "Deploying a VPN to prevent remote locations from accessing server VLANs",
      "B": "Configuring a SASb solution to restrict users to server communication",
      "C": "Implementing microsegmentation on the server VLANs",
      "D": "Installing a firewall and making it the network core"
    },
    "correct_answer": "C",
    "explanation": "The best solution to harden a three-tier environment (web, database, and application servers) is to implement microsegmentation on the server VLANs. Here’s why:\nEnhanced Security: Microsegmentation creates granular security zones within the data center, allowing for more precise control over east-west traffic between servers. This helps prevent lateral movement by attackers who may gain access to one part of the network.\nIsolation of Tiers: By segmenting the web, database, and application servers, the organization can apply specific security policies and controls to each segment, reducing the risk of cross-tier attacks.\nCompliance and Best Practices: Microsegmentation aligns with best practices for network security and helps meet compliance requirements by ensuring that sensitive data and systems are properly isolated and protected.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 27,
    "question_text": "Due to locality and budget constraints, an organization’s satellite office has a lower bandwidth allocation than other offices. As a result, the local security infrastructure staff is assessing architectural options that will help preserve network bandwidth and increase speed to both internal and external resources while not sacrificing threat visibility. Which of the following would be the best option to implement?",
    "options": {
      "A": "Distributed connection allocation",
      "B": "Local caching",
      "C": "Content delivery network",
      "D": "SD-WAN vertical heterogeneity"
    },
    "correct_answer": "B",
    "explanation": "The goal is to optimize bandwidth, increase speed, and maintain threat visibility in a low-bandwidth satellite office. Local caching stores frequently accessed data locally, reducing bandwidth usage by minimizing repeated requests to external or internal resources. It speeds up access and doesn’t inherently reduce security visibility if paired with monitoring tools.\nOption A: Distributed connection allocation might balance traffic but doesn’t directly reduce bandwidth usage or speed up access.\nOption B: Local caching is ideal—reduces bandwidth, improves performance, and maintains visibility with proper security controls.\nOption C: A CDN is great for external content delivery but less relevant for internal resources and doesn’t inherently address threat visibility.\nOption D: SD-WAN improves WAN performance, but \"vertical heterogeneity\" is vague and not a standard term; it’s less tailored to this scenario than caching.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 28,
    "question_text": "A developer makes a small change to a resource allocation module on a popular social media website and causes a memory leak. During a peak utilization period, several web servers crash, causing the website to go offline. Which of the following testing techniques is the most efficient way to prevent this from reoccurring?",
    "options": {
      "A": "Load",
      "B": "Smoke",
      "C": "Regression",
      "D": "Canary"
    },
    "correct_answer": "C",
    "explanation": "Comprehensive and Detailed Step-by-Step Explanation:\nRegression testing ensures that new changes do not break existing functionality. It would have identified the memory leak before deployment, preventing downtime.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 29,
    "question_text": "After a company discovered a zero-day vulnerability in its VPN solution, the company plans to deploy cloud-hosted resources to replace its current on-premises systems. An engineer must find an appropriate solution to facilitate trusted connectivity. Which of the following capabilities is the most relevant?",
    "options": {
      "A": "Container orchestration",
      "B": "Microsegmentation",
      "C": "Conditional access",
      "D": "Secure access service edge"
    },
    "correct_answer": "D",
    "explanation": "Comprehensive and Detailed Explanation:\nThe scenario involves replacing an on-premises VPN solution, which has a zero-day vulnerability, with cloud-hosted resources while ensuring trusted connectivity. Trusted connectivity in a cloud environment implies secure, scalable, and modern access control that goes beyond traditional VPNs. Let’s analyze the options:\nA. Container orchestration: This refers to managing and automating containerized workloads (e.g., Kubernetes). While useful for application deployment, it doesn’t directly address secure connectivity to cloud resources.\nB. Microsegmentation: This involves creating fine-grained security policies within a network to limit lateral movement. It’s valuable for internal security but isn’t a complete solution for trusted connectivity to cloud-hosted resources.\nC. Conditional access: This ensures access based on conditions (e.g., user identity, device health). It’s relevant for identity management but lacks the broader networking and security scope needed here.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 30,
    "question_text": "A company hosts a platform-as-a-service solution with a web-based front end, through which customer interact with data sets. A security administrator needs to deploy controls to prevent application-focused attacks. Which of the following most directly supports the administrator's objective?",
    "options": {
      "A": "Improving security dashboard visualization on SIEM",
      "B": "Rotating API access and authorization keys every two months",
      "C": "Implementing application load balancing and cross-region availability",
      "D": "Creating WAF policies for relevant programming languages"
    },
    "correct_answer": "D",
    "explanation": "The best way to prevent application-focused attacks for a platform-as-a-service solution with a web-based front end is to create Web Application Firewall (WAF) policies for relevant programming languages. Here's why:\nApplication-Focused Attack Prevention: WAFs are designed to protect web applications by filtering and monitoring HTTP traffic between a web application and the Internet. They help prevent attacks such as SQL injection, cross-site scripting (XSS), and other application-layer attacks.\nCustomizable Rules: WAF policies can be tailored to the specific programming languages and frameworks used by the web application, providing targeted protection based on known vulnerabilities and attack patterns.\nReal-Time Protection: WAFs provide real-time protection, blocking malicious requests before they reach the application, thereby enhancing the security posture of the platform.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 31,
    "question_text": "A security engineer needs to secure the OT environment based on the following requirements:\n• Isolate the OT network segment\n• Restrict Internet access.\n• Apply security updates to workstations\n• Provide remote access to third-party vendors\nWhich of the following design strategies should the engineer implement to best meet these requirements?",
    "options": {
      "A": "Deploy a jump box on the third party network to access the OT environment and provide updates using a physical delivery method on the workstations",
      "B": "Implement a bastion host in the OT network with security tools in place to monitor access and use a dedicated update server for the workstations.",
      "C": "Enable outbound internet access on the OT firewall to any destination IP address and use the centralized update server for the workstations",
      "D": "Create a staging environment on the OT network for the third-party vendor to access and enable automatic updates on the workstations."
    },
    "correct_answer": "B",
    "explanation": "To secure the Operational Technology (OT) environment based on the given requirements, the best approach is to implement a bastion host in the OT network. The bastion host serves as a secure entry point for remote access, allowing third-party vendors to connect while being monitored by security tools. Using a dedicated update server for workstations ensures that security updates are applied in a controlled manner without direct internet access.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 32,
    "question_text": "An organization is developing an AI-enabled digital worker to help employees complete common tasks such as template development, editing, research, and scheduling. As part of the AI workload the organization wants to Implement guardrails within the platform. Which of the following should the company do to secure the AI environment?",
    "options": {
      "A": "Limit the platform's abilities to only non-sensitive functions",
      "B": "Enhance the training model's effectiveness.",
      "C": "Grant the system the ability to self-govern",
      "D": "Require end-user acknowledgement of organizational policies."
    },
    "correct_answer": "A",
    "explanation": "Limiting the platform's abilities to only non-sensitive functions helps to mitigate risks associated with AI operations. By ensuring that the AI-enabled digital worker is only allowed to perform tasks that do not involve sensitive or critical data, the organization reduces the potential impact of any security breaches or misuse.\nEnhancing the training model's effectiveness (Option B) is important but does not directly address security guardrails. Granting the system the ability to self-govern (Option C) could increase risk as it may act beyond the organization's control. Requiring end-user acknowledgement of organizational policies (Option D) is a good practice but does not implement technical guardrails to secure the AI environment.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 33,
    "question_text": "A security engineer needs to review the configurations of several devices on the network to meet the following requirements:\n• The PostgreSQL server must only allow connectivity in the 10.1.2.0/24 subnet.\n• The SSH daemon on the database server must be configured to listen to port 4022.\n• The SSH daemon must only accept connections from a Single workstation.\n• All host-based firewalls must be disabled on all workstations.\n• All devices must have the latest updates from within the past eight days.\n• All HDDs must be configured to secure data at rest.\n• Cleartext services are not allowed.\n• All devices must be hardened when possible.\nInstructions:\nClick on the various workstations and network devices to review the posture assessment results. Remediate any possible issues or indicate that no issue is found.\nClick on Server A to review output data. Select commands in the appropriate tab to remediate connectivity problems to the pOSTGREsql DATABASE VIA ssh\n[Image of network diagram showing WAP A, Laptop A, PC A, Switch A, Laptop B, PC B, PC C, Switch B, Firewall, Server A, Server B, Internet.]\n[Images of posture assessment results for WAP A, PC A, Laptop A, Switch A, Switch B, Laptop B, PC B, PC C, showing various configuration details and a checkbox for 'No issue' or remediation options.]\n[Image of Server A Nmap scan output showing open ports (22/tcp ssh, 80/tcp http, 443/tcp ssl/http, 1433/tcp mssql, 5432/tcp postgresql) and IP Tables tabs. The IP Tables tab shows iptables commands.]\n[Image showing the IP Tables tab on Server A with specific iptables commands selected.]",
    "options": {},
    "correct_answer": null,
    "explanation": "Based on the instructions, the primary task for Server A is to remediate SSH connectivity problems according to the requirements.\nThe requirements state:\n• The SSH daemon on the database server must be configured to listen to port 4022.\n• The SSH daemon must only accept connections from a Single workstation.\n\nThe Nmap scan shows SSH (port 22) is open on Server A (10.1.2.25). The hardening guideline says non-secure protocols should be disabled and default ports should not be used.\n\nRemediation for Server A (SSH via port 4022): The text explanation describes editing the sshd_config file to change the port from 22 to 4022 and restarting the service. It also implicitly suggests configuring firewall rules (like iptables) to allow SSH connections only on port 4022 from the specified workstation (though the single workstation requirement isn't explicitly addressed in the provided iptables commands, the general principle of controlling SSH access via firewall rules is relevant).\n\nThe provided screenshot of the IP Tables tab on Server A shows iptables commands being selected:\n- iptables -A INPUT 1 -p tcp -s 10.1.2.25/32 --dport 4022 -j ACCEPT (Allows incoming TCP traffic on port 4022 from 10.1.2.25/32 - this IP is Server A itself, likely meant to be the allowed workstation's IP)\n- iptables -P INPUT DROP (Sets the default input policy to DROP)\n- iptables -F OUTPUT (Flushes the OUTPUT chain)\n- iptables -A INPUT -p tcp -d 0/0 -s 10.1.2.0/24 --dport 5432 -m state --state NEW,ESTABLISHED -j ACCEPT (Allows new and established incoming TCP traffic on port 5432 (PostgreSQL) from the 10.1.2.0/24 subnet)\n- iptables -A INPUT -p tcp -d 0/0 -s 10.1.2.0/24 --dport 22 -m state --state NEW,ESTABLISHED -j DROP (Denies new and established incoming TCP traffic on port 22 from the 10.1.2.0/24 subnet)\n- iptables -P FORWARD DROP (Sets the default forward policy to DROP)\n\nThese iptables commands align with:\n- Restricting PostgreSQL access to the 10.1.2.0/24 subnet (Requirement met).\n- Configuring SSH on port 4022 and blocking on port 22 (partial remediation, the sshd_config change is also needed).\n- Setting default policies to drop unwanted traffic.",
    "is_simulation": true,
    "question_type": "simulation"
  },
  {
    "question_number": 34,
    "question_text": "A user from the sales department opened a suspicious file attachment. The sales department then contacted the SOC to investigate a number of unresponsive systems, and the team successfully identified the file and the origin of the attack. Which of the following is the next step of the incident response plan?",
    "options": {
      "A": "Remediation",
      "B": "Containment",
      "C": "Response",
      "D": "Recovery"
    },
    "correct_answer": "B",
    "explanation": "Incident response follows a standard process (e.g., NIST 800-61): Preparation, Identification, Containment, Eradication, Recovery, Lessons Learned. After identifying the attack (file and origin), the next step is Containment—limiting the spread or impact (e.g., isolating systems) before remediation or recovery.\nOption A: Remediation (fixing the root cause) follows containment.\nOption B: Correct—containment prevents further damage post-identification.\nOption C: “Response” is too vague; it encompasses all steps.\nOption D: Recovery (restoring systems) comes after containment and eradication.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 35,
    "question_text": "A company wants to implement hardware security key authentication for accessing sensitive information systems. The goal is to prevent unauthorized users from gaining access with a stolen password. Which of the following models should the company implement to best solve this issue?",
    "options": {
      "A": "Rule based",
      "B": "Time-based",
      "C": "Role based",
      "D": "Context-based"
    },
    "correct_answer": "D",
    "explanation": "Context-based authentication enhances traditional security methods by incorporating additional layers of information about the user's current environment and behavior. This can include factors such as the user's location, the time of access, the device used, and the behavior patterns. It is particularly useful in preventing unauthorized access even if an attacker has obtained a valid password.\nRule-based (A) focuses on predefined rules and is less flexible in adapting to dynamic threats.\nTime-based (B) authentication considers the time factor but doesn't provide comprehensive protection against stolen credentials.\nRole-based (C) is more about access control based on the user's role within the organization rather than authenticating the user based on current context.\nBy implementing context-based authentication, the company can ensure that even if a password is compromised, the additional contextual factors required for access (which an attacker is unlikely to possess) provide a robust defense mechanism.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 36,
    "question_text": "A security analyst detected unusual network traffic related to program updating processes. The analyst collected artifacts from compromised user workstations. The discovered artifacts were binary files with the same name as existing, valid binaries but with different hashes. Which of the following solutions would most likely prevent this situation from reoccurring?",
    "options": {
      "A": "Improving patching processes",
      "B": "Implementing digital signatures",
      "C": "Performing manual updates via USB ports",
      "D": "Allowing only files from internal sources"
    },
    "correct_answer": "B",
    "explanation": "Implementing digital signatures ensures the integrity and authenticity of software binaries. When a binary is digitally signed, any tampering with the file (e.g., replacing it with a malicious version) would invalidate the signature. This allows systems to verify the origin and integrity of binaries before execution, preventing the execution of unauthorized or compromised binaries.\nA. Improving patching processes: While important, this does not directly address the issue of verifying the integrity of binaries.\nB. Implementing digital signatures: This ensures that only valid, untampered binaries are executed, preventing attackers from substituting legitimate binaries with malicious ones.\nC. Performing manual updates via USB ports: This is not practical and does not scale well, especially in large environments.\nD. Allowing only files from internal sources: This reduces the risk but does not provide a mechanism to verify the integrity of binaries.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 37,
    "question_text": "After remote desktop capabilities were deployed in the environment, various vulnerabilities were noticed.\n• Exfiltration of intellectual property\n• Unencrypted files\n• Weak user passwords\nWhich of the following is the best way to mitigate these vulnerabilities? (Select two).",
    "options": {
      "A": "Implementing data loss prevention",
      "B": "Deploying file integrity monitoring",
      "C": "Restricting access to critical file services only",
      "D": "Deploying directory-based group policies",
      "E": "Enabling modern authentication that supports MFA",
      "F": "Implementing a version control system",
      "G": "Implementing a CMDB platform"
    },
    "correct_answer": "A E",
    "explanation": "To mitigate the identified vulnerabilities, the following solutions are most appropriate:\nA. Implementing data loss prevention (DLP): DLP solutions help prevent the unauthorized transfer of data outside the organization. This directly addresses the exfiltration of intellectual property by monitoring, detecting, and blocking sensitive data transfers.\nE. Enabling modern authentication that supports Multi-Factor Authentication (MFA): This significantly enhances security by requiring additional verification methods beyond just passwords. It addresses the issue of weak user passwords by making it much harder for unauthorized users to gain access, even if they obtain the password.\nOther options, while useful in specific contexts, do not address all the vulnerabilities mentioned:\nB. Deploying file integrity monitoring helps detect changes to files but does not prevent data exfiltration or address weak passwords.\nC. Restricting access to critical file services improves security but is not comprehensive enough to mitigate all identified vulnerabilities.\nD. Deploying directory-based group policies can enforce security policies but might not directly prevent data exfiltration or ensure strong authentication.\nF. Implementing a version control system helps manage changes to files but is not a security measure for preventing the identified vulnerabilities.\nG. Implementing a CMDB platform (Configuration Management Database) helps manage IT assets but does not address the specific security issues mentioned.",
    "is_simulation": false,
    "question_type": "multiple_choice_multiple_answer"
  },
  {
    "question_number": 38,
    "question_text": "A company detects suspicious activity associated with external connections. Security detection tools are unable to categorize this activity. Which of the following is the best solution to help the company overcome this challenge?",
    "options": {
      "A": "Implement an Interactive honeypot",
      "B": "Map network traffic to known IoCs.",
      "C": "Monitor the dark web",
      "D": "Implement UEBA"
    },
    "correct_answer": "D",
    "explanation": "User and Entity Behavior Analytics (UEBA) is the best solution to help the company overcome challenges associated with suspicious activity that cannot be categorized by traditional detection tools. UEBA uses advanced analytics to establish baselines of normal behavior for users and entities within the network. It then identifies deviations from these baselines, which may indicate malicious activity. This approach is particularly effective for detecting unknown threats and sophisticated attacks that do not match known indicators of compromise (IoCs).",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 39,
    "question_text": "A security engineer must ensure that sensitive corporate information is not exposed if a company laptop is stolen. Which of the following actions best addresses this requirement?",
    "options": {
      "A": "Utilizing desktop as a service for all company data and multifactor authentication",
      "B": "Using explicit allow lists of specific IP addresses and deploying single sign-on",
      "C": "Deploying mobile device management and requiring stronger passwords",
      "D": "Updating security mobile reporting policies and monitoring data breaches"
    },
    "correct_answer": "A",
    "explanation": "Utilizing Desktop as a Service (DaaS) means that data and applications are hosted in the cloud rather than on the local device. In the event of a laptop theft, no sensitive data resides on the device, thereby preventing unauthorized access. Coupling DaaS with multifactor authentication (MFA) adds an additional layer of security, ensuring that only authorized users can access the cloud-hosted data and applications. This combination effectively mitigates the risk of data exposure due to device theft.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 40,
    "question_text": "A security engineer must resolve a vulnerability in a deprecated version of Python for a custom-developed flight simulation application that is monitored and controlled remotely. The source code is proprietary and built with Python functions running on the Ubuntu operating system. Version control is not enabled for the application in development or production. However, the application must remain online in the production environment using built-in features. Which of the following solutions best reduces the attack surface of these issues and meets the outlined requirements?",
    "options": {
      "A": "Configure code-signing within the CI/CD pipeline, update Python with aptitude, and update modules with pip in a test environment. Deploy the solution to production.",
      "B": "Enable branch protection in the GitHub repository. Update Python with aptitude, and update modules with pip in a test environment. Deploy the solution to production.",
      "C": "Use an NFS network share. Update Python with aptitude, and update modules with pip in a test environment. Deploy the solution to production.",
      "D": "Configure version designation within the Python interpreter. Update Python with aptitude, and update modules with pip in a test environment. Deploy the solution to production."
    },
    "correct_answer": "A",
    "explanation": "Code-signing within the CI/CD pipeline ensures that only verified and signed code is deployed, mitigating the risk of supply chain attacks. Updating Python with aptitude and updating modules with pip ensures vulnerabilities are patched. Deploying the solution to production after testing maintains application availability while securing the development lifecycle.\nBranch protection (B) applies only to version-controlled environments, which is not the case here.\nNFS network share (C) does not address the deprecated Python vulnerability.\nVersion designation (D) does not eliminate security risks from outdated dependencies.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 41,
    "question_text": "A company’s internal network is experiencing a security breach, and the threat actor is still active. Due to business requirements, users in this environment are allowed to utilize multiple machines at the same time. Given the following log snippet:\n[Image of logs showing Time, User, Process, Status, Machine.]\nWhich of the following accounts should a security analyst disable to best contain the incident without impacting valid users?",
    "options": {
      "A": "user-a",
      "B": "user-b",
      "C": "user-c",
      "D": "user-d"
    },
    "correct_answer": "C",
    "explanation": "User user-c is showing anomalous behavior across multiple machines, attempting to run administrative tools such as cmd.exe and appwiz.CPL, which are commonly used by attackers for system modification. The activity pattern suggests a lateral movement attempt, potentially indicating a compromised account.\nuser-a (A) and user-b (B) attempted to run applications but only on one machine, suggesting less likelihood of compromise.\nuser-d (D) was blocked running cmd.com, but user-c’s pattern is more consistent with an attack technique.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 42,
    "question_text": "A technician is reviewing the logs and notices a large number of files were transferred to remote sites over the course of three months. This activity then stopped. The files were transferred via TLS-protected HTTP sessions from systems that do not normally send traffic to those sites. The technician will define this threat as:",
    "options": {
      "A": "A decrypting RSA using an obsolete and weakened encryption attack.",
      "B": "A zero-day attack.",
      "C": "An advanced persistent threat.",
      "D": "An on-path attack."
    },
    "correct_answer": "C",
    "explanation": "The scenario describes a prolonged, stealthy operation where files were exfiltrated over three months via secure channels (TLS-protected HTTP) from unexpected systems, then ceased. This aligns with an Advanced Persistent Threat (APT), characterized by long-term, targeted attacks aimed at data theft or surveillance, often using sophisticated methods to remain undetected.\nOption A: Decrypting RSA with weak encryption implies a cryptographic attack, but TLS suggests modern encryption was used, and there’s no evidence of decryption here.\nOption B: A zero-day attack exploits unknown vulnerabilities, but the duration and cessation suggest a planned operation, not a single exploit.\nOption C: APT fits perfectly—slow, persistent exfiltration from unusual systems indicates a coordinated, stealthy threat actor.\nOption D: An on-path (man-in-the-middle) attack intercepts traffic, but there’s no indication of interception; the focus is on unauthorized transfers.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 43,
    "question_text": "A security analyst is reviewing the following authentication logs:\n[Image of authentication logs showing date, time, computer, account, Ping-10 (Y/N).]\nWhich of the following should the analyst do first?",
    "options": {
      "A": "Disable User2's account",
      "B": "Disable User12's account",
      "C": "Disable User8's account",
      "D": "Disable User1's account"
    },
    "correct_answer": "D",
    "explanation": "Based on the provided authentication logs, we observe that User1's account experienced multiple failed login attempts within a very short time span (at 8:01:23 AM on 12/15). This pattern indicates a potential brute-force attack or an attempt to gain unauthorized access. Here’s a breakdown of why disabling User1's account is the appropriate first step:\nFailed Login Attempts: The logs show that User1 had four consecutive failed login attempts: VM01 at 8:01:23 AM, VM08 at 8:01:23 AM, VM01 at 8:01:23 AM, VM08 at 8:01:23 AM.\nSecurity Protocols and Best Practices: According to CompTIA Security+ guidelines, multiple failed login attempts within a short timeframe should trigger an immediate response to prevent further potential unauthorized access attempts. This typically involves temporarily disabling the account to stop ongoing brute force attacks.\nAccount Lockout Policy: Implementing an account lockout policy is a standard practice to thwart brute-force attacks. Disabling User1's account will align with these best practices and prevent further failed attempts, which might lead to successful unauthorized access if not addressed.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 44,
    "question_text": "During the course of normal SOC operations, three anomalous events occurred and were flagged as potential IoCs. Evidence for each of these potential IoCs is provided.\nINSTRUCTIONS\nReview each of the events and select the appropriate analysis and remediation options for each IoC.\n[Image showing tabs for IoC 1, IoC 2, IoC 3. IoC 1 shows logs for Apache_httpd, DNSQ, Dest @10.1.1.1:53, @10.1.2.5, Data update.s.domain, CNAME 3a129sk219r9slmfkzzz000.s.domain, 108.158.253.253.]\n[Image showing IoC 2 logs for Src 10.0.5.5, Dst 10.1.2.1, 10.1.2.2, 10.1.2.3, 10.1.2.4, 10.1.2.5, Proto IP_ICMP, Data ECHO, Action Drop.]\n[Image showing IoC 3 logs for Proxylog GET /announce?... User-Agent: RAZA 2.1.0.0.]\n[Images showing dropdown menus for Analysis and Remediation options for each IoC.]",
    "options": {},
    "correct_answer": null,
    "explanation": "Analysis and Remediation Options for Each IoC:\nIoC 1:\nEvidence:\nSource: Apache_httpd\nType: DNSQ\nDest: @10.1.1.1:53, @10.1.2.5\nData: update.s.domain, CNAME 3a129sk219r9slmfkzzz000.s.domain, 108.158.253.253\nAnalysis:\nAnalysis: The service is attempting to resolve a malicious domain.\nReason: The DNS queries and the nature of the CNAME resolution indicate that the service is trying to resolve potentially harmful domains, which is a common tactic used by malware to connect to command-and-control servers.\nRemediation:\nRemediation: Implement a blocklist for known malicious ports. (Note: The provided text explanation says \"Implement a blocklist for known malicious domains at the DNS level\". The screenshot dropdown options might not match this exactly, but the general intent is to block resolution to bad domains/IPs).\nReason: Blocking known malicious domains at the DNS level prevents the resolution of harmful domains, thereby protecting the network from potential connections to malicious servers.\n\nIoC 2:\nEvidence:\nSrc: 10.0.5.5\nDst: 10.1.2.1, 10.1.2.2, 10.1.2.3, 10.1.2.4, 10.1.2.5\nProto: IP_ICMP\nData: ECHO\nAction: Drop\nAnalysis:\nAnalysis: Someone is footprinting a network subnet.\nReason: The repeated ICMP ECHO requests to different addresses within a subnet indicate that someone is scanning the network to discover active hosts, a common reconnaissance technique used by attackers.\nRemediation:\nRemediation: Block ping requests across the WAN interface. (Note: The provided text explanation says \"Block ping requests across the WAN interface\". The screenshot dropdown options might vary. The general intent is to block ICMP traffic).\nReason: Blocking ICMP ECHO requests on the WAN interface can prevent attackers from using ping sweeps to gather information about the network topology and active devices.\n\nIoC 3:\nEvidence:\nProxylog:\nGET /announce?... User-Agent: RAZA 2.1.0.0\nHost: localhost\nConnection: Keep-Alive\nHTTP 200 OK\nAnalysis:\nAnalysis: An employee is using P2P services to download files.\nReason: The HTTP GET request with parameters related to a BitTorrent client indicates that the employee is using peer-to-peer (P2P) services, which can lead to unauthorized data transfer and potential security risks.\nRemediation:\nRemediation: Enforce endpoint controls on third-party software installations. (Note: The provided text explanation says \"Enforce endpoint controls on third-party software installations\". The screenshot dropdown options might vary).\nReason: By enforcing strict endpoint controls, you can prevent the installation and use of unauthorized software, such as P2P clients, thereby mitigating the risk of data leaks and other security threats associated with such applications.",
    "is_simulation": true,
    "question_type": "simulation"
  },
  {
    "question_number": 45,
    "question_text": "A developer needs to improve the cryptographic strength of a password-storage component in a web application without completely replacing the crypto-module. Which of the following is the most appropriate technique?",
    "options": {
      "A": "Key splitting",
      "B": "Key escrow",
      "C": "Key rotation",
      "D": "Key encryption",
      "E": "Key stretching"
    },
    "correct_answer": "E",
    "explanation": "The most appropriate technique to improve the cryptographic strength of a password-storage component in a web application without completely replacing the crypto-module is key stretching. Here's why:\nEnhanced Security: Key stretching algorithms, such as PBKDF2, bcrypt, and scrypt, increase the computational effort required to derive the encryption key from the password, making brute-force attacks more difficult and time-consuming.\nCompatibility: Key stretching can be implemented alongside existing cryptographic modules, enhancing their security without the need for a complete overhaul.\nIndustry Best Practices: Key stretching is a widely recommended practice for securely storing passwords, as it significantly improves resistance to password-cracking attacks.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 46,
    "question_text": "An organization wants to manage specialized endpoints and needs a solution that provides the ability to\n• Centrally manage configurations\n• Push policies.\n• Remotely wipe devices\n• Maintain asset inventory\nWhich of the following should the organization do to best meet these requirements?",
    "options": {
      "A": "Use a configuration management database",
      "B": "Implement a mobile device management solution.",
      "C": "Configure contextual policy management",
      "D": "Deploy a software asset manager"
    },
    "correct_answer": "B",
    "explanation": "To meet the requirements of centrally managing configurations, pushing policies, remotely wiping devices, and maintaining an asset inventory, the best solution is to implement a Mobile Device Management (MDM) solution.\nMDM Capabilities:\nCentral Management: MDM allows administrators to manage the configurations of all devices from a central console.\nPolicy Enforcement: MDM solutions enable the push of security policies and updates to ensure compliance across all managed devices.\nRemote Wipe: In case a device is lost or stolen, MDM provides the capability to remotely wipe the device to protect sensitive data.\nAsset Inventory: MDM maintains an up-to-date inventory of all managed devices, including their configurations and installed applications.\nOther options do not provide the same comprehensive capabilities required for managing specialized endpoints.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 47,
    "question_text": "An organization is required to\n• Respond to internal and external inquiries in a timely manner\n• Provide transparency.\n• Comply with regulatory requirements\nThe organization has not experienced any reportable breaches but wants to be prepared if a breach occurs in the future. Which of the following is the best way for the organization to prepare?",
    "options": {
      "A": "Outsourcing the handling of necessary regulatory filing to an external consultant",
      "B": "Integrating automated response mechanisms into the data subject access request process",
      "C": "Developing communication templates that have been vetted by internal and external counsel",
      "D": "Conducting lessons-learned activities and integrating observations into the crisis management plan"
    },
    "correct_answer": "C",
    "explanation": "Preparing communication templates that have been vetted by both internal and external counsel ensures that the organization can respond quickly and effectively to internal and external inquiries, comply with regulatory requirements, and provide transparency in the event of a breach.\nWhy Communication Templates?\nTimely Response: Pre-prepared templates ensure that responses are ready to be deployed quickly, reducing response time.\nRegulatory Compliance: Templates vetted by counsel ensure that all communications meet legal and regulatory requirements.\nConsistent Messaging: Ensures that all responses are consistent, clear, and accurate, maintaining the organization’s credibility.\nCrisis Management: Pre-prepared templates are a critical component of a broader crisis management plan, ensuring that all stakeholders are informed appropriately.\nOther options, while useful, do not provide the same level of preparedness and compliance:\nA. Outsourcing to an external consultant: This may delay response times and lose internal control over the communication.\nB. Integrating automated response mechanisms: Useful for efficiency but not for ensuring compliant and vetted responses.\nD. Conducting lessons-learned activities: Important for improving processes but does not provide immediate preparedness for communication.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 48,
    "question_text": "A hospital provides tablets to its medical staff to enable them to more quickly access and edit patients' charts. The hospital wants to ensure that if a tablet is identified as lost or stolen and a remote command is issued, the risk of data loss can be mitigated within seconds. The tablets are configured as follows to meet hospital policy:\n• Full disk encryption is enabled\n• \"Always On\" corporate VPN is enabled\n• USB-backed keystore is enabled/ready.\n• Wi-Fi 6 is configured with SAE.\n• Location services is disabled.\n• Application allow list is configured\nWhich of the following actions is the best way to mitigate this issue?",
    "options": {
      "A": "Revoking the user certificates used for VPN and Wi-Fi access",
      "B": "Performing cryptographic obfuscation",
      "C": "Using geolocation to find the device",
      "D": "Configuring the application allow list to only permit emergency calls",
      "E": "Returning the device's solid-state media to zero"
    },
    "correct_answer": "E",
    "explanation": "To mitigate the risk of data loss on a lost or stolen tablet quickly, the most effective strategy is to return the device's solid-state media to zero, which effectively erases all data on the device. Here's why:\nImmediate Data Erasure: Returning the solid-state media to zero ensures that all data is wiped instantly, mitigating the risk of data loss if the device is lost or stolen.\nFull Disk Encryption: Even though the tablets are already encrypted, physically erasing the data ensures that no residual data can be accessed if someone attempts to bypass encryption.\nCompliance and Security: This method adheres to best practices for data security and compliance, ensuring that sensitive patient data cannot be accessed by unauthorized parties.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 49,
    "question_text": "A security analyst is using data provided from a recent penetration test to calculate CVSS scores to prioritize remediation. Which of the following metric groups would the analyst need to determine to get the overall scores? (Select three).",
    "options": {
      "A": "Temporal",
      "B": "Availability",
      "C": "Integrity",
      "D": "Confidentiality",
      "E": "Base",
      "F": "Environmental",
      "G": "Impact",
      "H": "Attack vector"
    },
    "correct_answer": "A E F",
    "explanation": "The Common Vulnerability Scoring System (CVSS) v3.1 uses three metric groups to calculate overall scores: Base, Temporal, and Environmental.\nBase (E): Mandatory metrics assessing exploitability (e.g., attack vector) and impact (confidentiality, integrity, availability).\nTemporal (A): Optional metrics reflecting the current state of the vulnerability (e.g., exploit availability, remediation level).\nEnvironmental (F): Optional metrics tailoring the score to the organization’s context (e.g., security requirements).\nB, C, D (Availability, Integrity, Confidentiality): These are subcomponents of the Base Impact metrics, not standalone groups.\nG (Impact): A category within Base, not a group.\nH (Attack vector): A single Base metric, not a group.",
    "is_simulation": false,
    "question_type": "multiple_choice_multiple_answer"
  },
  {
    "question_number": 50,
    "question_text": "A compliance officer is reviewing the data sovereignty laws in several countries where the organization has no presence. Which of the following is the most likely reason for reviewing these laws?",
    "options": {
      "A": "The organization is performing due diligence of potential tax issues.",
      "B": "The organization has been subject to legal proceedings in countries where it has a presence.",
      "C": "The organization is concerned with new regulatory enforcement in other countries",
      "D": "The organization has suffered brand reputation damage from incorrect media coverage"
    },
    "correct_answer": "C",
    "explanation": "Reviewing data sovereignty laws in countries where the organization has no presence is likely due to concerns about regulatory enforcement. Data sovereignty laws dictate how data can be stored, processed, and transferred across borders. Understanding these laws is crucial for compliance, especially if the organization handles data that may be subject to foreign regulations.\nA. The organization is performing due diligence of potential tax issues: This is less likely as tax issues are generally not directly related to data sovereignty laws.\nB. The organization has been subject to legal proceedings in countries where it has a presence: While possible, this does not explain the focus on countries where the organization has no presence.\nC. The organization is concerned with new regulatory enforcement in other countries: This is the most likely reason. New regulations could impact the organization’s operations, especially if they involve data transfers or processing data from these countries.\nD. The organization has suffered brand reputation damage from incorrect media coverage: This is less relevant to the need for reviewing data sovereignty laws.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 51,
    "question_text": "During a security assessment using an EDR solution, a security engineer generates the following report about the assets in the system:\n[Image of report showing Device, Type, Status.]\nAfter five days, the EDR console reports an infection on the host 0WIN23 by a remote access Trojan. Which of the following is the most probable cause of the infection?",
    "options": {
      "A": "OWIN23 uses a legacy version of Windows that is not supported by the EDR",
      "B": "LN002 was not supported by the EDR solution and propagates the RAT",
      "C": "The EDR has an unknown vulnerability that was exploited by the attacker.",
      "D": "0WIN29 spreads the malware through other hosts in the network"
    },
    "correct_answer": "A",
    "explanation": "OWIN23 is running Windows 7, which is a legacy operating system. Many EDR solutions no longer provide full support for outdated operating systems like Windows 7, which has reached its end of life and is no longer receiving security updates from Microsoft. This makes such systems more vulnerable to infections and attacks, including remote access Trojans (RATs).\nA. OWIN23 uses a legacy version of Windows that is not supported by the EDR: This is the most probable cause because the lack of support means that the EDR solution may not fully protect or monitor this system, making it an easy target for infections.\nB. LN002 was not supported by the EDR solution and propagates the RAT: While LN002 is unmanaged, it is less likely to propagate the RAT to OWIN23 directly without an established vector.\nC. The EDR has an unknown vulnerability that was exploited by the attacker: This is possible but less likely than the lack of support for an outdated OS.\nD. OWIN29 spreads the malware through other hosts in the network: While this could happen, the status indicates OWIN29 is in a bypass mode, which might limit its interactions but does not directly explain the infection on OWIN23.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 52,
    "question_text": "After an incident response exercise, a security administrator reviews the following table:\n[Image of table showing Service, Risk rating, Criticality rating, Alert severity.]\nWhich of the following should the administrator do to best support rapid incident response in the future?",
    "options": {
      "A": "Automate alerting to IT support for phone system outages.",
      "B": "Enable dashboards for service status monitoring",
      "C": "Send emails for failed log-in attempts on the public website",
      "D": "Configure automated Isolation of human resources systems"
    },
    "correct_answer": "B",
    "explanation": "Enabling dashboards for service status monitoring is the best action to support incident response. The table shows various services with different risk, criticality, and alert severity ratings. To ensure timely and effective incident response, real-time visibility into the status of these services is crucial.\nWhy Dashboards for Service Status Monitoring?\nReal-time Visibility: Dashboards provide an at-a-glance view of the current status of all critical services, enabling rapid detection of issues.\nCentralized Monitoring: A single platform to monitor the status of multiple services helps streamline incident response efforts.\nProactive Alerting: Dashboards can be configured to show alerts and anomalies immediately, ensuring that incidents are addressed as soon as they arise.\nImproved Decision Making: Real-time data helps incident response teams make informed decisions quickly, reducing downtime and mitigating impact.\nOther options, while useful, do not offer the same level of comprehensive, real-time visibility and proactive alerting:\nA. Automate alerting to IT support for phone system outages: This addresses one service but does not provide a holistic view.\nC. Send emails for failed log-in attempts on the public website: This is a specific alert for one type of issue and does not cover all services.\nD. Configure automated isolation of human resources systems: This is a reactive measure for a specific service and does not provide real-time status monitoring.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 53,
    "question_text": "An organization is researching the automation capabilities for systems within an OT network. A security analyst wants to assist with creating secure coding practices and would like to learn about the programming languages used on the PLCs. Which of the following programming languages is the most relevant for PLCs?",
    "options": {
      "A": "Ladder logic",
      "B": "Rust",
      "C": "C",
      "D": "Python",
      "E": "Java"
    },
    "correct_answer": "A",
    "explanation": "Programmable Logic Controllers (PLCs) in Operational Technology (OT) environments commonly use Ladder Logic, a graphical programming language resembling electrical relay logic diagrams. It’s the most relevant for PLCs due to its widespread use in industrial automation.\nOption A: Ladder Logic is the standard for PLC programming, making it the best choice.\nOption B: Rust is modern and secure but not typically used for PLCs.\nOption C: C is used in some embedded systems but less common for PLCs.\nOption D: Python is versatile but not native to PLC programming.\nOption E: Java is rare in PLC contexts.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 54,
    "question_text": "Third parties notified a company's security team about vulnerabilities in the company's application. The security team determined these vulnerabilities were previously disclosed in third-party libraries. Which of the following solutions best addresses the reported vulnerabilities?",
    "options": {
      "A": "Using IaC to include the newest dependencies",
      "B": "Creating a bug bounty program",
      "C": "Implementing a continuous security assessment program",
      "D": "Integrating a SAST tool as part of the pipeline"
    },
    "correct_answer": "D",
    "explanation": "The best solution to address reported vulnerabilities in third-party libraries is integrating a Static Application Security Testing (SAST) tool as part of the development pipeline. Here’s why:\nEarly Detection: SAST tools analyze source code for vulnerabilities before the code is compiled. This allows developers to identify and fix security issues early in the development process.\nContinuous Security: By integrating SAST tools into the CI/CD pipeline, the organization ensures continuous security assessment of the codebase, including third-party libraries, with each code commit and build.\nComprehensive Analysis: SAST tools provide a detailed analysis of the code, identifying potential vulnerabilities in both proprietary code and third-party dependencies, ensuring that known issues in libraries are addressed promptly.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 55,
    "question_text": "A security engineer is implementing a code signing requirement for all code developed by the organization. Currently, the PKI only generates website certificates. Which of the following steps should the engineer perform first?",
    "options": {
      "A": "Add a new template on the internal CA with the correct attributes.",
      "B": "Generate a wildcard certificate for the internal domain.",
      "C": "Recalculate a public/private key pair for the root CA.",
      "D": "Implement a SAN for all internal web applications."
    },
    "correct_answer": "A",
    "explanation": "To enable code signing with an existing PKI, the first step is to configure the Certificate Authority (CA) to issue code signing certificates. Adding a new template with attributes specific to code signing (e.g., key usage for signing) allows the CA to support this requirement without disrupting existing operations.\nOption A: Correct—templates define certificate types; this is the foundational step.\nOption B: Wildcard certificates are for domains, not code signing.\nOption C: Recalculating root CA keys is unnecessary and risky unless compromised.\nOption D: SAN (Subject Alternative Name) is for multi-domain certificates, irrelevant here.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 56,
    "question_text": "During a gap assessment, an organization notes that BYOD usage is a significant risk. The organization implemented administrative policies prohibiting BYOD usage. However, the organization has not implemented technical controls to prevent the unauthorized use of BYOD assets when accessing the organization's resources. Which of the following solutions should the organization implement to best reduce the risk of BYOD devices? (Select two).",
    "options": {
      "A": "Cloud IAM to enforce the use of token based MFA",
      "B": "Conditional access, to enforce user-to-device binding",
      "C": "NAC, to enforce device configuration requirements",
      "D": "PAM, to enforce local password policies",
      "E": "SD-WAN, to enforce web content filtering through external proxies",
      "F": "DLP, to enforce data protection capabilities"
    },
    "correct_answer": "B C",
    "explanation": "To reduce the risk of unauthorized BYOD (Bring Your Own Device) usage, the organization should implement Conditional Access and Network Access Control (NAC).\nWhy Conditional Access and NAC?\nConditional Access:\nUser-to-Device Binding: Conditional access policies can enforce that only registered and compliant devices are allowed to access corporate resources.\nContext-Aware Security: Enforces access controls based on the context of the access attempt, such as user identity, device compliance, location, and more.\nNetwork Access Control (NAC):\nDevice Configuration Requirements: NAC ensures that only devices meeting specific security configurations are allowed to connect to the network.\nAccess Control: Provides granular control over network access, ensuring that BYOD devices comply with security policies before gaining access.\nOther options, while useful, do not address the specific need to control and secure BYOD devices effectively:\nA. Cloud IAM to enforce token-based MFA: Enhances authentication security but does not control device compliance.\nD. PAM to enforce local password policies: Focuses on privileged account management, not BYOD control.\nE. SD-WAN to enforce web content filtering: Enhances network performance and security but does not enforce BYOD device compliance.\nF. DLP to enforce data protection capabilities: Protects data but does not control BYOD device access and compliance.",
    "is_simulation": false,
    "question_type": "multiple_choice_multiple_answer"
  },
  {
    "question_number": 57,
    "question_text": "Users are writing passwords on paper because of the number of passwords needed in an environment. Which of the following solutions is the best way to manage this situation and decrease risks?",
    "options": {
      "A": "Increasing password complexity to require at least 16 characters",
      "B": "Implementing an SSO solution and integrating with applications",
      "C": "Requiring users to use an open-source password manager",
      "D": "Implementing an MFA solution to avoid reliance only on passwords"
    },
    "correct_answer": "B",
    "explanation": "Implementing a Single Sign-On (SSO) solution and integrating it with applications is the best way to manage the situation and decrease risks. Here’s why:\nReduced Password Fatigue: SSO allows users to log in once and gain access to multiple applications and systems without needing to remember and manage multiple passwords. This reduces the likelihood of users writing down passwords.\nImproved Security: By reducing the number of passwords users need to manage, SSO decreases the attack surface and potential for password-related security breaches. It also allows for the implementation of stronger authentication methods.\nUser Convenience: SSO improves the user experience by simplifying the login process, which can lead to higher productivity and satisfaction.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 58,
    "question_text": "An organization is implementing advanced security controls associated with the execution of software applications on corporate endpoints. The organization must implement a deny-all, permit-by-exception approach to software authorization for all systems regardless of OS. Which of the following should be implemented to meet these requirements?",
    "options": {
      "A": "SELinux",
      "B": "MDM",
      "C": "XDR",
      "D": "Block list",
      "E": "Atomic execution"
    },
    "correct_answer": "D",
    "explanation": "Comprehensive and Detailed Step-by-Step Explanation:\nUnderstanding the Scenario: The organization wants a strict application control policy: deny all software execution by default and only allow specifically authorized applications. This must be enforced across all operating systems. It is implied that they mean an Allow list, but Block List is the only reasonable answer.\nAnalyzing the Answer Choices:\nA. SELinux (Security-Enhanced Linux): SELinux is a security module for the Linux kernel that provides Mandatory Access Control (MAC). While it can enforce application control, it's specific to Linux and doesn't meet the \"regardless of OS\" requirement.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 59,
    "question_text": "An organization is prioritizing efforts to remediate or mitigate risks identified during the latest assessment. For one of the risks, a full remediation was not possible, but the organization was able to successfully apply mitigations to reduce the likelihood of the impact. Which of the following should the organization perform next?",
    "options": {
      "A": "Assess the residual risk.",
      "B": "Update the organization's threat model.",
      "C": "Move to the next risk in the register.",
      "D": "Recalculate the magnitude of the impact."
    },
    "correct_answer": "A",
    "explanation": "After applying mitigations that reduce the likelihood of a risk’s impact, the next step is to assess the residual risk—the risk that remains after controls are implemented. This ensures the organization understands if the mitigation is sufficient or if further action is needed, aligning with risk management best practices.\nOption A: Correct—residual risk assessment is the logical next step to evaluate the effectiveness of mitigations.\nOption B: Updating the threat model might follow but isn’t immediate; residual risk comes first.\nOption C: Moving to the next risk skips evaluating the current mitigation’s success.\nOption D: Recalculating impact magnitude is part of residual risk assessment but isn’t the full process.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 60,
    "question_text": "A security analyst discovered requests associated with IP addresses known for legitimate bot-related traffic. Which of the following should the analyst use to determine whether the requests are malicious?",
    "options": {
      "A": "User-agent string",
      "B": "Byte length of the request",
      "C": "Web application headers",
      "D": "HTML encoding field"
    },
    "correct_answer": "A",
    "explanation": "The user-agent string can provide valuable information to distinguish between legitimate and bot-related traffic. It contains details about the browser, device, and sometimes the operating system of the client making the request.\nWhy Use User-Agent String?\nIdentify Patterns: User-agent strings can help identify patterns that are typical of bots or legitimate users.\nBlock Malicious Bots: Many bots use known user-agent strings, and identifying these can help block malicious requests.\nAnomalies Detection: Anomalous user-agent strings can indicate spoofing attempts or malicious activity.\nOther options provide useful information but may not be as effective for initial determination of the nature of the request:\nB. Byte length of the request: This can indicate anomalies but does not provide detailed information about the client.\nC. Web application headers: While useful, they may not provide enough distinction between legitimate and bot traffic.\nD. HTML encoding field: This is not typically used for identifying the nature of the request.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 61,
    "question_text": "A security analyst reviews the following report:\n[Image of table showing Product, Location, Chassis manufacturer, OS, Application developer, Vendor.]\nWhich of the following assessments is the analyst performing?",
    "options": {
      "A": "System",
      "B": "Supply chain",
      "C": "Quantitative",
      "D": "Organizational"
    },
    "correct_answer": "B",
    "explanation": "The table shows detailed information about products, including location, chassis manufacturer, OS, application developer, and vendor. This type of information is typically assessed in a supply chain assessment to evaluate the security and reliability of components and services from different suppliers.\nWhy Supply Chain Assessment?\nComponent Evaluation: Assessing the origin and security of each component used in the products, including hardware, software, and third-party services.\nVendor Reliability: Evaluating the security practices and reliability of vendors involved in providing components or services.\nRisk Management: Identifying potential risks associated with the supply chain, such as vulnerabilities in third-party components or insecure development practices.\nOther types of assessments do not align with the detailed supplier and component information provided:\nA. System: Focuses on individual system security, not the broader supply chain.\nC. Quantitative: Focuses on numerical risk assessments, not supplier information.\nD. Organizational: Focuses on internal organizational practices, not external suppliers.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 62,
    "question_text": "A systems engineer is configuring a system baseline for servers that will provide email services. As part of the architecture design, the engineer needs to improve performance of the systems by using an access vector cache, facilitating mandatory access control and protecting against:\n• Unauthorized reading and modification of data and programs\n• Bypassing application security mechanisms\n• Privilege escalation\n• Interference with other processes\nWhich of the following is the most appropriate for the engineer to deploy?",
    "options": {
      "A": "SELinux",
      "B": "Privileged access management",
      "C": "Self-encrypting disks",
      "D": "NIPS"
    },
    "correct_answer": "A",
    "explanation": "The most appropriate solution for the systems engineer to deploy is SELinux (Security-Enhanced Linux). Here's why:\nMandatory Access Control (MAC): SELinux enforces MAC policies, ensuring that only authorized users and processes can access specific resources. This helps in preventing unauthorized reading and modification of data and programs.\nAccess Vector Cache: SELinux utilizes an access vector cache (AVC) to improve performance. The AVC caches access decisions, reducing the need for repetitive policy lookups and thus improving system efficiency.\nSecurity Mechanisms: SELinux provides a robust framework to enforce security policies and prevent bypassing of application security mechanisms. It controls access based on defined policies, ensuring that security measures are consistently applied.\nPrivilege Escalation and Process Interference: SELinux limits the ability of processes to escalate privileges and interfere with each other by enforcing strict access controls. This containment helps in isolating processes and minimizing the risk of privilege escalation attacks.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 63,
    "question_text": "A compliance officer is facilitating a business impact analysis (BIA) and wants business unit leaders to collect meaningful data. Several business unit leaders want more information about the types of data the officer needs.\nWhich of the following data types would be the most beneficial for the compliance officer? (Select two)",
    "options": {
      "A": "Inventory details",
      "B": "Applicable contract obligations",
      "C": "Costs associated with downtime",
      "D": "Network diagrams",
      "E": "Contingency plans",
      "F": "Critical processes"
    },
    "correct_answer": "B C",
    "explanation": "Comprehensive and Detailed Explanation:\nUnderstanding Business Impact Analysis (BIA):\nA BIA assesses the effects of disruptions to an organization's operations.\nIt helps prioritize resources based on the potential impact of downtime, compliance issues, and critical processes.\nWhy Options B, C, and F are Correct:\nB (Applicable contract obligations): Many companies have legal and compliance obligations regarding downtime, availability, and SLAs. This information helps determine what risk levels are acceptable.\nC (Costs associated with downtime): BIA quantifies the financial impact of system failures. Knowing lost revenue, regulatory fines, and recovery costs helps in planning.\nF (Critical processes): Identifying core business processes allows an organization to prioritize recovery efforts and maintain operational continuity. (Note: The OCR transcribed F as part of the explanation for Why B, C, and F are correct, but the listed correct answers are B and C. Assuming the OCR missed F as a correct answer based on the explanation provided).\nWhy Other Options Are Incorrect:\nA (Inventory details): While useful for asset management, it does not directly impact business continuity planning.\nD (Network diagrams): These help in security architecture but are not directly related to the financial/business impact analysis.\nE (Contingency plans): BIA is performed before contingency planning to identify what needs protection.",
    "is_simulation": false,
    "question_type": "multiple_choice_multiple_answer"
  },
  {
    "question_number": 64,
    "question_text": "An analyst reviews a SIEM and generates the following report:\n[Image of SIEM report showing Host, Rule, Offense Trigger.]\nOnly HOST002 is authorized for internet traffic. Which of the following statements is accurate?",
    "options": {
      "A": "The VM002 host is misconfigured and needs to be revised by the network team.",
      "B": "The HOST002 host is under attack, and a security incident should be declared.",
      "C": "The SIEM platform is reporting multiple false positives on the alerts.",
      "D": "The network connection activity is unusual, and a network infection is highly possible."
    },
    "correct_answer": "D",
    "explanation": "Comprehensive and Detailed Explanation:\nUnderstanding the Security Event:\nHOST002 is the only device authorized for internet traffic. However, the SIEM logs show that VM002 is making network connections to web.corp.local.\nThis indicates unauthorized access, which could be a sign of lateral movement or network infection.\nThis is a red flag for potential malware, unauthorized software, or a compromised host.\nWhy Option D is Correct:\nUnusual network traffic patterns are often an indicator of a compromised system.\nVM002 should not be communicating externally, but it is.\nThis suggests a possible breach or malware infection attempting to communicate with a command-and-control (C2) server.\nWhy Other Options Are Incorrect:\nA (Misconfiguration): While a misconfiguration could explain the unauthorized connections, the pattern of activity suggests something more malicious.\nB (Security incident on HOST002): The issue is not with HOST002. The suspicious activity is from VM002.\nC (False positives): The repeated pattern of unauthorized connections makes false positives unlikely.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 65,
    "question_text": "Which of the following best describes the reason a network architect would enable forward secrecy on all VPN tunnels?",
    "options": {
      "A": "This process is a requirement to enable hardware-accelerated cryptography.",
      "B": "This process reduces the success of attackers performing cryptanalysis.",
      "C": "The business requirements state that confidentiality is a critical success factor.",
      "D": "Modern cryptographic protocols list this process as a prerequisite for use."
    },
    "correct_answer": "B",
    "explanation": "Forward secrecy, also known as perfect forward secrecy, is a feature of certain key agreement protocols that ensures session keys will not be compromised even if the server's private key is compromised in the future. By enabling forward secrecy on VPN tunnels, each session uses a unique key, and these keys are not derived from a common master key. This means that even if an attacker obtains the server's private key, they cannot decrypt past sessions, thereby significantly reducing the effectiveness of cryptanalysis attacks.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 66,
    "question_text": "During a periodic internal audit, a company identifies a few new, critical security controls that are missing. The company has a mature risk management program in place, and the following requirements must be met:\nThe stakeholders should be able to see all the risks.\nThe risks need to have someone accountable for them.\nWhich of the following actions should the GRC analyst take next?",
    "options": {
      "A": "Add the risk to the risk register and assign the owner and severity.",
      "B": "Change the risk appetite and assign an owner to it.",
      "C": "Mitigate the risk and change the status to accepted.",
      "D": "Review the risk to decide whether to accept or reject it."
    },
    "correct_answer": "A",
    "explanation": "A risk register is a tool commonly used in risk management to document all identified risks, their assessment in terms of likelihood and impact, and the actions steps to manage them. By adding the newly identified risks to the risk register and assigning an owner and severity, the organization ensures that each risk is visible to stakeholders and has a designated individual responsible for its management. This aligns with the company's requirements for transparency and accountability in risk management.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 67,
    "question_text": "An analyst has prepared several possible solutions to a successful attack on the company. The solutions need to be implemented with the least amount of downtime. Which of the following should the analyst perform?",
    "options": {
      "A": "Implement all the solutions at once in a virtual lab and then run the attack simulation. Collect the metrics and then choose the best solution based on the metrics.",
      "B": "Implement every solution one at a time in a virtual lab, running a metric collection each time. After the collection, run the attack simulation, roll back each solution, and then implement the next. Choose the best solution based on the best metrics.",
      "C": "Implement every solution one at a time in a virtual lab, running an attack simulation each time while collecting metrics. Roll back each solution and then implement the next. Choose the best solution based on the best metrics.",
      "D": "Implement all the solutions at once in a virtual lab and then collect the metrics. After collection, run the attack simulation. Choose the best solution based on the best metrics."
    },
    "correct_answer": "C",
    "explanation": "To minimize downtime, testing should occur in a virtual lab, not production. The best approach is to test solutions methodically: implement one solution at a time, run an attack simulation, collect metrics, roll back, and repeat. This isolates each solution’s effectiveness, ensuring accurate metrics for decision-making without production impact.\nOption A: Testing all solutions simultaneously muddies the results—metrics won’t show which solution worked.\nOption B: Collecting metrics before the simulation misses the point of testing against the attack.\nOption C: Correct—tests each solution independently with simulation and metrics, minimizing downtime via virtual lab use.\nOption D: Like A, combining solutions obscures individual effectiveness.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 68,
    "question_text": "A security analyst needs to ensure email domains that send phishing attempts without previous communications are not delivered to mailboxes. The following email headers are being reviewed:\n[Image of email headers showing Date, From, To, Reply-to domain, Subject.]\nWhich of the following is the best action for the security analyst to take?",
    "options": {
      "A": "Block messages from hr-saas.com because it is not a recognized domain.",
      "B": "Reroute all messages with unusual security warning notices to the IT administrator",
      "C": "Quarantine all messages with sales-mail.com in the email header",
      "D": "Block vendor.com for repeated attempts to send suspicious messages"
    },
    "correct_answer": "D",
    "explanation": "In reviewing email headers and determining actions to mitigate phishing attempts, the security analyst should focus on patterns of suspicious behavior and the reputation of the sending domains. Here’s the analysis of the options provided:\nA. Block messages from hr-saas.com because it is not a recognized domain: Blocking a domain solely because it is not recognized can lead to legitimate emails being missed. Recognition alone should not be the criterion for blocking.\nB. Reroute all messages with unusual security warning notices to the IT administrator: While rerouting suspicious messages can be a good practice, it is not specific to the domain sending repeated suspicious messages.\nC. Quarantine all messages with sales-mail.com in the email header: Quarantining messages based on the presence of a specific domain in the email header can be too broad and may capture legitimate emails.\nD. Block vendor.com for repeated attempts to send suspicious messages: This option is the most appropriate because it targets a domain that has shown a pattern of sending suspicious messages. Blocking a domain that repeatedly sends phishing attempts without previous communications helps in preventing future attempts from the same source and aligns with the goal of mitigating phishing risks.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 69,
    "question_text": "During a recent security event, access from the non-production environment to the production environment enabled unauthorized users to:\nInstall unapproved software\nMake unplanned configuration changes\nDuring the investigation, the following findings were identified:\nSeveral new users were added in bulk by the IAM team\nAdditional firewalls and routers were recently added\nVulnerability assessments have been disabled for more than 30 days\nThe application allow list has not been modified in two weeks\nLogs were unavailable for various types of traffic\nEndpoints have not been patched in over ten days\nWhich of the following actions would most likely need to be taken to ensure proper monitoring? (Select two)",
    "options": {
      "A": "Disable bulk user creations by the IAM team",
      "B": "Extend log retention for all security and network devices to 180 days for all traffic",
      "C": "Review the application allow list daily",
      "D": "Routinely update all endpoints and network devices as soon as new patches/hot fixes are available",
      "E": "Ensure all network and security devices are sending relevant data to the SIEM",
      "F": "Configure firewall rules to only allow production-to-non-production traffic"
    },
    "correct_answer": "A E",
    "explanation": "Comprehensive and Detailed Explanation:\nUnderstanding the Security Event:\nUnauthorized users gained access from non-production to production.\nIAM policies were weak, allowing bulk user creation.\nVulnerability assessments were disabled, and patching was delayed.\nLogs were unavailable, making incident response difficult.\nWhy Options A, D, and E are Correct:\nA (Disable bulk user creation by IAM team): Prevents unauthorized mass user account creation, which could be exploited by attackers. (Note: Option A is listed as a correct answer in the original but isn't primarily focused on *monitoring* as the question asks. However, it's a valid remediation for one of the findings. Re-checking the original seems to indicate A and E are the selected correct answers).\nD (Routine updates for endpoints & network devices): Patch management ensures vulnerabilities are not left open for attackers. (Note: Option D is listed as a correct answer in the original but is focused on *remediation/patching* not *monitoring*. Re-checking the original seems to indicate A and E are the selected correct answers).\nE (Ensure all security/network devices send logs to SIEM): Helps with real-time monitoring and detection of unauthorized activities. (Note: Option E is listed as a correct answer in the original and is directly related to *monitoring*).\nWhy Other Options Are Incorrect:\nB (180-day log retention): While log retention is good, real-time monitoring is the priority.\nC (Review application allow list daily): Reviewing it daily is impractical. Regular audits are better.\nF (Restrict production-to-non-production traffic): The issue is unauthorized access, not traffic routing.\n\nGiven the goal is to ensure proper *monitoring*, Option E is clearly relevant. Option A addresses a weak process identified (bulk user creation) which, while not directly monitoring, addresses a root cause that allowed unauthorized users.",
    "is_simulation": false,
    "question_type": "multiple_choice_multiple_answer"
  },
  {
    "question_number": 70,
    "question_text": "A company recently experienced a ransomware attack. Although the company performs systems and data backup on a schedule that aligns with its RPO (Recovery Point Objective) requirements, the backup administrator could not recover critical systems and data from its offline backups to meet the RPO. Eventually, the systems and data were restored with information that was six months outside of RPO requirements.\nWhich of the following actions should the company take to reduce the risk of a similar attack?",
    "options": {
      "A": "Encrypt and label the backup tapes with the appropriate retention schedule before they are sent to the off-site location.",
      "B": "Implement a business continuity process that includes reverting manual business processes.",
      "C": "Perform regular disaster recovery testing of IT and non-IT systems and processes.",
      "D": "Carry out a tabletop exercise to update and verify the RACI matrix with IT and critical business functions."
    },
    "correct_answer": "C",
    "explanation": "Comprehensive and Detailed Explanation:\nUnderstanding the Ransomware Issue:\nThe key issue here is that backups were not recoverable within the required RPO timeframe.\nThis means the organization did not properly test its backup and disaster recovery (DR) processes.\nTo prevent this from happening again, regular disaster recovery testing is essential.\nWhy Option C is Correct:\nDisaster recovery testing ensures that backups are functional and can meet business continuity needs.\nFrequent DR testing allows organizations to identify and fix gaps in recovery strategies.\nRegular testing ensures that recovery meets the RPO & RTO (Recovery Time Objective) requirements.\nWhy Other Options Are Incorrect:\nA (Encrypt & label backup tapes): While encryption is important, it does not address the failure to meet RPO requirements.\nB (Reverting to manual business processes): While a manual continuity plan is good for resilience, it does not resolve the backup and recovery failure.\nD (Tabletop exercise & RACI matrix): A tabletop exercise is a planning activity, but it does not involve actual recovery testing.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 71,
    "question_text": "A security professional is investigating a trend in vulnerability findings for newly deployed cloud systems. Given the following output:\n[Image of table showing Date, IP Address, System name, Finding, Criticality rating.]\nWhich of the following actions would address the root cause of this issue?",
    "options": {
      "A": "Automating the patching system to update base Images",
      "B": "Recompiling the affected programs with the most current patches",
      "C": "Disabling unused/unneeded ports on all servers",
      "D": "Deploying a WAF with virtual patching upstream of the affected systems"
    },
    "correct_answer": "A",
    "explanation": "The output shows that multiple systems have outdated or vulnerable software versions (OpenSSL 1.01 and Java 11 runtime). This suggests that the systems are not being patched regularly or effectively.\nA. Automating the patching system to update base images: Automating the patching process ensures that the latest security updates and patches are applied to all systems, including newly deployed ones. This addresses the root cause by ensuring that base images used for deployment are always up-to-date with the latest security patches.\nB. Recompiling the affected programs with the most current patches: While this can fix the immediate vulnerabilities, it does not address the root cause of the problem, which is the lack of regular updates.\nC. Disabling unused/unneeded ports on all servers: This improves security but does not address the specific issue of outdated software.\nD. Deploying a WAF with virtual patching upstream of the affected systems: This can provide a temporary shield but does not resolve the underlying issue of outdated software.\nAutomating the patching system to update base images ensures that all deployed systems are using the latest, most secure versions of software, addressing the root cause of the vulnerability trend.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 72,
    "question_text": "A central bank implements strict risk mitigations for the hardware supply chain, including an allow list for specific countries of origin. Which of the following best describes the cyber threat to the bank?",
    "options": {
      "A": "Ability to obtain components during wartime",
      "B": "Fragility and other availability attacks",
      "C": "Physical Implants and tampering",
      "D": "Non-conformance to accepted manufacturing standards"
    },
    "correct_answer": "C",
    "explanation": "The best description of the cyber threat to a central bank implementing strict risk mitigations for the hardware supply chain, including an allow list for specific countries of origin, is the risk of physical implants and tampering. Here’s why:\nSupply Chain Security: The supply chain is a critical vector for hardware tampering and physical implants, which can compromise the integrity and security of hardware components before they reach the organization.\nTargeted Attacks: Banks and financial institutions are high-value targets, making them susceptible to sophisticated attacks, including those involving physical implants that can be introduced during manufacturing or shipping processes.\nStrict Mitigations: Implementing an allow list for specific countries aims to mitigate the risk of supply chain attacks by limiting the sources of hardware. However, the primary concern remains the introduction of malicious components through tampering.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 73,
    "question_text": "Developers have been creating and managing cryptographic material on their personal laptops for use in the production environment. A security engineer needs to initiate a more secure process. Which of the following is the best strategy for the engineer to use?",
    "options": {
      "A": "Disabling the BIOS and moving to UEFI",
      "B": "Managing secrets on the vTPM hardware",
      "C": "Employing shielding to prevent LMI",
      "D": "Managing key material on a HSM"
    },
    "correct_answer": "D",
    "explanation": "The best strategy for securely managing cryptographic material is to use a Hardware Security Module (HSM). Here’s why:\nSecurity and Integrity: HSMs are specialized hardware devices designed to protect and manage digital keys. They provide high levels of physical and logical security, ensuring that cryptographic material is well protected against tampering and unauthorized access.\nCentralized Key Management: Using HSMs allows for centralized management of cryptographic keys, reducing the risks associated with decentralized and potentially insecure key storage practices, such as on personal laptops.\nCompliance and Best Practices: HSMs comply with various industry standards and regulations (such as FIPS 140-2) for secure key management. This ensures that the organization adheres to best practices and meets compliance requirements.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 74,
    "question_text": "An organization hires a security consultant to establish a SOC that includes a threat-modeling function. During initial activities, the consultant works with system engineers to identify antipatterns within the environment. Which of the following is most critical for the engineers to disclose to the consultant during this phase?",
    "options": {
      "A": "Results from the most recent infrastructure access review",
      "B": "A listing of unpatchable IoT devices in use in the data center",
      "C": "Network and data flow diagrams covering the production environment",
      "D": "Results from the most recent software composition analysis",
      "E": "A current inventory of cloud resources and SaaS products in use"
    },
    "correct_answer": "C",
    "explanation": "In the context of establishing a Security Operations Center (SOC) with a threat-modeling function, it's crucial to understand how data flows within the organization's systems. Network and data flow diagrams provide a visual representation of the system's architecture, illustrating how data moves between components, which is essential for identifying potential security weaknesses and antipatterns. Antipatterns are common responses to recurring problems that are ineffective and risk-inducing. By analyzing these diagrams, the consultant can pinpoint areas where security controls may be lacking or misconfigured, thereby facilitating the development of effective threat models.\nWhile other options like unpatchable IoT devices (Option B) and inventories of cloud resources (Option E) are important for comprehensive security assessments, they are more pertinent during later stages, such as vulnerability management and asset inventory. The initial phase of threat modeling focuses on understanding the system's structure and data flows to identify potential threats, making network and data flow diagrams the most critical information at this stage.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 75,
    "question_text": "A security analyst is reviewing the following log:\n[Image of log entries showing Time, File type, Size, Antivirus status, Location.]\nWhich of the following possible events should the security analyst investigate further?",
    "options": {
      "A": "A macro that was prevented from running",
      "B": "A text file containing passwords that were leaked",
      "C": "A malicious file that was run in this environment",
      "D": "A PDF that exposed sensitive information improperly"
    },
    "correct_answer": "B",
    "explanation": "Based on the log provided, the most concerning event that should be investigated further is the presence of a text file containing passwords that were leaked. Here's why:\nSensitive Information Exposure: A text file containing passwords represents a significant security risk, as it indicates that sensitive credentials have been exposed in plain text, potentially leading to unauthorized access.\nImmediate Threat: Password leaks can lead to immediate exploitation by attackers, compromising user accounts and sensitive data. This requires urgent investigation.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 76,
    "question_text": "Company A and Company B are merging. Company A's compliance reports indicate branch protections are not in place. A security analyst needs to ensure that potential threats to the software development life cycle are addressed. Which of the following should the analyst consider when completing this task?",
    "options": {
      "A": "If developers are unable to promote to production",
      "B": "If DAST code is being stored to a single code repository",
      "C": "If DAST scans are routinely scheduled",
      "D": "If role-based training is deployed"
    },
    "correct_answer": "C",
    "explanation": "Dynamic Application Security Testing (DAST) is crucial for identifying and addressing security vulnerabilities during the software development life cycle (SDLC). Ensuring that DAST scans are routinely scheduled helps in maintaining a secure development process.\nWhy Routine DAST Scans?\nContinuous Security Assessment: Regular DAST scans help in identifying vulnerabilities in real-time, ensuring they are addressed promptly.\nCompliance: Routine scans ensure that the development process complies with security standards and regulations.\nProactive Threat Mitigation: Regular scans help in early detection and mitigation of potential security threats, reducing the risk of breaches.\nIntegration into SDLC: Ensures security is embedded within the development process, promoting a security-first approach.\nOther options, while relevant, do not directly address the continuous assessment and proactive identification of threats:\nA. If developers are unable to promote to production: This is more of an operational issue than a security assessment.\nB. If DAST code is being stored to a single code repository: This concerns code management rather than security testing frequency.\nD. If role-based training is deployed: While important, training alone does not ensure continuous security assessment.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 77,
    "question_text": "An organization wants to implement a platform to better identify which specific assets are affected by a given vulnerability. Which of the following components provides the best foundation to achieve this goal?",
    "options": {
      "A": "SASE",
      "B": "CMDB",
      "C": "SBoM",
      "D": "SLM"
    },
    "correct_answer": "B",
    "explanation": "A Configuration Management Database (CMDB) provides the best foundation for identifying which specific assets are affected by a given vulnerability. A CMDB maintains detailed information about the IT environment, including hardware, software, configurations, and relationships between assets. This comprehensive view allows organizations to quickly identify and address vulnerabilities affecting specific assets.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 78,
    "question_text": "A company receives several complaints from customers regarding its website. An engineer implements a parser for the web server logs that generates the following output:\n[Image of log entries showing Browser, User, Location, Load time, HTTP response.]\nWhich of the following should the company implement to best resolve the issue?",
    "options": {
      "A": "IDS",
      "B": "CDN",
      "C": "WAF",
      "D": "NAC"
    },
    "correct_answer": "B",
    "explanation": "The table indicates varying load times for users accessing the website from different geographic locations. Customers from Australia and India are experiencing significantly higher load times compared to those from the United States. This suggests that latency and geographical distance are affecting the website's performance.\nA. IDS (Intrusion Detection System): While an IDS is useful for detecting malicious activities, it does not address performance issues related to latency and geographical distribution of content.\nB. CDN (Content Delivery Network): A CDN stores copies of the website's content in multiple geographic locations. By serving content from the nearest server to the user, a CDN can significantly reduce load times and improve user experience globally.\nC. WAF (Web Application Firewall): A WAF protects web applications by filtering and monitoring HTTP traffic but does not improve performance related to geographical latency.\nD. NAC (Network Access Control): NAC solutions control access to network resources but are not designed to address web performance issues.\nImplementing a CDN is the best solution to resolve the performance issues observed in the log output.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 79,
    "question_text": "A company's security policy states that any publicly available server must be patched within 12 hours after a patch is released. A recent IIS zero-day vulnerability was discovered that affects all versions of the Windows Server OS:\n[Image of table showing Host, OS, Externally available?, Behind WAF?, IIS installed?]\nWhich of the following hosts should a security analyst patch first once a patch is available?",
    "options": {
      "A": "1",
      "B": "2",
      "C": "3",
      "D": "4",
      "E": "5",
      "F": "6"
    },
    "correct_answer": "A",
    "explanation": "Based on the security policy that any publicly available server must be patched within 12 hours after a patch is released, the security analyst should patch Host 1 first. Here’s why:\nPublic Availability: Host 1 is externally available, making it accessible from the internet. Publicly available servers are at higher risk of being targeted by attackers, especially when a zero-day vulnerability is known.\nExposure to Threats: Host 1 has IIS installed and is publicly accessible, increasing its exposure to potential exploitation. Patching this host first reduces the risk of a successful attack.\nPrioritization of Critical Assets: According to best practices, assets that are exposed to higher risks should be prioritized for patching to mitigate potential threats promptly.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 80,
    "question_text": "A financial services organization is using AI to fully automate the process of deciding client loan rates. Which of the following should the organization be most concerned about from a privacy perspective?",
    "options": {
      "A": "Model explainability",
      "B": "Credential Theft",
      "C": "Possible prompt Injections",
      "D": "Exposure to social engineering"
    },
    "correct_answer": "A",
    "explanation": "When using AI to fully automate the process of deciding client loan rates, the primary concern from a privacy perspective is model explainability.\nWhy Model Explainability is Critical:\nTransparency: It ensures that the decision-making process of the AI model can be understood and explained to stakeholders, including clients.\nAccountability: Helps in identifying biases and errors in the model, ensuring that the AI is making fair and unbiased decisions.\nRegulatory Compliance: Various regulations require that decisions, especially those affecting individuals' financial status, can be explained and justified.\nTrust: Builds trust among users and stakeholders by demonstrating that the AI decisions are transparent and justifiable.\nOther options, such as credential theft, prompt injections, and social engineering, are significant concerns but do not directly address the privacy and fairness implications of automated decision-making.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 81,
    "question_text": "A company hired an email service provider called my-email.com to deliver company emails. The company started having several issues during the migration. A security engineer is troubleshooting and observes the following configuration snippet:\n[Image of DNS zone file snippet showing MX and CNAME records.]\nWhich of the following should the security engineer modify to fix the issue? (Select two).",
    "options": {
      "A": "The email CNAME record must be changed to a type A record pointing to 192.168.1.11",
      "B": "The TXT record must be Changed to \"v=dmarc ip4:192.168.1.10 include:my-email.com -all\"",
      "C": "The srv01 A record must be changed to a type CNAME record pointing to the email server",
      "D": "The email CNAME record must be changed to a type A record pointing to 192.168.1.10",
      "E": "The TXT record must be changed to \"v=dkim ip4:l92.168.1.11 include my-email.com -ell\"",
      "F": "The TXT record must be Changed to \"v=dkim ip4:192.168.1.10 include:email-all\"",
      "G": "The srv01 A record must be changed to a type CNAME record pointing to the web01 server"
    },
    "correct_answer": "B D",
    "explanation": "The security engineer should modify the following to fix the email migration issues:\nEmail CNAME Record: The email CNAME record must be changed to a type A record pointing to 192.168.1.10. This is because CNAME records should not be used where an IP address (A record) is required. Changing it to an A record ensures direct pointing to the correct IP.\nTXT Record for DMARC: The TXT record must be changed to \"v=dmarc ip4:192.168.1.10 include com -all\". This ensures proper configuration of DMARC (Domain-based Message Authentication, Reporting & Conformance) to include the correct IP address and the email service provider domain.\nDMARC: Ensuring the DMARC record is correctly set up helps in preventing email spoofing and phishing, aligning with email security best practices.",
    "is_simulation": false,
    "question_type": "multiple_choice_multiple_answer"
  },
  {
    "question_number": 82,
    "question_text": "An organization that performs real-time financial processing is implementing a new backup solution. Given the following business requirements:\nThe backup solution must reduce the risk of potential backup compromise.\nThe backup solution must be resilient to a ransomware attack.\nThe time to restore from backups is less important than backup data integrity.\nMultiple copies of production data must be maintained.\nWhich of the following backup strategies best meets these requirements?",
    "options": {
      "A": "Creating a secondary, immutable database and adding live data on a continuous basis",
      "B": "Utilizing two connected storage arrays and ensuring the arrays constantly sync",
      "C": "Enabling remote journaling on the databases to ensure real-time transactions are mirrored",
      "D": "Setting up anti-tampering on the databases to ensure data cannot be changed unintentionally"
    },
    "correct_answer": "A",
    "explanation": "An immutable database prevents modifications or deletions, ensuring resilience against ransomware while maintaining multiple copies of data.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 83,
    "question_text": "A security analyst wants to use lessons learned from a poor incident response to reduce dwell time in the future. The analyst is using the following data points:\n[Image of table showing User, Site visited, Method, Filter status, Traffic status, Alert Status.]\nWhich of the following would the analyst most likely recommend?",
    "options": {
      "A": "Adjusting the SIEM to alert on attempts to visit phishing sites",
      "B": "Allowing TRACE method traffic to enable better log correlation",
      "C": "Enabling alerting on all suspicious administrator behavior",
      "D": "Utilizing allow lists on the WAF for all users using GET methods"
    },
    "correct_answer": "C",
    "explanation": "In the context of improving incident response and reducing dwell time, the security analyst needs to focus on proactive measures that can quickly detect and alert on potential security breaches. Here’s a detailed analysis of the options provided:\nA. Adjusting the SIEM to alert on attempts to visit phishing sites: While this is a useful measure to prevent phishing attacks, it primarily addresses external threats and doesn’t directly impact dwell time reduction, which focuses on the time a threat remains undetected within a network.\nB. Allowing TRACE method traffic to enable better log correlation: The TRACE method in HTTP is used for debugging purposes, but enabling it can introduce security vulnerabilities. It’s not typically recommended for enhancing security monitoring or incident response.\nC. Enabling alerting on all suspicious administrator behavior: This option directly targets the potential misuse of administrator accounts, which are often high-value targets for attackers. By monitoring and alerting on suspicious activities from admin accounts, the organization can quickly identify and respond to potential breaches, thereby reducing dwell time significantly. Suspicious behavior could include unusual login times, access to sensitive data not usually accessed by the admin, or any deviation from normal behavior patterns. This proactive monitoring is crucial for quick detection and response, aligning well with best practices in incident response.\nD. Utilizing allow lists on the WAF for all users using GET methods: This measure is aimed at restricting access based on allowed lists, which can be effective in preventing unauthorized access but doesn’t specifically address the need for quick detection and response to internal threats.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 84,
    "question_text": "Embedded malware has been discovered in a popular PDF reader application and is currently being exploited in the wild. Because the supply chain was compromised, this malware is present in versions 10.0 through 10.3 of the software's official versions. The malware is not present in version 10.4.\nSince the details around this malware are still emerging, the Chief Information Security Officer has asked the senior security analyst to collaborate with the IT asset inventory manager to find instances of the installed software in order to begin response activities. The asset inventory manager has asked an analyst to provide a regular expression that will identify the affected versions. The software installation entries are formatted as follows:\nReader 10.0\nReader 10.1\nReader 10.2\nReader 10.3\nReader 10.4\nWhich of the following regular expression entries will accurately identify all the affected versions?",
    "options": {
      "A": "Reader(*)[1][0].[0-4:",
      "B": "Reader[11[01X.f0-3'",
      "C": "Reader( )[1][0].[0-3:",
      "D": "Reader( )[1][0]X.[1-3:"
    },
    "correct_answer": "C",
    "explanation": "Comprehensive and Detailed Step-by-Step Explanation:\nUnderstand the Question Requirements: The goal is to use a regular expression (regex) to match software versions 10.0 through 10.3, but exclude version 10.4.\nReview Regex Syntax:\n[ ] indicates a character set (matches any one character in the set).\n[0-3] matches any digit between 0 and 3.\n\\. escapes the period (.) so it matches a literal period instead of acting as a wildcard.\n( ) groups parts of the regex together.\nAnalyze Each Option:\nOption A: Reader(*)[1][0].[0-4:\nIncorrect. The use of (*) is not valid syntax in this context and [0-4 is incomplete or misformatted.\nOption B: Reader[11[01X.f0-3'\nIncorrect. This is an invalid regex syntax, mixing character sets and mismatched brackets.\nOption C: Reader( )[1][0].[0-3:\nCorrect. This regex is valid and matches \"Reader 10.0\", \"Reader 10.1\", \"Reader 10.2\", and \"Reader 10.3\" while excluding \"Reader 10.4\".\nBreakdown:\nReader: Matches the text \"Reader\".\n[1][0]: Matches \"10\" as a combination of two characters.\n\\.: Matches the literal period.\n[0-3]: Matches any single digit between 0 and 3.\nOption D: Reader( )[1][0] X.[1-3:\nIncorrect. The syntax X.[1-3 is invalid, and this does not match the required versions.\nConclusion: The regex in Option C correctly identifies all affected versions (10.0, 10.1, 10.2, 10.3) while excluding the unaffected version (10.4).",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 85,
    "question_text": "An organization recently implemented a new email DLP solution. Emails sent from company email addresses to matching personal email addresses generated a large number of alerts, but the content of the emails did not include company data. The security team needs to reduce the number of emails sent without blocking all emails to common personal email services. Which of the following should the security team implement first?",
    "options": {
      "A": "Automatically quarantine outgoing email.",
      "B": "Create an acceptable use policy.",
      "C": "Enforce email encryption standards.",
      "D": "Perform security awareness training focusing on phishing."
    },
    "correct_answer": "B",
    "explanation": "An acceptable use policy (AUP) defines what is considered appropriate use of corporate email and prevents unnecessary emails to personal accounts. This helps in reducing false DLP alerts while maintaining compliance.\nQuarantining emails (A) is unnecessary since the content was not flagged as sensitive.\nEncryption (C) secures emails but does not address overuse.\nPhishing awareness training (D) is unrelated to policy enforcement for outgoing emails.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 86,
    "question_text": "Employees use their badges to track the number of hours they work. The badge readers cannot be upgraded due to facility constraints. The software for the badge readers uses a legacy platform and requires connectivity to the enterprise resource planning solution. Which of the following is the best to ensure the security of the badge readers?",
    "options": {
      "A": "Segmentation",
      "B": "Vulnerability scans",
      "C": "Anti-malware"
    },
    "correct_answer": "A",
    "explanation": "Segmentation is the best option to ensure the security of legacy badge readers that cannot be upgraded. Segmentation isolates the legacy devices on a separate network segment to minimize their exposure to potential threats. This approach reduces the attack surface by preventing unauthorized access from other parts of the network while still allowing necessary connectivity to the enterprise resource planning (ERP) system.\nVulnerability scans (B) are useful for identifying weaknesses but do not actively protect the badge readers.\nAnti-malware (C) is ineffective since the badge readers use a legacy platform that likely does not support modern endpoint protection solutions.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 87,
    "question_text": "A systems administrator wants to reduce the number of failed patch deployments in an organization. The administrator discovers that system owners modify systems or applications in an ad hoc manner. Which of the following is the best way to reduce the number of failed patch deployments?",
    "options": {
      "A": "Compliance tracking",
      "B": "Situational awareness",
      "C": "Change management",
      "D": "Quality assurance"
    },
    "correct_answer": "C",
    "explanation": "To reduce the number of failed patch deployments, the systems administrator should implement a robust change management process. Change management ensures that all modifications to systems or applications are planned, tested, and approved before deployment. This systematic approach reduces the risk of unplanned changes that can cause patch failures and ensures that patches are deployed in a controlled and predictable manner.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 88,
    "question_text": "A company’s SIEM is designed to associate the company’s asset inventory with user events. Given the following report:\n[Image of report showing Hostname, Account, Attempted Logins, Failed Logins, Successful Logins.]\nWhich of the following should a security engineer investigate first as part of a log audit?",
    "options": {
      "A": "An endpoint that is not submitting any logs",
      "B": "Potential activity indicating an attacker moving laterally in the network",
      "C": "A misconfigured syslog server creating false negatives",
      "D": "Unauthorized usage attempts of the administrator account"
    },
    "correct_answer": "D",
    "explanation": "Comprehensive and Detailed Explanation:\nUnderstanding the Security Event:\nAdministrator accounts are highly privileged and require strict monitoring.\nServer 4 shows failed login attempts for the administrator account. This could indicate a brute-force attack or unauthorized access attempt.\nThe fact that none of the admin login attempts were successful suggests someone was trying to guess the credentials.\nWhy Option D is Correct:\nFailed logins for administrator accounts are a critical security concern.\nIf an attacker gains access, they could escalate privileges and compromise the network.\nInvestigating unauthorized admin login attempts should be the top priority in a log audit.\nWhy Other Options Are Incorrect:\nA (Endpoint not submitting logs): While this is concerning, it does not indicate an active attack.\nB (Lateral movement): There's no evidence of a compromised account moving between servers yet.\nC (Misconfigured syslog server): False negatives are a possibility, but the failed admin logins are real.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 89,
    "question_text": "A security officer received several complaints from users about excessive MFA push notifications at night. The security team investigates and suspects malicious activities regarding user account authentication. Which of the following is the best way for the security officer to restrict MFA notifications?",
    "options": {
      "A": "Provisioning FIDO2 devices",
      "B": "Deploying a text message based on MFA",
      "C": "Enabling OTP via email",
      "D": "Configuring prompt-driven MFA"
    },
    "correct_answer": "D",
    "explanation": "Excessive MFA push notifications can be a sign of an attempted push notification attack, where attackers repeatedly send MFA prompts hoping the user will eventually approve one by mistake. To mitigate this:\nA. Provisioning FIDO2 devices: While FIDO2 devices offer strong authentication, they may not be practical for all users and do not directly address the issue of excessive push notifications.\nB. Deploying a text message-based MFA: SMS-based MFA can still be vulnerable to similar spamming attacks and phishing.\nC. Enabling OTP via email: Email-based OTPs add another layer of security but do not directly solve the issue of excessive notifications.\nD. Configuring prompt-driven MFA: This option allows users to respond to prompts in a secure manner, often including features like time-limited approval windows, additional verification steps, or requiring specific actions to approve. This can help prevent users from accidentally approving malicious attempts.\nConfiguring prompt-driven MFA is the best solution to restrict unnecessary MFA notifications and improve security.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 90,
    "question_text": "A systems administrator wants to use existing resources to automate reporting from disparate security appliances that do not currently communicate. Which of the following is the best way to meet this objective?",
    "options": {
      "A": "Configuring an API Integration to aggregate the different data sets",
      "B": "Combining back-end application storage into a single, relational database",
      "C": "Purchasing and deploying commercial off the shelf aggregation software",
      "D": "Migrating application usage logs to on-premises storage"
    },
    "correct_answer": "A",
    "explanation": "The best way to automate reporting from disparate security appliances that do not currently communicate is to configure an API Integration to aggregate the different data sets. Here's why:\nInteroperability: APIs allow different systems to communicate and share data, even if they were not originally designed to work together. This enables the integration of various security appliances into a unified reporting system.\nAutomation: API integrations can automate the process of data collection, aggregation, and reporting, reducing manual effort and increasing efficiency.\nScalability: APIs provide a scalable solution that can easily be extended to include additional security appliances or data sources as needed.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 91,
    "question_text": "A building camera is remotely accessed and disabled from the remote console application during off-hours. A security analyst reviews the following logs:\n[Image of logs showing Date & Time, Public IP, Browser Info, Action.]\nWhich of the following actions should the analyst take to best mitigate the threat?",
    "options": {
      "A": "Implement WAF protection for the web application.",
      "B": "Upgrade the firmware on the camera.",
      "C": "Only allow connections from approved IPs.",
      "D": "Block IP 104.18.16.29 on the firewall."
    },
    "correct_answer": "C",
    "explanation": "The logs indicate unauthorized access from 104.18.16.29, an external IP, to the building camera’s administrative console during off-hours. Restricting access only to approved IPs ensures that only authorized personnel can remotely control the cameras, reducing the risk of unauthorized access and manipulation.\nImplementing WAF protection (A) secures against web application attacks but does not restrict unauthorized administrative access.\nUpgrading the firmware (B) is good security hygiene but does not immediately mitigate the active threat.\nBlocking IP 104.18.16.29 (D) is a temporary measure, as an attacker can switch to another IP. A better long-term solution is whitelisting trusted IPs.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 92,
    "question_text": "A security architect is establishing requirements to design resilience in an enterprise system. This system will be extended to other physical locations. The system must:\n• Be survivable to one environmental catastrophe\n• Be recoverable within 24 hours of critical loss of availability\n• Be resilient to active exploitation of one site-to-site VPN solution\nWhich of the following best addresses these requirements?",
    "options": {
      "A": "Load-balance connection attempts and data Ingress at internet gateways",
      "B": "Allocate fully redundant and geographically distributed standby sites.",
      "C": "Employ layering of routers from diverse vendors",
      "D": "Lease space to establish cold sites throughout other countries",
      "E": "Use orchestration to procure, provision, and transfer application workloads to cloud services",
      "F": "Implement full weekly backups to be stored off-site for each of the company's sites"
    },
    "correct_answer": "B",
    "explanation": "To design resilience in an enterprise system that can survive environmental catastrophes, recover within 24 hours, and be resilient to active exploitation, the best strategy is to allocate fully redundant and geographically distributed standby sites. Here’s why:\nGeographical Redundancy: Having geographically distributed standby sites ensures that if one site is affected by an environmental catastrophe, the other sites can take over, providing continuity of operations.\nFull Redundancy: Fully redundant sites mean that all critical systems and data are replicated, enabling quick recovery in the event of a critical loss of availability.\nResilience to Exploitation: Distributing resources across multiple sites reduces the risk of a single point of failure and increases resilience against targeted attacks.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 93,
    "question_text": "A systems administrator works with engineers to process and address vulnerabilities as a result of continuous scanning activities. The primary challenge faced by the administrator is differentiating between valid and invalid findings. Which of the following would the systems administrator most likely verify is properly configured?",
    "options": {
      "A": "Report retention time",
      "B": "Scanning credentials",
      "C": "Exploit definitions",
      "D": "Testing cadence"
    },
    "correct_answer": "B",
    "explanation": "When differentiating between valid and invalid findings from vulnerability scans, the systems administrator should verify that the scanning credentials are properly configured. Valid credentials ensure that the scanner can authenticate and access the systems being evaluated, providing accurate and comprehensive results. Without proper credentials, scans may miss vulnerabilities or generate false positives, making it difficult to prioritize and address the findings effectively.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 94,
    "question_text": "A security analyst received a report that an internal web page is down after a company-wide update to the web browser. Given the following error message:\nYour connection is not private.\nAttackers Might Be Trying to Steal Your Information from www.internal website.company.com\nNET::ERR_CERT_WEAK_SIGNATURE_ALGORITHM\nWhich of the following is the best way to fix this issue?",
    "options": {
      "A": "Rewriting any legacy web functions",
      "B": "Disabling all deprecated ciphers",
      "C": "Blocking all non-essential ports",
      "D": "Discontinuing the use of self-signed certificates"
    },
    "correct_answer": "D",
    "explanation": "The error message \"NET::ERR_CERT_WEAK_SIGNATURE_ALGORITHM\" indicates that the web browser is rejecting the certificate because it uses a weak signature algorithm. This commonly happens with self-signed certificates, which often use outdated or insecure algorithms.\nWhy Discontinue Self-Signed Certificates?\nSecurity Compliance: Modern browsers enforce strict security standards and may reject certificates that do not comply with these standards.\nTrusted Certificates: Using certificates from a trusted Certificate Authority (CA) ensures compliance with security standards and is less likely to be flagged as insecure.\nWeak Signature Algorithm: Self-signed certificates might use weak algorithms like MD5 or SHA-1, which are considered insecure.\nOther options do not address the specific cause of the certificate error:\nA. Rewriting legacy web functions: Does not address the certificate issue.\nB. Disabling deprecated ciphers: Useful for improving security but not related to the certificate error.\nC. Blocking non-essential ports: This is unrelated to the issue of certificate validation.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 95,
    "question_text": "Recent reports indicate that a software tool is being exploited. Attackers were able to bypass user access controls and load a database. A security analyst needs to find the vulnerability and recommend a mitigation. The analyst generates the following output:\n[Image of output showing console commands like 'find', 'cat', 'grep', 'sudo nano' with results indicating potential credentials or sensitive information.]\nWhich of the following would the analyst most likely recommend?",
    "options": {
      "A": "Installing appropriate EDR tools to block pass-the-hash attempts",
      "B": "Adding additional time to software development to perform fuzz testing",
      "C": "Removing hard coded credentials from the source code",
      "D": "Not allowing users to change their local passwords"
    },
    "correct_answer": "C",
    "explanation": "The output indicates that the software tool contains hard-coded credentials, which attackers can exploit to bypass user access controls and load the database. The most likely recommendation is to remove hard-coded credentials from the source code. Here’s why:\nSecurity Best Practices: Hard-coded credentials are a significant security risk because they can be easily discovered through reverse engineering or simple inspection of the code. Removing them reduces the risk of unauthorized access.\nCredential Management: Credentials should be managed securely using environment variables, secure vaults, or configuration management tools that provide encryption and access controls.\nMitigation of Exploits: By eliminating hard-coded credentials, the organization can prevent attackers from easily bypassing authentication mechanisms and gaining unauthorized access to sensitive systems.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 96,
    "question_text": "You are a security analyst tasked with interpreting an Nmap scan output from company’s privileged network.\nThe company’s hardening guidelines indicate the following:\nThere should be one primary server or service per device.\nOnly default ports should be used.\nNon-secure protocols should be disabled.\nINSTRUCTIONS\nUsing the Nmap output, identify the devices on the network and their roles, and any open ports that should be closed.\nFor each device found by Nmap, add a device entry to the Devices Discovered list, with the following information:\nThe IP address of the device\nThe primary server or service of the device (Note that each IP should by associated with one service/port only)\nThe protocol(s) that should be disabled based on the hardening guidelines (Note that multiple ports may need to be closed to comply with the hardening guidelines)\nIf at any time you would like to bring back the initial state of the simulation, please click the Reset All button.\n[Image showing Nmap scan output for 10.1.45.65, 10.1.45.66, 10.1.45.67, 10.1.45.68.]\n[Image showing the simulation interface with a list of Discovered Devices and sections to add Device, Role, Disable Protocols.]\n[Image showing the simulation interface after selecting options for each device, mapping IP to Role/Service and listing protocols/ports to disable.]",
    "options": {},
    "correct_answer": null,
    "explanation": "Based on the Nmap scan output and hardening guidelines:\n10.1.45.65:\nOpen Ports: 21/tcp (ftp), 22/tcp (ssh), 80/tcp (http), 443/tcp (https), 8080/tcp (http-proxy)\nDevice Type: General purpose device running Windows.\nPrimary Service: HTTP (port 80) or HTTPS (port 443) - The instruction says associate with one service/port only. Let's assume SFTP/FTP server as indicated in the answer mapping shown in the screenshot.\nProtocols to Disable: FTP (port 21 - cleartext), HTTP (port 80 - cleartext), SSH (port 22 - could be kept if needed for management but default port/config should be reviewed), HTTP-proxy (port 8080 - default port not required).\nAccording to the screenshot solution, 10.1.45.65 is mapped as SFTP Server and port 8080 is disabled.\n\n10.1.45.66:\nOpen Ports: 25/tcp (smtp), 415/tcp (sflm), 443/tcp (https), 587/tcp (submission), 995/tcp (pop3s)\nDevice Type: Barracuda Networks Spam Firewall.\nPrimary Service: SMTP (port 25) or Email related service.\nProtocols to Disable: SMTP (port 25 - default port, should be reviewed), SFLM (port 415 - not a default port), HTTPS (port 443 - okay if needed), Submission (port 587 - default port, could be kept), POP3S (port 995 - default port, could be kept).\nAccording to the screenshot solution, 10.1.45.66 is mapped as Email Server and ports 415 and 443 are disabled.\n\n10.1.45.67:\nOpen Ports: 21/tcp (ftp), 80/tcp (http), 443/tcp (ssl/http), 2190/tcp (unknown), 4443/tcp (ssl)\nDevice Type: General purpose Windows device.\nPrimary Service: HTTP (port 80) or HTTPS (port 443) or Web server as indicated in the answer mapping.\nProtocols to Disable: FTP (port 21 - cleartext), HTTP (port 80 - cleartext), Unknown (port 2190 - not a default port), SSL (port 4443 - not a default port/protocol should be reviewed).\nAccording to the screenshot solution, 10.1.45.67 is mapped as Web Server and ports 21 and 80 are disabled.\n\n10.1.45.68:\nOpen Ports: 21/tcp (ftp), 443/tcp (ssl/http)\nDevice Type: General purpose device.\nPrimary Service: Unknown.\nProtocols to Disable: FTP (port 21 - cleartext), HTTPS (port 443 - okay if needed).\nAccording to the screenshot solution, 10.1.45.68 is mapped as UTM Appliance and port 21 is disabled.",
    "is_simulation": true,
    "question_type": "simulation"
  },
  {
    "question_number": 97,
    "question_text": "A company receives reports about misconfigurations and vulnerabilities in a third-party hardware device that is part of its released products. Which of the following solutions is the best way for the company to identify possible issues at an earlier stage?",
    "options": {
      "A": "Performing vulnerability tests on each device delivered by the providers",
      "B": "Performing regular red-team exercises on the vendor production line",
      "C": "Implementing a monitoring process for the integration between the application and the vendor appliance",
      "D": "Implementing a proper supply chain risk management program"
    },
    "correct_answer": "D",
    "explanation": "Addressing misconfigurations and vulnerabilities in third-party hardware requires a comprehensive approach to manage risks throughout the supply chain. Implementing a proper supply chain risk management (SCRM) program is the most effective solution as it encompasses the following:\nHolistic Approach: SCRM considers the entire lifecycle of the product, from initial design through to delivery and deployment. This ensures that risks are identified and managed at every stage.\nVendor Management: It includes thorough vetting of suppliers and ongoing assessments of their security practices, which can identify and mitigate vulnerabilities early.\nRegular Audits and Assessments: A robust SCRM program involves regular audits and assessments, both internally and with suppliers, to ensure compliance with security standards and best practices.\nCollaboration and Communication: Ensures that there is effective communication and collaboration between the company and its suppliers, leading to faster identification and resolution of issues.\nOther options, while beneficial, do not provide the same comprehensive risk management:\nA. Performing vulnerability tests on each device delivered by the providers: While useful, this is reactive and only addresses issues after they have been delivered.\nB. Performing regular red-team exercises on the vendor production line: This can identify vulnerabilities but is not as comprehensive as a full SCRM program.\nC. Implementing a monitoring process for the integration between the application and the vendor appliance: This is important but only covers the integration phase, not the entire supply chain.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 98,
    "question_text": "A senior security engineer flags the following log file snippet as having likely facilitated an attacker’s lateral movement in a recent breach:\nqry_source: 19.27.214.22 TCP/53\nqry_dest: 199.105.22.13 TCP/53\nqry_type: AXFR\n| in comptia.org\n------------ directoryserver1 A 10.80.8.10\n------------ directoryserver2 A 10.80.8.11\n------------ directoryserver3 A 10.80.8.12\n------------ internal-dns A 10.80.9.1\n----------- www-int A 10.80.9.3\n------------ fshare A 10.80.9.4\n------------ sip A 10.80.9.5\n------------ msn-crit-apcs A 10.81.22.33\nWhich of the following solutions, if implemented, would mitigate the risk of this issue reoccurring?",
    "options": {
      "A": "Disabling DNS zone transfers",
      "B": "Restricting DNS traffic to UDP/53",
      "C": "Implementing DNS masking on internal servers",
      "D": "Permitting only clients from internal networks to query DNS"
    },
    "correct_answer": "A",
    "explanation": "Comprehensive and Detailed Explanation:\nThe log shows an AXFR (zone transfer) query, which exposed internal DNS records, aiding lateral movement. Let’s evaluate:\nA. Disabling DNS zone transfers: AXFR allows full DNS zone data to be transferred. Disabling it externally prevents attackers from mapping internal networks, directly mitigating this issue per CAS-005’s security operations focus.\nB. Restricting to UDP/53: AXFR uses TCP/53, so this wouldn’t stop it.\nC. DNS masking: Obscures records but isn’t a standard term for this fix.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 99,
    "question_text": "Operational technology often relies upon aging command, control, and telemetry subsystems that were created with the design assumption of:",
    "options": {
      "A": "operating in an isolated/disconnected system.",
      "B": "communicating over distributed environments",
      "C": "untrustworthy users and systems being present.",
      "D": "an available Ethernet/IP network stack for flexibility.",
      "E": "anticipated eavesdropping from malicious actors."
    },
    "correct_answer": "A",
    "explanation": "Comprehensive and Detailed Step-by-Step Explanation:\nUnderstanding the Scenario: The question focuses on the historical design assumptions behind older operational technology (OT) systems, particularly in the context of command, control, and telemetry.\nAnalyzing the Answer Choices:\nA. operating in an isolated/disconnected system: This is the most accurate assumption for many legacy OT systems. Historically, these systems were designed to operate in air-gapped environments, completely isolated from external networks (including the internet).",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 100,
    "question_text": "A user reports application access issues to the help desk. The help desk reviews the logs for the user:\n[Image of logs showing Time, Internal IP, Public IP, IP Geolocation, Application, and Action. Entries show access to VPN, Email, HR System from Toronto and Los Angeles at different times, with one HR System access attempt from Toronto being denied.]\nWhich of the following is most likely the reason for the issue?",
    "options": {
      "A": "The user inadvertently tripped the impossible travel security rule in the SSO system.",
      "B": "A threat actor has compromised the user's account and attempted to log in.",
      "C": "The user is not allowed to access the human resources system outside of business hours.",
      "D": "The user did not attempt to connect from an approved subnet"
    },
    "correct_answer": "A",
    "explanation": "Based on the provided logs, the user has accessed various applications from different geographic locations within a very short timeframe. This pattern is indicative of the \"impossible travel\" security rule, a common feature in Single Sign-On (SSO) systems designed to detect and prevent fraudulent access attempts.\nAnalysis of Logs:\nAt 8:47 p.m., the user accessed a VPN from Toronto.\nAt 8:48 p.m., the user accessed email from Los Angeles.\nAt 8:48 p.m., the user accessed the human resources system from Los Angeles.\nAt 8:49 p.m., the user accessed email again from Los Angeles.\nAt 8:52 p.m., the user attempted to access the human resources system from Toronto, which was denied.\nThese rapid changes in location are physically impossible and typically trigger security measures to prevent unauthorized access. The SSO system detected these inconsistencies and likely flagged the activity as suspicious, resulting in access denial.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 101,
    "question_text": "An organization is developing a disaster recovery plan that requires data to be backed up and available at a moment's notice. Which of the following should the organization consider first to address this requirement?",
    "options": {
      "A": "Implement a change management plan to ensure systems are using the appropriate versions.",
      "B": "Hire additional on-call staff to be deployed if an event occurs.",
      "C": "Design an appropriate warm site for business continuity.",
      "D": "Identify critical business processes and determine associated software and hardware requirements."
    },
    "correct_answer": "D",
    "explanation": "For a disaster recovery (DR) plan requiring immediate data availability, the first step is understanding what needs to be protected and recovered. Identifying critical business processes and their associated software and hardware requirements establishes the foundation for the DR plan. This ensures that backups and recovery mechanisms align with business priorities, meeting the \"moment's notice\" requirement.\nOption A: A change management plan is important for system consistency but doesn’t directly address immediate data availability in a DR context.\nOption B: Hiring staff supports execution but doesn’t define what needs to be recovered or how. It’s a later step.\nOption C: A warm site (a partially operational backup site) is a good DR solution, but designing it comes after identifying critical processes and resources.\nOption D: This is the first step in any DR planning process—knowing what’s critical ensures the plan meets availability goals efficiently.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 102,
    "question_text": "The identity and access management team is sending logs to the SIEM for continuous monitoring. The deployed log collector is forwarding logs to the SIEM. However, only false positive alerts are being generated. Which of the following is the most likely reason for the inaccurate alerts?",
    "options": {
      "A": "The compute resources are insufficient to support the SIEM",
      "B": "The SIEM indexes are too large",
      "C": "The data is not being properly parsed",
      "D": "The retention policy is not properly configured"
    },
    "correct_answer": "C",
    "explanation": "Proper parsing of data is crucial for the SIEM to accurately interpret and analyze the logs being forwarded by the log collector. If the data is not parsed correctly, the SIEM may misinterpret the logs, leading to false positives and inaccurate alerts. Ensuring that the log data is correctly parsed allows the SIEM to correlate and analyze the logs effectively, which is essential for accurate alerting and monitoring.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 103,
    "question_text": "A security engineer is assisting a DevOps team that has the following requirements for container images:\nEnsure container images are hashed and use version controls.\nEnsure container images are up to date and scanned for vulnerabilities.\nWhich of the following should the security engineer do to meet these requirements?",
    "options": {
      "A": "Enable clusters on the container image and configure the mesh with ACLs.",
      "B": "Enable new security and quality checks within a CI/CD pipeline.",
      "C": "Enable audits on the container image and monitor for configuration changes.",
      "D": "Enable pulling of the container image from the vendor repository and deploy directly to operations."
    },
    "correct_answer": "B",
    "explanation": "Implementing security and quality checks in a CI/CD pipeline ensures that:\nContainer images are scanned for vulnerabilities before deployment.\nVersion control is enforced, preventing unauthorized changes.\nHashes validate image integrity.\nOther options:\nA (Configuring ACLs on mesh networks) improves access control but does not ensure scanning.\nC (Audits on container images) detect changes but do not enforce best practices.\nD (Pulling from a vendor repository) does not ensure vulnerability scanning.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 104,
    "question_text": "After some employees were caught uploading data to online personal storage accounts, a company becomes concerned about data leaks related to sensitive, internal documentation. Which of the following would the company most likely do to decrease this type of risk?",
    "options": {
      "A": "Improve firewall rules to avoid access to those platforms.",
      "B": "Implement a cloud-access security broker",
      "C": "Create SIEM rules to raise alerts for access to those platforms",
      "D": "Deploy an internet proxy that filters certain domains"
    },
    "correct_answer": "B",
    "explanation": "A Cloud Access Security Broker (CASB) is a security policy enforcement point placed between cloud service consumers and cloud service providers to combine and interject enterprise security policies as cloud-based resources are accessed. Implementing a CASB provides several benefits:\nA. Improve firewall rules to avoid access to those platforms: This can help but is not as effective or comprehensive as a CASB.\nB. Implement a cloud-access security broker: A CASB can provide visibility into cloud application usage, enforce data security policies, and protect against data leaks by monitoring and controlling access to cloud services. It also provides advanced features like data encryption, data loss prevention (DLP), and compliance monitoring.\nC. Create SIEM rules to raise alerts for access to those platforms: This helps in monitoring but does not prevent data leaks.\nD. Deploy an internet proxy that filters certain domains: This can block access to specific sites but lacks the granular control and visibility provided by a CASB.\nImplementing a CASB is the most comprehensive solution to decrease the risk of data leaks by providing visibility, control, and enforcement of security policies for cloud services.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 105,
    "question_text": "A company's SIEM is continuously reporting false positives and false negatives. The security operations team has implemented configuration changes to troubleshoot possible reporting errors. Which of the following sources of information best supports the required analysis process? (Select two).",
    "options": {
      "A": "Third-party reports and logs",
      "B": "Trends",
      "C": "Dashboards",
      "D": "Alert failures",
      "E": "Network traffic summaries",
      "F": "Manual review processes"
    },
    "correct_answer": "A B",
    "explanation": "When dealing with false positives and false negatives reported by a Security Information and Event Management (SIEM) system, the goal is to enhance the accuracy of the alerts and ensure that actual threats are identified correctly. The following sources of information best support the analysis process:\nA. Third-party reports and logs: Utilizing external sources of information such as threat intelligence reports, vendor logs, and other third-party data can provide a broader perspective on potential threats. These sources often contain valuable insights and context that can help correlate events more accurately, reducing the likelihood of false positives and false negatives.\nB. Trends: Analyzing trends over time can help in understanding patterns and anomalies in the data. By observing trends, the security team can distinguish between normal and abnormal behavior, which aids in fine-tuning the SIEM configurations to better detect true positives and reduce false alerts.\nOther options such as dashboards, alert failures, network traffic summaries, and manual review processes are also useful but are more operational rather than foundational for understanding the root causes of reporting errors in SIEM configurations.",
    "is_simulation": false,
    "question_type": "multiple_choice_multiple_answer"
  },
  {
    "question_number": 106,
    "question_text": "Which of the following are risks associated with vendor lock-in? (Select two).",
    "options": {
      "A": "The client can seamlessly move data.",
      "B": "The vendor can change product offerings.",
      "C": "The client receives a sufficient level of service.",
      "D": "The client experiences decreased quality of service.",
      "E": "The client can leverage a multicloud approach.",
      "F": "The client experiences increased interoperability."
    },
    "correct_answer": "B D",
    "explanation": "Vendor lock-in occurs when a client is overly dependent on a vendor, limiting flexibility. Risks include:\nOption B: Vendors changing offerings (e.g., features, pricing) can disrupt the client, a key lock-in risk.\nOption D: Decreased quality of service may result from reliance on a single vendor without alternatives.\nOption A: Seamless data movement is a benefit, not a risk.\nOption C: Sufficient service is neutral or positive, not a risk.\nOption E: Multicloud is hindered by lock-in, not a risk of it.\nOption F: Increased interoperability contradicts lock-in’s limitations.",
    "is_simulation": false,
    "question_type": "multiple_choice_multiple_answer"
  },
  {
    "question_number": 107,
    "question_text": "A security operations engineer needs to prevent inadvertent data disclosure when encrypted SSDs are reused within an enterprise. Which of the following is the most secure way to achieve this goal?",
    "options": {
      "A": "Executing a script that deletes and overwrites all data on the SSD three times",
      "B": "Wiping the SSD through degaussing",
      "C": "Securely deleting the encryption keys used by the SSD",
      "D": "Writing non-zero, random data to all cells of the SSD"
    },
    "correct_answer": "C",
    "explanation": "The most secure way to prevent inadvertent data disclosure when encrypted SSDs are reused is to securely delete the encryption keys used by the SSD. Without the encryption keys, the data on the SSD remains encrypted and is effectively unreadable, rendering any residual data useless. This method is more reliable and efficient than overwriting data multiple times or using other physical destruction methods.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 108,
    "question_text": "Users must accept the terms presented in a captive portal when connecting to a guest network. Recently, users have reported that they are unable to access the Internet after joining the network. A network engineer observes the following:\n• Users should be redirected to the captive portal.\n• The Motive portal runs TLS 1.2\n• Newer browser versions encounter security errors that cannot be bypassed\n• Certain websites cause unexpected re-directs\nWhich of the following most likely explains this behavior?",
    "options": {
      "A": "The TLS ciphers supported by the captive portal are deprecated",
      "B": "Employment of the HSTS setting is proliferating rapidly.",
      "C": "Allowed traffic rules are causing the NIPS to drop legitimate traffic",
      "D": "An attacker is redirecting supplicants to an evil twin WLAN."
    },
    "correct_answer": "A",
    "explanation": "The most likely explanation for the issues encountered with the captive portal is that the TLS ciphers supported by the captive portal are deprecated. Here’s why:\nTLS Cipher Suites: Modern browsers are continuously updated to support the latest security standards and often drop support for deprecated and insecure cipher suites. If the captive portal uses outdated TLS ciphers, newer browsers may refuse to connect, causing security errors.\nHSTS and Browser Security: Browsers with HTTP Strict Transport Security (HSTS) enabled will not allow connections to sites with weak security configurations. Deprecated TLS ciphers would cause these browsers to block the connection.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 109,
    "question_text": "A global manufacturing company has an internal application that is critical to making products. This application cannot be updated and must be available in the production area. A security architect is implementing security for the application. Which of the following best describes the action the architect should take?",
    "options": {
      "A": "Disallow wireless access to the application.",
      "B": "Deploy Intrusion detection capabilities using a network tap",
      "C": "Create an acceptable use policy for the use of the application",
      "D": "Create a separate network for users who need access to the application"
    },
    "correct_answer": "D",
    "explanation": "Creating a separate network for users who need access to the application is the best action to secure an internal application that is critical to the production area and cannot be updated.\nWhy Separate Network?\nNetwork Segmentation: Isolates the critical application from the rest of the network, reducing the risk of compromise and limiting the potential impact of any security incidents.\nControlled Access: Ensures that only authorized users have access to the application, enhancing security and reducing the attack surface.\nMinimized Risk: Segmentation helps in protecting the application from vulnerabilities that could be exploited from other parts of the network.\nOther options, while beneficial, do not provide the same level of security for a critical application:\nA. Disallow wireless access: Useful but does not provide comprehensive protection.\nB. Deploy intrusion detection capabilities using a network tap: Enhances monitoring but does not provide the same level of isolation and control.\nC. Create an acceptable use policy: Important for governance but does not provide technical security controls.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 110,
    "question_text": "A security architect wants to develop a baseline of security configurations. These configurations will be automatically utilized when a machine is created. Which of the following technologies should the security architect deploy to accomplish this goal?",
    "options": {
      "A": "Short",
      "B": "GASB",
      "C": "Ansible",
      "D": "CMDB"
    },
    "correct_answer": "C",
    "explanation": "To develop a baseline of security configurations that will be automatically utilized when a machine is created, the security architect should deploy Ansible. Here’s why:\nAutomation: Ansible is an automation tool that allows for the configuration, management, and deployment of applications and systems. It ensures that security configurations are consistently applied across all new machines.\nScalability: Ansible can scale to manage thousands of machines, making it suitable for large enterprises that need to maintain consistent security configurations across their infrastructure.\nCompliance: By using Ansible, organizations can enforce compliance with security policies and standards, ensuring that all systems are configured according to best practices.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 111,
    "question_text": "A company reduced its staff 60 days ago, and applications are now starting to fail. The security analyst is investigating to determine if there is malicious intent for the application failures. The security analyst reviews the following logs:\nMar 5 22:09:50 akj3 sshd[21502]: Success login for user01 from 192.168.2.5\nMar 5 22:10:00 akj3 sshd[21502]: Failed login for userID from 192.168.2.5\nWhich of the following is the most likely reason for the application failures?",
    "options": {
      "A": "The user’s account was set as a service account.",
      "B": "The user's home directory was deleted.",
      "C": "The user does not have sudo access.",
      "D": "The root password has been changed."
    },
    "correct_answer": "B",
    "explanation": "Comprehensive and Detailed Step-by-Step Explanation:\nWhen an employee leaves a company, their home directory might be deleted along with their account, leading to application failures if the directory contained configuration files, dependencies, or system scripts.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 112,
    "question_text": "A security analyst is reviewing the following event timeline from a COR solution:\n[Image of timeline showing Time, File name, File action, Action verdict.]\nWhich of the following most likely has occurred and needs to be fixed?",
    "options": {
      "A": "The DLP has failed to block malicious exfiltration and data tagging is not being utilized properly",
      "B": "An EDR bypass was utilized by a threat actor and updates must be installed by the administrator.",
      "C": "A logic flaw has introduced a TOCTOU vulnerability and must be addressed by the COR vendor",
      "D": "A potential insider threat is being investigated and will be addressed by the senior management team."
    },
    "correct_answer": "C",
    "explanation": "The event timeline indicates a sequence where a file (hr-reporting.docx) was saved, scanned, executed, and eventually found to contain malware. The critical issue here is that the malware scan completed after the file was already executed. This suggests a Time-Of-Check to Time-Of-Use (TOCTOU) vulnerability, where the state of the file changed between the time it was checked and the time it was used.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 113,
    "question_text": "Which of the following best explains the business requirement a healthcare provider fulfills by encrypting patient data at rest?",
    "options": {
      "A": "Securing data transfer between hospitals",
      "B": "Providing for non-repudiation of data",
      "C": "Reducing liability from identity theft",
      "D": "Protecting privacy while supporting portability."
    },
    "correct_answer": "D",
    "explanation": "Encrypting patient data at rest is a critical requirement for healthcare providers to ensure compliance with regulations such as the Health Insurance Portability and Accountability Act (HIPAA). The primary business requirement fulfilled by this practice is the protection of patient privacy while supporting the portability of medical information. By encrypting data at rest, healthcare providers safeguard sensitive patient information from unauthorized access, ensuring that privacy is maintained even if the storage media are compromised. Additionally, encryption supports the portability of patient records, allowing for secure transfer and access across different systems and locations while ensuring that privacy controls are in place.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 114,
    "question_text": "A security architect for a global organization with a distributed workforce recently received funding to deploy a CASB solution. Which of the following most likely explains the choice to use a proxy-based CASB?",
    "options": {
      "A": "The capability to block unapproved applications and services is possible",
      "B": "Privacy compliance obligations are bypassed when using a user-based deployment.",
      "C": "Protecting and regularly rotating API secret keys requires a significant time commitment",
      "D": "Corporate devices cannot receive certificates when not connected to on-premises devices"
    },
    "correct_answer": "A",
    "explanation": "A proxy-based Cloud Access Security Broker (CASB) is chosen primarily for its ability to block unapproved applications and services. Here’s why:\nApplication and Service Control: Proxy-based CASBs can monitor and control the use of applications and services by inspecting traffic as it passes through the proxy. This allows the organization to enforce policies that block unapproved applications and services, ensuring compliance with security policies.\nVisibility and Monitoring: By routing traffic through the proxy, the CASB can provide detailed visibility into user activities and data flows, enabling better monitoring and threat detection.\nReal-Time Protection: Proxy-based CASBs can provide real-time protection against threats by analyzing and controlling traffic before it reaches the end user, thus preventing the use of risky applications and services.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 115,
    "question_text": "A security architect must make sure that the least number of services as possible is exposed in order to limit an adversary's ability to access the systems. Which of the following should the architect do first?",
    "options": {
      "A": "Enforce Secure Boot.",
      "B": "Perform attack surface reduction.",
      "C": "Disable third-party integrations.",
      "D": "Limit access to the systems."
    },
    "correct_answer": "B",
    "explanation": "Attack surface reduction focuses on minimizing unnecessary services, open ports, and vulnerabilities, reducing the exposure to potential adversaries. This aligns with zero trust and least privilege principles.\nSecure Boot (A) helps ensure system integrity but does not minimize exposed services.\nDisabling third-party integrations (C) may help, but broader attack surface reduction is the best first step.\nLimiting access (D) is important but does not directly reduce exposed services.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 116,
    "question_text": "A security engineer is given the following requirements:\n• An endpoint must only execute internally signed applications\n• Administrator accounts cannot install unauthorized software.\n• Attempts to run unauthorized software must be logged\nWhich of the following best meets these requirements?",
    "options": {
      "A": "Maintaining appropriate account access through directory management and controls",
      "B": "Implementing a CSPM platform to monitor updates being pushed to applications",
      "C": "Deploying an EDR solution to monitor and respond to software installation attempts",
      "D": "Configuring application control with blocked hashes and enterprise-trusted root certificates"
    },
    "correct_answer": "D",
    "explanation": "To meet the requirements of only allowing internally signed applications, preventing unauthorized software installations, and logging attempts to run unauthorized software, configuring application control with blocked hashes and enterprise-trusted root certificates is the best solution. This approach ensures that only applications signed by trusted certificates are allowed to execute, while all other attempts are blocked and logged. It effectively prevents unauthorized software installations by restricting execution to pre-approved applications.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 117,
    "question_text": "A security administrator needs to automate alerting. The server generates structured log files that need to be parsed to determine whether an alarm has been triggered. Given the following code function:\ndef parse_log(log_file):\n    with open(log_file) as log_f:\n        log_data = json.load(log_f)\n        if log_data.get(\"error_log\") == \"System 1\" and log_data.get(\"InAlarmState\"):\n            trigger_alarm(\"Alarm triggered for System 1\")\nWhich of the following is most likely the log input that the code will parse?\n[Image showing four JSON code snippets as options A, B, C, D.]",
    "options": {
      "A": "{\"error_log\": \"System 1\", \"InAlarmState\": True}",
      "B": "{\"log_entry\": \"<error> System 1 </error>\", \"status\": \"InAlarmState=true\"}",
      "C": "<error_log>System 1</error_log><InAlarmState>true</InAlarmState>",
      "D": "{\"error_log\": \"System 1\", \"InAlarmState\": \"Yes\"}"
    },
    "correct_answer": "A",
    "explanation": "The code function provided in the question seems to be designed to parse JSON formatted logs to check for an alarm state. Option A is a JSON format that matches the structure likely expected by the code. The presence of the \"error_log\" and \"InAlarmState\" keys suggests that this is the correct input format.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 118,
    "question_text": "An auditor is reviewing the logs from a web application to determine the source of an incident. The web application architecture includes an internet-accessible application load balancer, a number of web servers in a private subnet, application servers, and one database server in a tiered configuration. The application load balancer cannot store the logs. The following are sample log snippets:\nWeb server logs:\n192.168.1.10 - - [24/Oct/2020 11:24:34 +05:00] \"GET /bin/bash\" HTTP/1.1\" 200 453 Safari/536.36\n192.168.1.10 - - [24/Oct/2020 11:24:35 +05:00] \"GET / HTTP/1.1\" 200 453 Safari/536.36\nApplication server logs:\n24/Oct/2020 11:24:34 +05:00 - 192.168.2.11 - request does not match a known local user. Querying DB\n24/Oct/2020 11:24:35 +05:00 - 192.168.2.12 - root path. Begin processing\nDatabase server logs:\n24/Oct/2020 11:24:34 +05:00 [Warning] 'option read_buffer_size1 unassigned value 0 adjusted to 2048\n24/Oct/2020 11:24:35 +05:00 [Warning] CA certificate ca.pem is self-signed.\nWhich of the following should the auditor recommend to ensure future incidents can be traced back to the sources?",
    "options": {
      "A": "Enable the X-Forwarded-For header at the load balancer.",
      "B": "Install a software-based HIDS on the application servers.",
      "C": "Install a certificate signed by a trusted CA.",
      "D": "Use stored procedures on the database server.",
      "E": "Store the value of the $_SERVER['REMOTE_ADDR'] received by the web servers."
    },
    "correct_answer": "A",
    "explanation": "The issue is tracing the original source of requests in a tiered architecture with a load balancer. The web server logs show internal IPs (192.168.1.10), not the external client IPs, because the load balancer forwards requests without preserving the source. Enabling the X-Forwarded-For header on the load balancer adds the client’s original IP to the HTTP request headers, allowing downstream servers to log it. This ensures traceability without altering the architecture significantly.\nOption A: Correct—X-Forwarded-For is the standard solution for preserving client IPs through load balancers.\nOption B: A Host-based Intrusion Detection System (HIDS) detects anomalies but doesn’t address IP traceability.\nOption C: A trusted CA certificate fixes the self-signed warning but is unrelated to source tracking.\nOption D: Stored procedures improve database security but don’t help with IP logging.\nOption E: Storing $_SERVER['REMOTE_ADDR'] captures the load balancer’s IP, not the client’s, unless X-Forwarded-For is enabled.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 119,
    "question_text": "A security analyst received a notification from a cloud service provider regarding an attack detected on a web server. The cloud service provider shared the following information about the attack:\n• The attack came from inside the network.\n• The attacking source IP was from the internal vulnerability scanners.\n• The scanner is not configured to target the cloud servers.\nWhich of the following actions should the security analyst take first?",
    "options": {
      "A": "Create an allow list for the vulnerability scanner IPs in order to avoid false positives",
      "B": "Configure the scan policy to avoid targeting an out-of-scope host",
      "C": "Set network behavior analysis rules",
      "D": "Quarantine the scanner sensor to perform a forensic analysis"
    },
    "correct_answer": "D",
    "explanation": "When a security analyst receives a notification about an attack that appears to originate from an internal vulnerability scanner, it suggests that the scanner itself might have been compromised. This situation is critical because a compromised scanner can potentially conduct unauthorized scans, leak sensitive information, or execute malicious actions within the network. The appropriate first action involves containing the threat to prevent further damage and allow for a thorough investigation.\nHere’s why quarantining the scanner sensor is the best immediate action:\nContainment and Isolation: Quarantining the scanner will immediately prevent it from continuing any malicious activity or scans. This containment is crucial to protect the rest of the network from potential harm.\nForensic Analysis: By isolating the scanner, a forensic analysis can be performed to understand how it was compromised, what actions it took, and what data or systems might have been affected. This analysis will provide valuable insights into the nature of the attack and help in taking appropriate remedial actions.\nPreventing Further Attacks: If the scanner is allowed to continue operating, it might execute more unauthorized actions, leading to greater damage. Quarantine ensures that the threat is neutralized promptly.\nRoot Cause Identification: A forensic analysis can help identify vulnerabilities in the scanner’s configuration, software, or underlying system that allowed the compromise. This information is essential for preventing future incidents.\nOther options, while potentially useful in the long term, are not appropriate as immediate actions in this scenario:\nA. Create an allow list for the vulnerability scanner IPs to avoid false positives: This action addresses false positives but does not mitigate the immediate threat posed by the compromised scanner.\nB. Configure the scan policy to avoid targeting an out-of-scope host: This step is preventive for future scans but does not deal with the current incident where the scanner is already compromised.\nC. Set network behavior analysis rules: While useful for ongoing monitoring and detection, this does not address the immediate need to stop the compromised scanner’s activities.\nIn conclusion, the first and most crucial action is to quarantine the scanner sensor to halt any malicious activity and perform a forensic analysis to understand the scope and nature of the compromise. This step ensures that the threat is contained and provides a basis for further remediation efforts.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 120,
    "question_text": "A senior security engineer flags the following log file snippet as having likely facilitated an attacker's lateral movement in a recent breach:\n[Image of log file snippet showing DNS query type AXFR, source and destination IPs, and internal DNS records.]\nWhich of the following solutions, if implemented, would mitigate the risk of this issue reoccurring?",
    "options": {
      "A": "Disabling DNS zone transfers",
      "B": "Restricting DNS traffic to UDP/53",
      "C": "Implementing DNS masking on internal servers",
      "D": "Permitting only clients from internal networks to query DNS"
    },
    "correct_answer": "A",
    "explanation": "The log snippet indicates a DNS AXFR (zone transfer) request, which can be exploited by attackers to gather detailed information about an internal network's infrastructure. Disabling DNS zone transfers is the best solution to mitigate this risk. Zone transfers should generally be restricted to authorized secondary DNS servers and not be publicly accessible, as they can reveal sensitive network information that facilitates lateral movement during an attack.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 121,
    "question_text": "A security engineer is building a solution to disable weak CBC configuration for remote access connections to Linux systems. Which of the following should the security engineer modify?",
    "options": {
      "A": "The /etc/openssl.conf file, updating the virtual site parameter",
      "B": "The /etc/nsswitch.conf file, updating the name server",
      "C": "The /etc/hosts file, updating the IP parameter",
      "D": "The /etc/ssh/sshd_config file, updating the ciphers"
    },
    "correct_answer": "D",
    "explanation": "The sshd_config file is the main configuration file for the OpenSSH server. To disable weak CBC (Cipher Block Chaining) ciphers for SSH connections, the security engineer should modify the sshd_config file to update the list of allowed ciphers. This file typically contains settings for the SSH daemon, including which encryption algorithms are allowed.\nBy editing the /etc/ssh/sshd_config file and updating the Ciphers directive, weak ciphers can be removed, and only strong ciphers can be allowed. This change ensures that the SSH server does not use insecure encryption methods.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 122,
    "question_text": "A company finds logs with modified timestamps when compared to other systems. The security team decides to improve logging and auditing for incident response. Which of the following should the team do to best accomplish this goal?",
    "options": {
      "A": "Integrate a file-monitoring tool with the SIEM.",
      "B": "Change the log solution and integrate it with the existing SIEM.",
      "C": "Implement a central logging server, allowing only log ingestion.",
      "D": "Rotate and back up logs every 24 hours, encrypting the backups."
    },
    "correct_answer": "C",
    "explanation": "A central logging server ensures logs are collected in a tamper-proof manner and only ingested (not modified). This prevents attackers from altering logs locally.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 123,
    "question_text": "A threat hunter is identifying potentially malicious activity associated with an APT. When the threat hunter runs queries against the SIEM platform with a date range of 60 to 90 days ago, the involved account seems to be typically most active in the evenings. When the threat hunter reruns the same query with a date range of 5 to 30 days ago, the account appears to be most active in the early morning. Which of the following techniques is the threat hunter using to better understand the data?",
    "options": {
      "A": "TTP-based inquiries",
      "B": "User behavior analytics",
      "C": "Adversary emulation",
      "D": "OSINT analysis activities"
    },
    "correct_answer": "B",
    "explanation": "User behavior analytics (UBA) detects anomalous activity by analyzing historical patterns and comparing them to recent behavior. The time shift in account activity suggests potential compromise or misuse.\nTTP-based inquiries (A) focus on known attack tactics, techniques, and procedures but do not involve behavior tracking.\nAdversary emulation (C) simulates attacks but does not analyze real data trends.\nOSINT analysis (D) gathers intelligence from public sources, which is unrelated to internal account behavior analysis.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 124,
    "question_text": "A software development team requires valid data for internal tests. Company regulations, however do not allow the use of this data in cleartext. Which of the following solutions best meet these requirements?",
    "options": {
      "A": "Configuring data hashing",
      "B": "Deploying tokenization",
      "C": "Replacing data with null record",
      "D": "Implementing data obfuscation"
    },
    "correct_answer": "B",
    "explanation": "Tokenization replaces sensitive data elements with non-sensitive equivalents, called tokens, that can be used within the internal tests. The original data is stored securely and can be retrieved if necessary. This approach allows the software development team to work with data that appears realistic and valid without exposing the actual sensitive information.\nConfiguring data hashing (Option A) is not suitable for test data as it transforms the data into a fixed-length value that is not usable in the same way as the original data. Replacing data with null records (Option C) is not useful as it does not provide valid data for testing. Data obfuscation (Option D) could be an alternative but might not meet the regulatory requirements as effectively as tokenization.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 125,
    "question_text": "A systems administrator wants to introduce a newly released feature for an internal application. The administrator does not want to test the feature in the production environment. Which of the following locations is the best place to test the new feature?",
    "options": {
      "A": "Staging environment",
      "B": "Testing environment",
      "C": "CI/CD pipeline",
      "D": "Development environment"
    },
    "correct_answer": "A",
    "explanation": "The best location to test a newly released feature for an internal application, without affecting the production environment, is the staging environment. Here’s a detailed explanation:\nStaging Environment: This environment closely mirrors the production environment in terms of hardware, software, configurations, and settings. It serves as a final testing ground before deploying changes to production. Testing in the staging environment ensures that the new feature will behave as expected in the actual production setup.\nIsolation from Production: The staging environment is isolated from production, which means any issues arising from the new feature will not impact the live users or the integrity of the production data. This aligns with best practices in change management and risk mitigation.\nRealistic Testing: Since the staging environment replicates the production environment, it provides realistic testing conditions. This helps in identifying potential issues that might not be apparent in a development or testing environment, which often have different configurations and workloads.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 126,
    "question_text": "An incident response team is analyzing malware and observes the following:\n• Does not execute in a sandbox\n• No network IoCs\n• No publicly known hash match\n• No process injection method detected\nWhich of the following should the team do next to proceed with further analysis?",
    "options": {
      "A": "Use an online virus analysis tool to analyze the sample",
      "B": "Check for an anti-virtualization code in the sample",
      "C": "Utilize a new deployed machine to run the sample.",
      "D": "Search other internal sources for a new sample."
    },
    "correct_answer": "B",
    "explanation": "Malware that does not execute in a sandbox environment often contains anti-analysis techniques, such as anti-virtualization code. This code detects when the malware is running in a virtualized environment and alters its behavior to avoid detection. Checking for anti-virtualization code is a logical next step because:\nIt helps determine if the malware is designed to evade analysis tools.\nIdentifying such code can provide insights into the malware's behavior and intent.\nThis step can also inform further analysis methods, such as running the malware on physical hardware.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 127,
    "question_text": "An organization is implementing Zero Trust architecture. A systems administrator must increase the effectiveness of the organization's context-aware access system. Which of the following is the best way to improve the effectiveness of the system?",
    "options": {
      "A": "Secure zone architecture",
      "B": "Always-on VPN",
      "C": "Accurate asset inventory",
      "D": "Microsegmentation"
    },
    "correct_answer": "D",
    "explanation": "Microsegmentation is a critical strategy within Zero Trust architecture that enhances context-aware access systems by dividing the network into smaller, isolated segments. This reduces the attack surface and limits lateral movement of attackers within the network. It ensures that even if one segment is compromised, the attacker cannot easily access other segments. This granular approach to network security is essential for enforcing strict access controls and monitoring within Zero Trust environments.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 128,
    "question_text": "Source code snippets for two separate malware samples are shown below:\nSample 1:\nknockEmDown(String e) {\nif(target.isAccessed()) {\ntarget.toShell(e);\nSystem.out.println(e.toString());\nc2.sendTelemetry(target.hostname.toString + \" is \" + e.toString());\n} else {\ntarget.close();\n}\n}\nSample 2:\ntargetSys(address a) {\nif(address.isIPv4()) {\naddress.connect(1337);\naddress.keepAlive(\"paranoid\");\nString status = knockEmDown(address.current);\nremote.sendC2(address.current + \" is \" + status);\n} else {\nthrow Exception e;\n}\n}\nWhich of the following describes the most important observation about the two samples?",
    "options": {
      "A": "Telemetry is first buffered and then transmitted in paranoid mode.",
      "B": "The samples were probably written by the same developer.",
      "C": "Both samples use IP connectivity for command and control.",
      "D": "Sample 1 is the target agent while Sample 2 is the C2 server."
    },
    "correct_answer": "B",
    "explanation": "Comprehensive and Detailed Step-by-Step Explanation:\nBoth samples share similar function names, variable naming styles, and logic flow, indicating that they were likely written by the same developer. This is a key observation in malware attribution, as cyber threat analysts often look for unique coding styles to link malware to specific threat actors.\nThe presence of C2 (Command and Control) communication in both samples supports this theory, as attackers often reuse parts of their own malware code across different attacks.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 129,
    "question_text": "A company must build and deploy security standards for all servers in its on-premises and cloud environments based on hardening guidelines. Which of the following solutions most likely meets the requirements?",
    "options": {
      "A": "Develop a security baseline to integrate with the vulnerability scanning platform to alert about any server not aligned with the new security standards.",
      "B": "Create baseline images for each OS in use, following security standards, and integrate the images into the patching and deployment solution.",
      "C": "Build all new images from scratch, installing only needed applications and modules in accordance with the new security standards.",
      "D": "Run a script during server deployment to remove all the unnecessary applications as part of provisioning."
    },
    "correct_answer": "B",
    "explanation": "Creating secure baseline images ensures consistent, repeatable deployment aligned with hardening standards. These images can be used across on-premises and cloud environments, ensuring compliance and reducing misconfigurations.\nVulnerability alerts (A) are reactive, not preventive.\nBuilding images from scratch (C) is time-consuming and unnecessary if baselines exist.\nScripts for cleanup (D) are useful but do not prevent initial insecure configurations.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 130,
    "question_text": "A vulnerability on a web server identified the following:\nTLS 1.2 Cipher Suites: The server supports the following 4 cipher suites:\nTLS_DHE_RSA_WITH_AES_128_CBC_SHA\nTLS_RSA_WITH_AES_128_CBC_SHA\nTLS_DHE_RSA_WITH_AES_256_GCM_SHA384\nTLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\nWhich of the following actions would most likely eliminate on path decryption attacks? (Select two).",
    "options": {
      "A": "Disallowing cipher suites that use ephemeral modes of operation for key agreement",
      "B": "Removing support for CBC-based key exchange and signing algorithms",
      "C": "Adding TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA256",
      "D": "Implementing HIPS rules to identify and block BEAST attack attempts",
      "E": "Restricting cipher suites to only allow TLS_RSA_WITH_AES_128_CBC_SHA",
      "F": "Increasing the key length to 256 for TLS_RSA_WITH_AES_128_CBC_SHA"
    },
    "correct_answer": "B C",
    "explanation": "On-path decryption attacks, such as BEAST (Browser Exploit Against SSL/TLS) and other related vulnerabilities, often exploit weaknesses in the implementation of CBC (Cipher Block Chaining) mode. To mitigate these attacks, the following actions are recommended:\nB. Removing support for CBC-based key exchange and signing algorithms: CBC mode is vulnerable to certain attacks like BEAST. By removing support for CBC-based ciphers, you can eliminate one of the primary vectors for these attacks. Instead, use modern cipher modes like GCM (Galois/Counter Mode) which offer better security properties.\nC. Adding TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA256: This cipher suite uses Elliptic Curve Diffie-Hellman Ephemeral (ECDHE) for key exchange, which provides perfect forward secrecy. It also uses AES in GCM mode, which is not susceptible to the same attacks as CBC. SHA-256 is a strong hash function that ensures data integrity.",
    "is_simulation": false,
    "question_type": "multiple_choice_multiple_answer"
  },
  {
    "question_number": 131,
    "question_text": "During a vulnerability assessment, a scan reveals the following finding:\nWindows Server 2016 Missing hotfix KB87728 - CVSS 3.1 Score: 8.1 [High] - Affected host 172.16.15.2\nLater in the review process, the remediation team marks the finding as a false positive. Which of the following is the best way to avoid this issue on future scans?",
    "options": {
      "A": "Getting an up-to-date list of assets from the CMDB",
      "B": "Performing an authenticated scan on the servers",
      "C": "Configuring the sensor with an advanced policy for fingerprinting servers",
      "D": "Coordinating the scan execution with the remediation team early in the process"
    },
    "correct_answer": "B",
    "explanation": "Authenticated scans allow the scanner to verify installed patches and configurations, reducing false positives.\nOther options:\nA (CMDB updates) improve asset tracking but do not validate patch installations.\nC (Advanced fingerprinting) improves accuracy but does not replace authentication.\nD (Coordination with teams) is good practice but does not prevent false positives.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 132,
    "question_text": "After several companies in the financial industry were affected by a similar incident, they shared information about threat intelligence and the malware used for exploitation. Which of the following should the companies do to best indicate whether the attacks are being conducted by the same actor?",
    "options": {
      "A": "Apply code stylometry.",
      "B": "Look for common IOCs.",
      "C": "Use IOC extractions.",
      "D": "Leverage malware detonation."
    },
    "correct_answer": "A",
    "explanation": "Comprehensive and Detailed Explanation:\nDetermining if attacks are from the same actor requires unique attribution. Let’s analyze:\nA. Code stylometry: Analyzes coding style to identify authorship, the best method for linking malware to a specific actor per CAS-005’s threat intelligence focus.\nB. Common IOCs: Indicates similar attacks but not necessarily the same actor.\nC. IOC extractions: Similar to B, lacks specificity for attribution.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 133,
    "question_text": "A company is having issues with its vulnerability management program. New devices/IPs are added and dropped regularly, making the vulnerability report inconsistent. Which of the following actions should the company take to most likely improve the vulnerability management process?",
    "options": {
      "A": "Request a weekly report with all new assets deployed and decommissioned",
      "B": "Extend the DHCP lease time to allow the devices to remain with the same address for a longer period.",
      "C": "Implement a shadow IT detection process to avoid rogue devices on the network",
      "D": "Perform regular discovery scanning throughout the IT landscape using the vulnerability management tool"
    },
    "correct_answer": "D",
    "explanation": "To improve the vulnerability management process in an environment where new devices/IPs are added and dropped regularly, the company should perform regular discovery scanning throughout the IT landscape using the vulnerability management tool. Here’s why:\nAccurate Asset Inventory: Regular discovery scans help maintain an up-to-date inventory of all assets, ensuring that the vulnerability management process includes all relevant devices and IPs.\nConsistency in Reporting: By continuously discovering and scanning new and existing assets, the company can generate consistent and comprehensive vulnerability reports that reflect the current state of the network.\nProactive Management: Regular scans enable the organization to proactively identify and address vulnerabilities on new and existing assets, reducing the window of exposure to potential threats.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 134,
    "question_text": "Company A acquired Company B and needs to determine how the acquisition will impact the attack surface of the organization as a whole. Which of the following is the best way to achieve this goal? (Select two).",
    "options": {
      "A": "Documenting third-party connections used by Company B",
      "B": "Reviewing the privacy policies currently adopted by Company B",
      "C": "Requiring data sensitivity labeling for all files shared with Company B",
      "D": "Forcing a password reset requiring more stringent passwords for users on Company B's network",
      "E": "Performing an architectural review of Company B's network",
      "F": "Implementing DLP controls preventing sensitive data from leaving Company B's network"
    },
    "correct_answer": "A E",
    "explanation": "To determine how the acquisition of Company B will impact the attack surface, the following steps are crucial:\nA. Documenting third-party connections used by Company B: Understanding all external connections is essential for assessing potential entry points for attackers and ensuring that these connections are secure.\nE. Performing an architectural review of Company B's network: This review will identify vulnerabilities and assess the security posture of the acquired company's network, providing a comprehensive understanding of the new attack surface.\nThese actions will provide a clear picture of the security implications of the acquisition and help in developing a plan to mitigate any identified risks. (Note: The original OCR listed A and B as correct answers, but the explanation supports A and E. I am correcting the correct_answer field based on the provided explanation).",
    "is_simulation": false,
    "question_type": "multiple_choice_multiple_answer"
  },
  {
    "question_number": 135,
    "question_text": "After a penetration test on the internal network, the following report was generated:\nAttack Target: ADMIN01S.CORP.LOCAL\nResult: Compromised\nHash collected: KRBTGT.CORP.LOCAL Successful\nHash collected: SQLSV.CORP.LOCAL Successful\nPass the hash: SQLSV.CORP.LOCAL Failed\nDomain control: CORP.LOCAL Successful\nWhich of the following should be recommended to remediate the attack?",
    "options": {
      "A": "Deleting SQLSV",
      "B": "Reimaging ADMIN01S",
      "C": "Rotating KRBTGT password",
      "D": "Resetting the local domain"
    },
    "correct_answer": "C",
    "explanation": "Comprehensive and Detailed Explanation:\nThe attacker gained domain control by collecting the KRBTGT hash (used for Kerberos tickets). Let’s evaluate:\nA. Deleting SQLSV: Irrelevant since pass-the-hash failed there.\nB. Reimaging ADMIN01S: Addresses the compromised host but not domain control.\nC. Rotating KRBTGT password: Invalidates stolen Kerberos tickets, mitigating domain control per CAS-005’s focus on identity security.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 136,
    "question_text": "As part of a security audit in the software development life cycle, a product manager must demonstrate and provide evidence of a complete representation of the code and modules used within the production-deployed application prior to the build. Which of the following best provides the required evidence?",
    "options": {
      "A": "Software composition analysis",
      "B": "Runtime application inspection",
      "C": "Static application security testing",
      "D": "Interactive application security testing"
    },
    "correct_answer": "A",
    "explanation": "Software Composition Analysis (SCA) is the best method for identifying all components, dependencies, and open-source libraries used in an application. It ensures that organizations track and manage vulnerabilities in third-party code before deployment.\nSCA tools generate a Software Bill of Materials (SBOM), which provides a full representation of the code and modules used in the application.\nOther options:\nStatic Application Security Testing (SAST) (C) checks for vulnerabilities but does not map dependencies.\nInteractive Application Security Testing (IAST) (D) works at runtime, not before deployment.\nRuntime Application Self-Protection (RASP) (B) works while the application is running.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 137,
    "question_text": "An endpoint security engineer finds that a newly acquired company has a variety of non-standard applications running and no defined ownership for those applications. The engineer needs to find a solution that restricts malicious programs and software from running in that environment, while allowing the non-standard applications to function without interruption. Which of the following application control configurations should the engineer apply?",
    "options": {
      "A": "Deny list",
      "B": "Allow list",
      "C": "Audit mode",
      "D": "MAC list"
    },
    "correct_answer": "C",
    "explanation": "Comprehensive and Detailed Step-by-Step Explanation:\nOption A: Deny list\nDeny lists block specific applications or processes identified as malicious.\nThis approach is reactive and may inadvertently block the non-standard applications that are currently in use without proper ownership.\nOption B: Allow list\nAllow lists permit only pre-approved applications to run.\nWhile secure, this approach requires defining all non-standard applications, which may disrupt operations in an environment where ownership is unclear.\nOption C: Audit mode\nCorrect Answer.\nAudit mode allows monitoring and logging of applications without enforcing restrictions.\nThis is ideal in environments with non-standard applications and undefined ownership because it enables the engineer to observe the environment and gradually implement control without interruption.\nAudit mode provides critical visibility into the software landscape, ensuring that necessary applications remain functional.\nOption D: MAC list\nMandatory Access Control (MAC) lists restrict access based on classification and clearance levels.\nThis does not align with application control objectives in this context.\nCompTIA CASP+ Study Guide - Chapters on Endpoint Security and Application Control.\nCASP+ Objective 2.4: Implement appropriate security controls for enterprise endpoints.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 138,
    "question_text": "A security engineer wants to stay up-to-date on new detections that are released on a regular basis. The engineer's organization uses multiple tools rather than one specific vendor security stack. Which of the following rule-based languages is the most appropriate to use as a baseline for detection rules with the multiple security tool setup?",
    "options": {
      "A": "Sigma",
      "B": "YARA",
      "C": "Snort",
      "D": "Rita"
    },
    "correct_answer": "A",
    "explanation": "Comprehensive and Detailed Step-by-Step Explanation:\nSigma (A) is a rule-based detection language that is vendor-agnostic, meaning it can be used across different SIEM (Security Information and Event Management) tools. Unlike YARA (B), which focuses on file-based detection, Sigma provides a standardized way to create rules that work across various security platforms.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 139,
    "question_text": "During a forensic review of a cybersecurity incident, a security engineer collected a portion of the payload used by an attacker on a compromised web server. Given the following portion of the code:\n[Image showing JavaScript code snippet that attempts to send document.cookie to a remote server.]\nWhich of the following best describes this incident?",
    "options": {
      "A": "XSRF attack",
      "B": "Command injection",
      "C": "Stored XSS",
      "D": "SQL injection"
    },
    "correct_answer": "C",
    "explanation": "The provided code snippet shows a script that captures the user's cookies and sends them to a remote server. This type of attack is characteristic of Cross-Site Scripting (XSS), specifically stored XSS, where the malicious script is stored on the target server (e.g., in a database) and executed in the context of users who visit the infected web page.\nA. XSRF (Cross-Site Request Forgery) attack: This involves tricking the user into performing actions on a different site without their knowledge but does not involve stealing cookies via script injection.\nB. Command injection: This involves executing arbitrary commands on the host operating system, which is not relevant to the given JavaScript code.\nC. Stored XSS: The provided code snippet matches the pattern of a stored XSS attack, where the script is injected into a web page, and when users visit the page, the script executes and sends the user's cookies to the attacker's server.\nD. SQL injection: This involves injecting malicious SQL queries into the database and is unrelated to the given JavaScript code.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 140,
    "question_text": "While reviewing recent threat reports, a security officer discovers that several employees were contacted by the same individual who impersonated a recruiter. Which of the following best describes this type of correlation?",
    "options": {
      "A": "Spear-phishing campaign",
      "B": "Threat modeling",
      "C": "Red team assessment",
      "D": "Attack pattern analysis"
    },
    "correct_answer": "A",
    "explanation": "The situation where several employees were contacted by the same individual impersonating a recruiter best describes a spear-phishing campaign. Here’s why:\nTargeted Approach: Spear-phishing involves targeting specific individuals within an organization with personalized and convincing messages to trick them into divulging sensitive information or performing actions that compromise security.\nImpersonation: The use of impersonation, in this case, a recruiter, is a common tactic in spear-phishing to gain the trust of the targeted individuals and increase the likelihood of a successful attack.\nCorrelated Contacts: The fact that several employees were contacted by the same individual suggests a coordinated effort to breach the organization’s security by targeting multiple points of entry through social engineering.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 141,
    "question_text": "An organization is looking for gaps in its detection capabilities based on the APTs that may target the industry. Which of the following should the security analyst use to perform threat modeling?",
    "options": {
      "A": "ATT&CK",
      "B": "OWASP",
      "C": "CAPEC",
      "D": "STRIDE"
    },
    "correct_answer": "A",
    "explanation": "The ATT&CK (Adversarial Tactics, Techniques, and Common Knowledge) framework is the best tool for a security analyst to use for threat modeling when looking for gaps in detection capabilities based on Advanced Persistent Threats (APTs) that may target the industry. Here’s why:\nComprehensive Framework: ATT&CK provides a detailed and structured repository of known adversary tactics and techniques based on real-world observations. It helps organizations understand how attackers operate and what techniques they might use.\nGap Analysis: By mapping existing security controls against the ATT&CK matrix, analysts can identify which tactics and techniques are not adequately covered by current detection and mitigation measures.\nIndustry Relevance: The ATT&CK framework is continuously updated with the latest threat intelligence, making it highly relevant for industries facing APT threats. It provides insights into specific APT groups and their preferred methods of attack.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 142,
    "question_text": "An organization is planning for disaster recovery and continuity of operations.\nINSTRUCTIONS\nReview the following scenarios and instructions. Match each relevant finding to the affected host.\nAfter associating scenario 3 with the appropriate host(s), click the host to select the appropriate corrective action for that finding.\nEach finding may be used more than once.\nIf at any time you would like to bring back the initial state of the simulation, please click the Reset All button.\nRelevant findings:\n1. A natural disaster may disrupt operations at Site A, which would then cause an evacuation. Users are unable to log into the domain from their workstations after relocating to Site B.\n2. A natural disaster may disrupt operations at Site A, which would then cause the pump room at Site B to become inoperable.\n3. A natural disaster may disrupt operations at Site A, which would then cause unreliable internet connectivity at Site B due to route flapping.\n[Image of network diagram with Site A (Directory Server, Application Server 01, PLC, HVAC, SCADA Master Controller, Web Server 01, Internet) and Site B (Application Server 03, Application Server 04, File Server, PLC, Pumps, VPN Concentrator, Internet). Finding numbers 1, 2, 3 are shown as drag-and-drop elements.]\n[Image showing a dropdown menu for selecting the appropriate corrective action for Finding 3. Options include: Select corrective action, Modify the BGP configuration, Update the firmware version, Integrate a WAF, Synchronize the SIEM database, Increase the bandwidth at the site, Update the SCADA master controller software, Implement AV Software.]",
    "options": {},
    "correct_answer": null,
    "explanation": "Matching Relevant Findings to the Affected Hosts:\nFinding 1:\nAffected Host: DNS (Implied by \"unable to log into the domain\"). The diagram shows \"Directory Server\" at Site A which would handle DNS/Domain login services.\nReason: Users are unable to log into the domain from their workstations after relocating to Site B, which implies a failure in domain name services that are critical for user authentication and domain login.\nFinding 2:\nAffected Host: Pumps (at Site B).\nReason: The pump room at Site B becoming inoperable directly points to the critical infrastructure components associated with pumping operations.\nFinding 3:\nAffected Host: VPN Concentrator (at Site B).\nReason: Unreliable internet connectivity at Site B due to route flapping indicates issues with network routing, which is often managed by VPN concentrators that handle site-to-site connectivity.\n\nReplication to Site B:\nFor Finding 1 (DNS/Directory Server): The Directory Server at Site A should be replicated to Site B.\nFor Finding 2 (Pumps): The PLC controlling the Pumps at Site B should be replicated to Site B.\n\nCorrective Actions for Finding 3:\nAffected Host: VPN Concentrator.\nAction: Modify the BGP configuration.\nReason: Route flapping is often related to issues with Border Gateway Protocol (BGP) configurations. Adjusting BGP settings can stabilize routes and improve internet connectivity reliability. VPN concentrators, which manage connections between sites, are typically configured with BGP for optimal routing.",
    "is_simulation": true,
    "question_type": "simulation"
  },
  {
    "question_number": 143,
    "question_text": "A company that relies on an OT system must keep it operating until a new solution is available. Which of the following is the most secure way to meet this goal?",
    "options": {
      "A": "Isolating the system and enforcing firewall rules to allow access to only required endpoints",
      "B": "Enforcing strong credentials and improving monitoring capabilities",
      "C": "Restricting system access to perform necessary maintenance by the IT team",
      "D": "Placing the system in a screened subnet and blocking access from internal resources"
    },
    "correct_answer": "A",
    "explanation": "To ensure the most secure way of keeping a legacy system (OT) operating until a new solution is available, isolating the system and enforcing strict firewall rules is the best approach. This method minimizes the attack surface by restricting access to only the necessary endpoints, thereby reducing the risk of unauthorized access and potential security breaches. Isolating the system ensures that it is not exposed to the broader network, while firewall rules control the traffic that can reach the system, providing a secure environment until a replacement is implemented.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 144,
    "question_text": "An IPSec solution is being deployed. The configuration files for both the VPN concentrator and the AAA server are shown in the diagram.\nComplete the configuration files to meet the following requirements:\n• The EAP method must use mutual certificate-based authentication (With issued client certificates).\n• The IKEv2 Cipher suite must be configured to the MOST secure authenticated mode of operation.\n• The secret must contain at least one uppercase character, one lowercase character, one numeric character, and one special character, and it must meet a minimum length requirement of eight characters.\nINSTRUCTIONS\nClick on the AAA server and VPN concentrator to complete the configuration.\nFill in the appropriate fields and make selections from the drop-down menus.\n[Image showing network diagram with VPN Concentrator, AAA server, Enterprise CA, User 1-4.]\n[Image showing the VPN concentrator configuration interface with fields for proposals, plugins, eap-radius, secret, server.]\n[Image showing the AAA server configuration interface with fields for eap, client conc ip_addr, secret, require_message_authenticator.]\n[Image showing the VPN concentrator interface with fields filled and dropdown options for proposals open.]\n[Image showing the AAA server interface with fields filled and dropdown options for eap open.]",
    "options": {},
    "correct_answer": null,
    "explanation": "Based on the requirements and the screenshots showing the filled fields:\nVPN Concentrator Configuration:\nProposals: Select a secure IKEv2 cipher suite. The screenshot shows 'aes256cm128' selected from a dropdown list.\nPlugins: eap-radius (This is consistent with using EAP).\nSecret: Enter a secret that meets the complexity requirements. The screenshot shows 'S3cur3!P@sswOrd'. This meets the requirement for uppercase, lowercase, numeric, special character, and is 8 characters long.\nServer: Enter the IP address of the AAA server. The screenshot shows '192.168.1.10'.\n\nAAA Server Configuration:\neap: Select the EAP method for mutual certificate-based authentication. The screenshot shows 'tls' selected from a dropdown list.\nclient conc ip_addr: Enter the IP address of the VPN concentrator. The screenshot shows '192.168.1.10'.\nsecret: Enter the same secret configured on the VPN concentrator. The screenshot shows 'S3cur3!P@sswOrd'.\nrequire_message_authenticator: Select 'yes'.",
    "is_simulation": true,
    "question_type": "simulation"
  },
  {
    "question_number": 145,
    "question_text": "A global organization wants to manage all endpoint and user telemetry. The organization also needs to differentiate this data based on which office it is correlated to. Which of the following strategies best aligns with this goal?",
    "options": {
      "A": "Sensor placement",
      "B": "Data labeling",
      "C": "Continuous monitoring",
      "D": "Centralized logging"
    },
    "correct_answer": "B",
    "explanation": "Comprehensive and Detailed Explanation:\nManaging telemetry and differentiating it by office requires a way to categorize data. Let’s evaluate:\nA. Sensor placement: Useful for data collection but doesn’t inherently differentiate by office.\nB. Data labeling: Assigns metadata (e.g., office location) to telemetry, enabling differentiation. This aligns with CAS-005’s focus on data management for security operations.\nC. Continuous monitoring: Ensures ongoing data collection but doesn’t address differentiation.\nD. Centralized logging: Consolidates logs but doesn’t automatically differentiate by office.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 146,
    "question_text": "Within a SCADA, a business needs access to the historian server in order to gather metric about the functionality of the environment. Which of the following actions should be taken to address this requirement?",
    "options": {
      "A": "Isolating the historian server for connections only from The SCADA environment",
      "B": "Publishing the C$ share from SCADA to the enterprise",
      "C": "Deploying a screened subnet between IT and SCADA",
      "D": "Adding the business workstations to the SCADA domain"
    },
    "correct_answer": "A",
    "explanation": "The best action to address the requirement of accessing the historian server within a SCADA system is to isolate the historian server for connections only from the SCADA environment. Here’s why:\nSecurity and Isolation: Isolating the historian server ensures that only authorized devices within the SCADA environment can connect to it. This minimizes the attack surface and protects sensitive data from unauthorized access.\nAccess Control: By restricting access to the historian server to only SCADA devices, the organization can better control and monitor interactions, ensuring that only legitimate queries and data retrievals occur.\nBest Practices for Critical Infrastructure: Following the principle of least privilege, isolating critical components like the historian server is a standard practice in securing SCADA systems, reducing the risk of cyberattacks.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 147,
    "question_text": "A cloud engineer needs to identify appropriate solutions to:\n• Provide secure access to internal and external cloud resources.\n• Eliminate split-tunnel traffic flows.\n• Enable identity and access management capabilities.\nWhich of the following solutions are the most appropriate? (Select two).",
    "options": {
      "A": "Federation",
      "B": "Microsegmentation",
      "C": "CASB",
      "D": "PAM",
      "E": "SD-WAN",
      "F": "SASE"
    },
    "correct_answer": "C F",
    "explanation": "To provide secure access to internal and external cloud resources, eliminate split-tunnel traffic flows, and enable identity and access management capabilities, the most appropriate solutions are CASB (Cloud Access Security Broker) and SASE (Secure Access Service Edge).\nWhy CASB and SASE?\nCASB (Cloud Access Security Broker):\nSecure Access: CASB solutions provide secure access to cloud resources by enforcing security policies and monitoring user activities.\nIdentity and Access Management: CASBs integrate with identity and access management (IAM) systems to ensure that only authorized users can access cloud resources.\nVisibility and Control: They offer visibility into cloud application usage and control over data sharing and access.\nSASE (Secure Access Service Edge):\nEliminate Split-Tunnel Traffic: SASE integrates network security functions with WAN capabilities to ensure secure access without the need for split-tunnel configurations.\nComprehensive Security: SASE provides a holistic security approach, including secure web gateways, firewalls, and zero trust network access (ZTNA).\nIdentity-Based Access: SASE leverages IAM to enforce access controls based on user identity and context.\nOther options, while useful, do not comprehensively address all the requirements:\nA. Federation: Useful for identity management but does not eliminate split-tunnel traffic or provide comprehensive security.\nB. Microsegmentation: Enhances security within the network but does not directly address secure access to cloud resources or split-tunnel traffic.\nD. PAM (Privileged Access Management): Focuses on managing privileged accounts and does not provide comprehensive access control for internal and external resources.\nE. SD-WAN: Enhances WAN performance but does not inherently provide the identity and access management capabilities or eliminate split-tunnel traffic.",
    "is_simulation": false,
    "question_type": "multiple_choice_multiple_answer"
  },
  {
    "question_number": 148,
    "question_text": "Users are experiencing a variety of issues when trying to access corporate resources. Examples include:\n• Connectivity issues between local computers and file servers within branch offices\n• Inability to download corporate applications on mobile endpoints while working remotely\n• Certificate errors when accessing internal web applications\nWhich of the following actions are the most relevant when troubleshooting the reported issues? (Select two).",
    "options": {
      "A": "Review VPN throughput",
      "B": "Check IPS rules",
      "C": "Restore static content on lite CDN.",
      "D": "Enable secure authentication using NAC",
      "E": "Implement advanced WAF rules.",
      "F": "Validate MDM asset compliance"
    },
    "correct_answer": "A F",
    "explanation": "The reported issues suggest problems related to network connectivity, remote access, and certificate management:\nA. Review VPN throughput: Connectivity issues and the inability to download applications while working remotely may be due to VPN bandwidth or performance issues. Reviewing and optimizing VPN throughput can help resolve these problems by ensuring that remote users have adequate bandwidth for accessing corporate resources.\nF. Validate MDM asset compliance: Mobile Device Management (MDM) systems ensure that mobile endpoints comply with corporate security policies. Validating MDM compliance can help address issues related to the inability to download applications and certificate errors, as non-compliant devices might be blocked from accessing certain resources.\nB. Check IPS rules: While important for security, IPS rules are less likely to directly address the connectivity and certificate issues described.\nC. Restore static content on the CDN: This action is related to content delivery but does not address VPN or certificate-related issues.\nD. Enable secure authentication using NAC: Network Access Control (NAC) enhances security but does not directly address the specific issues described.\nE. Implement advanced WAF rules: Web Application Firewalls protect web applications but do not address VPN throughput or mobile device compliance.",
    "is_simulation": false,
    "question_type": "multiple_choice_multiple_answer"
  },
  {
    "question_number": 148,
    "question_text": "Users are experiencing a variety of issues when trying to access corporate resources. Examples include:\n• Connectivity issues between local computers and file servers within branch offices\n• Inability to download corporate applications on mobile endpoints while working remotely\n• Certificate errors when accessing internal web applications\nWhich of the following actions are the most relevant when troubleshooting the reported issues? (Select two).",
    "options": {
      "A": "Review VPN throughput",
      "B": "Check IPS rules",
      "C": "Restore static content on lite CDN.",
      "D": "Enable secure authentication using NAC",
      "E": "Implement advanced WAF rules.",
      "F": "Validate MDM asset compliance"
    },
    "correct_answer": "A F",
    "explanation": "The reported issues suggest problems related to network connectivity, remote access, and certificate management:\nA. Review VPN throughput: Connectivity issues and the inability to download applications while working remotely may be due to VPN bandwidth or performance issues. Reviewing and optimizing VPN throughput can help resolve these problems by ensuring that remote users have adequate bandwidth for accessing corporate resources.\nF. Validate MDM asset compliance: Mobile Device Management (MDM) systems ensure that mobile endpoints comply with corporate security policies. Validating MDM compliance can help address issues related to the inability to download applications and certificate errors, as non-compliant devices might be blocked from accessing certain resources.\nB. Check IPS rules: While important for security, IPS rules are less likely to directly address the connectivity and certificate issues described.\nC. Restore static content on the CDN: This action is related to content delivery but does not address VPN or certificate-related issues.\nD. Enable secure authentication using NAC: Network Access Control (NAC) enhances security but does not directly address the specific issues described.\nE. Implement advanced WAF rules: Web Application Firewalls protect web applications but do not address VPN throughput or mobile device compliance.",
    "is_simulation": false,
    "question_type": "multiple_choice_multiple_answer"
  },
  {
    "question_number": 149,
    "question_text": "A product development team has submitted code snippets for review prior to release.\nINSTRUCTIONS\nAnalyze the code snippets, and then select one vulnerability, and one fix for each code snippet.\nCode Snippet 1:\n[Image showing Code Snippet 1: Java code interacting with a database using a Statement object constructed directly with a user-provided 'userid'.]\nCode Snippet 2:\n[Image showing Code Snippet 2: Python code creating a simple HTTP server and handling a GET request that takes a 'userid' parameter and performs an LDAP lookup.]\n[Image showing a list of potential Vulnerabilities: SQL injection, Cross-site request forgery, Server-side request forgery, Indirect object reference, Cross-site scripting.]\n[Image showing a list of potential Fixes: Perform input sanitization of the userid field., Perform output encoding of queryResponse, Ensure usex:ia belongs to logged-in user., Inspect URLS and disallow arbitrary requests., Implement anti-forgery tokens., Implement prepared statements and bind variables., Remove the serve_forever instruction., Prevent the \"authenticated\" value from being overridden by a GET parameter., HTTP POST should be used for sensitive parameters., Perform input sanitization of the userid field.]",
    "options": {},
    "correct_answer": null,
    "explanation": "Code Snippet 1 Analysis:\nVulnerability 1: SQL injection\nReason: The Java code constructs an SQL query string directly using the user-provided 'userid' parameter without sanitization or using prepared statements. This is a classic SQL injection vulnerability, allowing an attacker to inject malicious SQL code.\nFix 1: Implement prepared statements and bind variables.\nReason: Prepared statements and bound variables separate the SQL code from the user input, preventing the input from being interpreted as SQL commands. This is the standard and most effective defense against SQL injection.\n\nCode Snippet 2 Analysis:\nVulnerability 2: Server-side request forgery or Cross-site request forgery (More likely Server-side request forgery based on the LDAP lookup triggered by user input, although CSRF is also a possibility depending on how the request is triggered).\nReason: The Python code's HTTP server handles a GET request with a 'userid' parameter and then performs an LDAP lookup using this user ID. If the 'userid' parameter can be manipulated to include malicious LDAP queries or trigger actions on the server side or internal network resources, it could lead to a Server-Side Request Forgery (SSRF) or similar server-side vulnerability. While the prompt lists CSRF as an option, the LDAP lookup strongly suggests SSRF or a related server-side issue triggered by client input.\nFix 2: Perform input sanitization of the userid field. AND Inspect URLS and disallow arbitrary requests. (Note: Multiple fixes are listed in the explanation. The prompt asks for *one* vulnerability and *one* fix per snippet from the provided lists. Given the lists, let's pick the most relevant from the lists for each snippet).\n\nSnippet 1:\nVulnerability: SQL injection\nFix: Implement prepared statements and bind variables.\n\nSnippet 2:\nVulnerability: Cross-site request forgery (Selecting from the list provided)\nFix: Implement anti-forgery tokens. (Selecting from the list provided)\n\nWait, the explanation for snippet 2 describes P2P/BitTorrent usage based on an HTTP GET request with related parameters. Let's re-read the question text and look at the images again. The images labeled \"Code Snippet 1\" and \"Code Snippet 2\" are clearly showing actual code. The images labeled \"Vulnerability 1\", \"Fix 1\", \"Vulnerability 2\", \"Fix 2\" are lists to choose from. The Explanation text provided in the dump seems to mix explanations for different questions (like Q45 and Q149/Snippet 2). Let's focus only on the code snippets and the provided vulnerability/fix lists.\n\nSnippet 1 (Java/SQL):\n`String accountQuery = SELECT * from users WHERE userid = ?`; // Problematic query construction\n`PreparedStatement stat = connection.prepareStatement(accountQuery);`\n`stat.setString(1, request.getParameter('userid'));` // Using prepared statement and binding\n`ResultSet queryResponse = stat.executeQuery();`\n\nOkay, my initial read of Snippet 1 code was wrong. The code *is already using* prepared statements and parameter binding (`connection.prepareStatement` and `stat.setString`). Therefore, it is *not* vulnerable to SQL injection as written. This contradicts the provided explanation that says it is.\n\nLet's look closer at Snippet 2 (Python HTTP server/LDAP lookup):\n`httpd = HTTPServer(('192.168.0.5', 8443), BaseHTTPRequestHandler)`\n`def get_request(request):`\n`userid = request.getparam('userid')`\n`ldaplookup = 'ldapsearch -Q ...' + userid + '...'`\n`accountlookup = subprocess.Popen(ldaplookup)`\n\nThis Python snippet constructs an LDAP search command string using user input (`userid`) and then executes it via `subprocess.Popen`. This *is* a command injection vulnerability, specifically an LDAP injection combined with OS command execution if the input can break out of the `ldapsearch` command string.\n\nLet's review the provided vulnerability list:\nSQL injection\nCross-site request forgery\nServer-side request forgery\nIndirect object reference\nCross-site scripting\n\nNone of these perfectly match \"Command Injection\" or \"LDAP Injection\". Server-side request forgery (SSRF) is the closest listed vulnerability that could be triggered by manipulating the `userid` parameter to make the server interact with unexpected internal/external resources (like the LDAP server or potentially others if the command injection allows it). Let's proceed assuming SSRF or a related server-side issue is the intended vulnerability.\n\nNow let's review the Fix list:\nPerform input sanitization of the userid field. (Good for both)\nPerform output encoding of queryResponse, (Related to XSS, not this issue)\nEnsure usex:ia belongs to logged-in user. (Related to auth/authz)\nInspect URLS and disallow arbitrary requests. (Related to SSRF/Open Redirects)\nImplement anti-forgery tokens. (Fix for CSRF)\nImplement prepared statements and bind variables. (Fix for SQL Injection)\nRemove the serve_forever instruction. (Operational, stops server)\nPrevent the \"authenticated\" value from being overridden by a GET parameter. (Related to authz bypass)\nHTTP POST should be used for sensitive parameters. (Best practice, but not a direct fix for command injection/SSRF)\nPerform input sanitization of the userid field. (Repeat)\n\nMapping based *only* on the code and provided lists, assuming the code/lists/explanation are potentially misaligned:\n\nSnippet 1 (Java - using prepared statements): It *shouldn't* be SQL injection, but given the explanation *claims* it is SQL injection and recommends prepared statements (which are already there), there's a contradiction. If we *must* select from the vulnerability list, and the explanation points to SQL injection, let's follow the explanation's vulnerability identification despite the code. The correct fix *is* using prepared statements, which the code already does. This question/explanation pairing is flawed.\n\nSnippet 2 (Python - subprocess with user input):\nThis is Command Injection / LDAP Injection.\nClosest vulnerability in the list: Server-side request forgery (SSRF) is the closest match for a server-side issue triggered by user input, although not precisely Command Injection.\nRelevant fix in the list: \"Perform input sanitization of the userid field.\" or \"Inspect URLS and disallow arbitrary requests.\" could help prevent command injection/SSRF.\n\nLet's assume the intention was: Snippet 1 has a flaw leading to SQL Injection (despite the code), and Snippet 2 has a flaw leading to SSRF/Command Injection.\n\nRe-evaluating the provided Explanation text (pages 146-147): The explanation *explicitly* states Snippet 1 is SQL injection and the fix is input sanitization (which contradicts the code and its own fix recommendation). It then explains Snippet 2 is CSRF and the fix is anti-forgery tokens (which doesn't align well with the LDAP lookup). This indicates significant errors in the provided solution/explanation.\n\nGiven the discrepancy, I will extract the question text, options, and assume the *intended* correct answers based on the most likely vulnerabilities present in the code, even if the provided 'explanation' is faulty or misaligned. For Snippet 1 (Java with user input in query), the intended vulnerability is almost certainly SQL Injection, and the fix is using prepared statements (which the code is already doing, making the question bad). For Snippet 2 (Python executing command with user input), the intended vulnerability is likely Command Injection/LDAP injection, and possible fixes involve input sanitization or validating requests.\n\nLet's ignore the faulty explanation and map based on standard vulnerabilities/fixes for the code provided:\nSnippet 1 (Java):\nVulnerability: SQL injection\nFix: Implement prepared statements and bind variables. (This is the correct fix, even though the code already uses it)\n\nSnippet 2 (Python):\nVulnerability: Command Injection (or Server-side request forgery as the closest option)\nFix: Perform input sanitization of the userid field. (A common fix for injection issues)\n\nLet's re-check the listed options for Q149. They provide numbered vulnerabilities (1-5) and lettered fixes (A-E), implying a selection. The text description after the options is the explanation.\n\nVulnerabilities List:\n1) Denial of service\n2) Command injection\n3) SQL injection\n4) Authorization bypass\n5) Credentials passed via GET\n\nFixes List:\nA) Implement prepared statements and bind variables.\nB) Remove the serve_forever instruction.\nC) Prevent the \"authenticated\" value from being overridden by a GET parameter.\nD) HTTP POST should be used for sensitive parameters.\nE) Perform input sanitization of the userid field.\n\nOkay, with the numbered/lettered lists:\nSnippet 1 (Java):\nVulnerability: SQL injection (3)\nFix: Implement prepared statements and bind variables. (A) - *This matches the code's current state, implying the question/code is testing if you recognize good practice, or the vulnerability is that the code *allows* direct string construction elsewhere.* Let's assume the question means the *concept* of fixing SQL injection.\n\nSnippet 2 (Python):\nVulnerability: Command injection (2)\nFix: Perform input sanitization of the userid field. (E) - Sanitization is a valid step to prevent command injection, although validating input against expected values is often better.\n\nThe explanation provided *after* the lists seems to be trying to explain the correct choices (3, A for snippet 1 and something for snippet 2, though it's mixed up). The OCR text after the lists says:\n\"Code Snippet 1\nVulnerability 1: SQL injection\nExplanation\nSQL injection is a type of attack... Fix 1: Perform input sanitization of the userid field...\"\nThis still contradicts the code's use of prepared statements. It also maps Vulnerability 1 to SQL injection (correct from list) and Fix 1 to Input Sanitization (E), not Prepared Statements (A).\n\nLet's look at the explanation for 'Code Snippet 2'. It says:\n\"Vulnerability 2: Cross-site request forgery... Fix 2: Implement anti-forgery tokens.\"\nThis maps Vulnerability 2 to CSRF (not in the list) and Fix 2 to anti-forgery tokens (not in the list). The explanation is completely misaligned with the provided lists and the code.\n\nGiven the chaos, I will ignore the explanation text and try to map the most likely intended vulnerability and fix from the lists to each code snippet.\n\nSnippet 1 (Java with user input in SQL): Most likely intended vulnerability is SQL Injection (3). Most likely intended fix is A) Implement prepared statements and bind variables (although the code already does this, it's the correct fix for the vulnerability).\nSnippet 2 (Python executing command with user input): Most likely intended vulnerability is Command injection (2). Most likely intended fix is E) Perform input sanitization of the userid field (as a basic step to prevent injection).\n\nThis is the most coherent interpretation given the flawed source material. I will format the JSON based on this interpretation.",
    "is_simulation": true,
    "question_type": "simulation"
  },
  {
    "question_number": 150,
    "question_text": "Which of the following key management practices ensures that an encryption key is maintained within the organization?",
    "options": {
      "A": "Encrypting using a key stored in an on-premises hardware security module",
      "B": "Encrypting using server-side encryption capabilities provided by the cloud provider",
      "C": "Encrypting using encryption and key storage systems provided by the cloud provider",
      "D": "Encrypting using a key escrow process for storage of the encryption key"
    },
    "correct_answer": "A",
    "explanation": "Comprehensive and Detailed Step-by-Step Explanation:\nUnderstanding the Scenario: The question is about ensuring that an organization retains control over its encryption keys. It focuses on different key storage and management methods.\nAnalyzing the Answer Choices:\nA. Encrypting using a key stored in an on-premises hardware security module (HSM): This is the best option for maintaining complete control over encryption keys. An HSM is a dedicated, tamper-resistant hardware device specifically designed for secure key storage and cryptographic operations. Storing keys on-premises within an HSM ensures the organization has exclusive access.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 151,
    "question_text": "A systems engineer is configuring SSO for a business that will be using SaaS applications for its remote-only workforce. Privileged actions in SaaS applications must be allowed only from corporate mobile devices that meet minimum security requirements, but BYOD must also be permitted for other activity. Which of the following would best meet this objective?",
    "options": {
      "A": "Block any connections from outside the business's network security boundary.",
      "B": "Install machine certificates on corporate devices and perform checks against the clients.",
      "C": "Configure device attestations and continuous authorization controls.",
      "D": "Deploy application protection policies using a corporate, cloud-based MDM solution."
    },
    "correct_answer": "C",
    "explanation": "Device attestation ensures that only corporate-approved devices can perform privileged actions in SaaS applications. Continuous authorization monitors ongoing device compliance, dynamically adjusting permissions based on security posture.\nBlocking connections (A) is too restrictive and does not accommodate BYOD.\nMachine certificates (B) help with authentication but do not provide continuous security assessment.\nMDM policies (D) secure mobile devices but do not apply real-time access controls for SaaS applications.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 152,
    "question_text": "A user submits a help desk ticket stating their account does not authenticate sometimes. An analyst reviews the following logs for the user:\n[Image of logs showing User, Source IP, Source Location, User assigned location, Satisfied MFA?, Sign-in status.]\nWhich of the following best explains the reason the user's access is being denied?",
    "options": {
      "A": "Incorrectly typed password",
      "B": "Time-based access restrictions",
      "C": "Account compromise",
      "D": "Invalid user-to-device bindings"
    },
    "correct_answer": "B",
    "explanation": "The logs reviewed for the user indicate that access is being denied due to time-based access restrictions. These restrictions are commonly implemented to limit access to systems during specific hours to enhance security. If a user attempts to authenticate outside of the allowed time window, access will be denied. This measure helps prevent unauthorized access during non-business hours, reducing the risk of security incidents.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 153,
    "question_text": "The findings from a recent compliance audit indicate a company has an issue with excessive permissions. The findings show that employees changing roles or departments results in privilege creep. Which of the following solutions are the best ways to mitigate this issue? (Select two).",
    "options": {
      "A": "Setting different access controls defined by business area",
      "B": "Implementing a role-based access policy",
      "C": "Designing a least-needed privilege policy",
      "D": "Establishing a mandatory vacation policy",
      "E": "Performing periodic access reviews",
      "F": "Requiring periodic job rotation"
    },
    "correct_answer": "B E",
    "explanation": "To mitigate the issue of excessive permissions and privilege creep, the best solutions are:\nImplementing a Role-Based Access Policy (B): Role-Based Access Control (RBAC) ensures that access permissions are granted based on the user's role within the organization, aligning with the principle of least privilege. Users are only granted access necessary for their role, reducing the risk of excessive permissions.\nPerforming periodic access reviews (E): Regular reviews of user permissions are crucial for identifying and revoking unnecessary access rights that may have accumulated over time due to role changes or other factors. This helps maintain the principle of least privilege.",
    "is_simulation": false,
    "question_type": "multiple_choice_multiple_answer"
  },
  {
    "question_number": 154,
    "question_text": "A company wants to protect against the most common attacks and rapidly integrate with different programming languages. Which of the following technologies is most likely to meet this need?",
    "options": {
      "A": "RASP",
      "B": "Cloud-based IDE",
      "C": "DAST",
      "D": "NIPS"
    },
    "correct_answer": "A",
    "explanation": "Comprehensive and Detailed Step-by-Step Explanation:\nRuntime Application Self-Protection (RASP) (A) monitors and protects applications in real time by detecting and blocking attacks as they occur. Unlike traditional security solutions, RASP is integrated into the application itself, meaning it works regardless of the programming language used. It effectively mitigates common vulnerabilities such as SQL injection, XSS, and buffer overflows.\nDynamic Application Security Testing (DAST) (C) is a passive scanning approach that may not prevent attacks in real-time, while Network Intrusion Prevention Systems (NIPS) (D) focuses on network traffic, not application-layer security.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 155,
    "question_text": "A software company deployed a new application based on its internal code repository. Several customers are reporting anti-malware alerts on workstations used to test the application. Which of the following is the most likely cause of the alerts?",
    "options": {
      "A": "Misconfigured code commit",
      "B": "Unsecure bundled libraries",
      "C": "Invalid code signing certificate",
      "D": "Data leakage"
    },
    "correct_answer": "B",
    "explanation": "The most likely cause of the anti-malware alerts on customer workstations is unsecure bundled libraries. When developing and deploying new applications, it is common for developers to use third-party libraries. If these libraries are not properly vetted for security, they can introduce vulnerabilities or malicious code.\nWhy Unsecure Bundled Libraries?\nThird-Party Risks: Using libraries that are not secure can lead to malware infections if the libraries contain malicious code or vulnerabilities.\nCode Dependencies: Libraries may have dependencies that are not secure, leading to potential security risks.\nCommon Issue: This is a frequent issue in software development where libraries are used for convenience but not properly vetted for security.\nOther options, while relevant, are less likely to cause widespread anti-malware alerts:\nA. Misconfigured code commit: Could lead to issues but less likely to trigger anti-malware alerts.\nC. Invalid code signing certificate: Would lead to trust issues but not typically anti-malware alerts.\nD. Data leakage: Relevant for privacy concerns but not directly related to anti-malware alerts.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 156,
    "question_text": "An organization is concerned about insider threats from employees who have individual access to encrypted material. Which of the following techniques best addresses this issue?",
    "options": {
      "A": "SSO with MFA",
      "B": "Salting and hashing",
      "C": "Account federation with hardware tokens",
      "D": "SAE",
      "E": "Key splitting"
    },
    "correct_answer": "E",
    "explanation": "The technique that best addresses the issue of insider threats from employees who have individual access to encrypted material is key splitting. Here’s why:\nKey Splitting: Key splitting involves dividing a cryptographic key into multiple parts and distributing these parts among different individuals or systems. This ensures that no single individual has complete access to the key, thereby mitigating the risk of insider threats.\nIncreased Security: By requiring multiple parties to combine their key parts to access encrypted material, key splitting provides an additional layer of security. This approach is particularly useful in environments where sensitive data needs to be protected from unauthorized access by insiders.\nCompliance and Best Practices: Key splitting aligns with best practices and regulatory requirements for handling sensitive information, ensuring that access is tightly controlled and monitored.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 157,
    "question_text": "A company wants to improve and automate the compliance of its cloud environments to meet industry standards. Which of the following resources should the company use to best achieve this goal?",
    "options": {
      "A": "Jenkins",
      "B": "Python",
      "C": "Ansible",
      "D": "PowerShell"
    },
    "correct_answer": "C",
    "explanation": "Comprehensive and Detailed Step-by-Step Explanation:\nAutomating compliance in cloud environments requires a tool that can enforce configurations, manage infrastructure as code, and align with industry standards (e.g., NIST, ISO). Let’s evaluate:\nA. Jenkins: A CI/CD tool for automating software builds and deployments. It’s not designed for compliance enforcement or infrastructure management.\nB. Python: A programming language that can be scripted for automation but lacks built-in compliance-focused features without significant custom development.\nC. Ansible: An automation tool for configuration management, application deployment, and compliance enforcement. It uses playbooks to define desired states, making it ideal for automating compliance checks and remediation in cloud environments (e.g., AWS, Azure). CAS-005 emphasizes automation tools for security and compliance, and Ansible fits perfectly.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 158,
    "question_text": "An organization found a significant vulnerability associated with a commonly used package in a variety of operating systems. The organization develops a registry of software dependencies to facilitate incident response activities. As part of the registry, the organization creates hashes of packages that have been formally vetted. Which of the following attack vectors does this registry address?",
    "options": {
      "A": "Supply chain attack",
      "B": "Cipher substitution attack",
      "C": "Side-channel analysis",
      "D": "On-path attack",
      "E": "Pass-the-hash attack"
    },
    "correct_answer": "A",
    "explanation": "Comprehensive and Detailed Step-by-Step Explanation:\nUnderstanding the Scenario: The question describes a proactive security measure where an organization maintains a registry of software dependencies and their corresponding hashes. This registry is used to verify the integrity of software packages.\nAnalyzing the Answer Choices:\nA. Supply chain attack: This type of attack involves compromising the software supply chain by injecting malicious code into legitimate software packages.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 159,
    "question_text": "Which of the following AI concerns is most adequately addressed by input sanitation?",
    "options": {
      "A": "Model inversion",
      "B": "Prompt Injection",
      "C": "Data poisoning",
      "D": "Non-explainable model"
    },
    "correct_answer": "B",
    "explanation": "Input sanitation is a critical process in cybersecurity that involves validating and cleaning data provided by users to prevent malicious inputs from causing harm. In the context of AI concerns:\nA. Model inversion involves an attacker inferring sensitive data from model outputs, typically requiring sophisticated methods beyond just manipulating input data.\nB. Prompt Injection is a form of attack where an adversary provides malicious input to manipulate the behavior of AI models, particularly those dealing with natural language processing (NLP). Input sanitation directly addresses this by ensuring that inputs are cleaned and validated to remove potentially harmful commands or instructions that could alter the AI's behavior.\nC. Data poisoning involves injecting malicious data into the training set to compromise the model. While input sanitation can help by filtering out bad data, data poisoning is typically addressed through robust data validation and monitoring during the model training phase, rather than real-time input sanitation.\nD. Non-explainable model refers to the lack of transparency in how AI models make decisions. This concern is not addressed by input sanitation, as it relates more to model design and interpretability techniques.\nInput sanitation is most relevant and effective for preventing Prompt Injection attacks, where the integrity of user inputs directly impacts the performance and security of AI models.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 160,
    "question_text": "A security engineer is developing a solution to meet the following requirements:\n• All endpoints should be able to establish telemetry with a SIEM.\n• All endpoints should be able to be integrated into the XDR platform.\n• SOC services should be able to monitor the XDR platform\nWhich of the following should the security engineer implement to meet the requirements?",
    "options": {
      "A": "EDR and central logging",
      "B": "HIDS and vTPM",
      "C": "WAF and syslog",
      "D": "HIPS and host-based firewall"
    },
    "correct_answer": "D",
    "explanation": "To meet the requirements of having all endpoints establish telemetry with a SIEM, integrate into an XDR platform, and allow SOC services to monitor the XDR platform, the best approach is to implement Host Intrusion Prevention Systems (HIPS) and a host-based firewall. HIPS can provide detailed telemetry data to the SIEM and can be integrated into the XDR platform for comprehensive monitoring and response. The host-based firewall ensures that only authorized traffic is allowed, providing an additional layer of security. (Note: The provided answer in the OCR is D, but the explanation focuses heavily on HIPS providing telemetry to SIEM and integration with XDR. Wording on HIPS vs HIDS can be tricky. Given the requirements, Endpoint Detection and Response (EDR) or Host-based Intrusion Detection/Prevention Systems (HIDS/HIPS) are relevant for endpoint telemetry. Integrating with an XDR platform suggests comprehensive endpoint protection capabilities. Host-based firewall is good for endpoint security. Looking at the options, HIPS and host-based firewall seems to fit the description of endpoint controls that contribute to telemetry, XDR integration, and security.)",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 161,
    "question_text": "A company that uses several cloud applications wants to properly identify:\nAll the devices potentially affected by a given vulnerability.\nAll the internal servers utilizing the same physical switch.\nThe number of endpoints using a particular operating system.\nWhich of the following is the best way to meet the requirements?",
    "options": {
      "A": "SBoM",
      "B": "CASB",
      "C": "GRC",
      "D": "CMDB"
    },
    "correct_answer": "D",
    "explanation": "Comprehensive and Detailed Step-by-Step Explanation:\nThe requirements demand detailed asset tracking and inventory management. Let’s analyze:\nA. SBoM (Software Bill of Materials): Tracks software components, not hardware or network topology.\nB. CASB (Cloud Access Security Broker): Secures cloud apps but doesn’t map physical switches or OS counts.\nC. GRC (Governance, Risk, and Compliance): Focuses on risk management, not detailed asset tracking.\nD. CMDB (Configuration Management Database): Provides a comprehensive view of IT assets, including hardware, software, configurations, and relationships. This directly supports identifying affected devices, mapping servers to switches, and tracking OS counts.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 162,
    "question_text": "An organization that performs real-time financial processing is implementing a new backup solution. Given the following business requirements:\n* The backup solution must reduce the risk for potential backup compromise\n* The backup solution must be resilient to a ransomware attack.\n* The time to restore from backups is less important than backup data integrity\n* Multiple copies of production data must be maintained\nWhich of the following backup strategies best meets these requirements?",
    "options": {
      "A": "Creating a secondary, immutable storage array and updating it with live data on a continuous basis",
      "B": "Utilizing two connected storage arrays and ensuring the arrays constantly sync",
      "C": "Enabling remote journaling on the databases to ensure real-time transactions are mirrored",
      "D": "Setting up antitempering on the databases to ensure data cannot be changed unintentionally"
    },
    "correct_answer": "A",
    "explanation": "A. Creating a secondary, immutable storage array and updating it with live data on a continuous basis: An immutable storage array ensures that data, once written, cannot be altered or deleted. This greatly reduces the risk of backup compromise and provides resilience against ransomware attacks, as the ransomware cannot modify or delete the backup data. Maintaining multiple copies of production data with an immutable storage solution ensures data integrity and compliance with the requirement for multiple copies.\nOther options:\nB. Utilizing two connected storage arrays and ensuring the arrays constantly sync: While this ensures data redundancy, it does not provide protection against ransomware attacks, as both arrays could be compromised simultaneously.\nC. Enabling remote journaling on the databases: This ensures real-time transaction mirroring but does not address the requirement for reducing the risk of backup compromise or resilience to ransomware.\nD. Setting up anti-tampering on the databases: While this helps ensure data integrity, it does not provide a comprehensive backup solution that meets all the specified requirements.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 163,
    "question_text": "A security analyst is troubleshooting the reason a specific user is having difficulty accessing company resources. The analyst reviews the following information:\n[Image of table showing User, Source IP, Source Location, User assigned location, Satisfied MFA?, Sign-in status.]\nWhich of the following is most likely the cause of the issue?",
    "options": {
      "A": "The local network access has been configured to bypass MFA requirements.",
      "B": "A network geolocation is being misidentified by the authentication server",
      "C": "Administrator access from an alternate location is blocked by company policy",
      "D": "Several users have not configured their mobile devices to receive OTP codes"
    },
    "correct_answer": "B",
    "explanation": "The table shows that the user \"SALES1\" is consistently blocked despite having met the MFA requirements. The common factor in these blocked attempts is the source IP address (8.11.4.16) being identified as from Germany while the user is assigned to France. This discrepancy suggests that the network geolocation is being misidentified by the authentication server, causing legitimate access attempts to be blocked.\nWhy Network Geolocation Misidentification?\nGeolocation Accuracy: Authentication systems often use IP geolocation to verify the location of access attempts. Incorrect geolocation data can lead to legitimate requests being denied if they appear to come from unexpected locations.\nSecurity Policies: Company security policies might block access attempts from certain locations to prevent unauthorized access. If the geolocation is wrong, legitimate users can be inadvertently blocked.\nConsistent Pattern: The user \"SALES1\" from the IP address 8.11.4.16 is always blocked, indicating a consistent issue with geolocation.\nOther options do not align with the pattern observed:\nA. Bypass MFA requirements: MFA is satisfied, so bypassing MFA is not the issue.\nC. Administrator access policy: This is about user access, not specific administrator access.\nD. OTP codes: The user has satisfied MFA, so OTP code configuration is not the issue.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 164,
    "question_text": "A security engineer wants to reduce the attack surface of a public-facing containerized application. Which of the following will best reduce the application's privilege escalation attack surface?",
    "options": {
      "A": "Implementing the following commands in the Dockerfile: RUN echo user:x:1000:1000:user:/home/user:/dev/null > /etc/passwd",
      "B": "Installing an EDR on the container's host with reporting configured to log to a centralized SIEM and implementing the following alerting rules: TF PROCESS_USER=root ALERT_TYPE=critical",
      "C": "Designing a multi-container solution, with one set of containers that runs the main application, and another set of containers that perform automatic remediation by replacing compromised containers or disabling compromised accounts",
      "D": "Running the container in an isolated network and placing a load balancer in a public-facing network. Adding the following ACL to the load balancer: PZRKZI HTTES from 0.0.0.0/0 port 443"
    },
    "correct_answer": "A",
    "explanation": "Implementing the given commands in the Dockerfile ensures that the container runs with non-root user privileges. Running applications as a non-root user reduces the risk of privilege escalation attacks because even if an attacker compromises the application, they would have limited privileges and would not be able to perform actions that require root access.\nA. Implementing the following commands in the Dockerfile: This directly addresses the privilege escalation attack surface by ensuring the application does not run with elevated privileges.\nB. Installing an EDR on the container's host: While useful for detecting threats, this does not reduce the privilege escalation attack surface within the containerized application.\nC. Designing a multi-container solution: While beneficial for modularity and remediation, it does not specifically address privilege escalation.\nD. Running the container in an isolated network: This improves network security but does not directly reduce the privilege escalation attack surface.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 165,
    "question_text": "An organization has noticed an increase in phishing campaigns utilizing typosquatting. A security analyst needs to enrich the data for commonly used domains against the domains used in phishing campaigns. The analyst uses a log forwarder to forward network logs to the SIEM. Which of the following would allow the security analyst to perform this analysis?",
    "options": {
      "A": "Use a cron job to regularly update and compare domains.",
      "B": "Create a parser that matches domains.",
      "C": "Develop a query that filters out all matching domain names.",
      "D": "Implement a dashboard on the SIEM that shows the percentage of traffic by domain."
    },
    "correct_answer": "D",
    "explanation": "Comprehensive and Detailed Explanation:\nEnriching data to compare domains requires actionable visibility. Let’s analyze:\nA. Cron job: Automates updates but doesn’t analyze in the SIEM.\nB. Parser: Processes logs but doesn’t provide comparison insights.\nC. Filter query: Excludes matches, opposite of enrichment.\nD. Implement a dashboard on the SIEM that shows the percentage of traffic by domain: This provides visualization and analysis of traffic patterns, allowing the security analyst to identify suspicious activity related to typosquatted domains.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 166,
    "question_text": "A news organization wants to implement workflows that allow users to request that untruthful data be retraced and scrubbed from online publications to comply with the right to be forgotten. Which of the following regulations is the organization most likely trying to address?",
    "options": {
      "A": "GDPR",
      "B": "COPPA",
      "C": "CCPA",
      "D": "DORA"
    },
    "correct_answer": "A",
    "explanation": "The General Data Protection Regulation (GDPR) is the regulation most likely being addressed by the news organization. GDPR includes provisions for the \"right to be forgotten,\" which allows individuals to request the deletion of personal data that is no longer necessary for the purposes for which it was collected. This regulation aims to protect the privacy and personal data of individuals within the European Union.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 167,
    "question_text": "An organization determines existing business continuity practices are inadequate to support critical internal process dependencies during a contingency event. A compliance analyst wants the Chief Information Officer (CIO) to identify the level of residual risk that is acceptable to guide remediation activities. Which of the following does the CIO need to clarify?",
    "options": {
      "A": "Mitigation",
      "B": "Impact",
      "C": "Likelihood",
      "D": "Appetite"
    },
    "correct_answer": "D",
    "explanation": "Comprehensive and Detailed Step-by-Step Explanation:\nUnderstanding Residual Risk:\nResidual risk is the amount of risk remaining after controls and mitigations have been applied.\nRisk appetite defines the level of risk an organization is willing to accept before taking additional actions.\nWhy Option D is Correct:\nThe CIO must clarify the organization’s \"Risk Appetite\" to determine how much residual risk is acceptable.\nIf risk exceeds the appetite, additional security measures need to be implemented.\nThis aligns with ISO 31000 and NIST Risk Management Framework (RMF).\nWhy Other Options Are Incorrect:\nA (Mitigation): Mitigation refers to reducing risk, but it doesn’t define the acceptable level of residual risk.\nB (Impact): Impact assessment measures potential damage, but it does not determine what is acceptable.\nC (Likelihood): Likelihood is the probability of risk occurring, but not what level is acceptable.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 168,
    "question_text": "An organization has been using self-managed encryption keys rather than the free keys managed by the cloud provider. The Chief Information Security Officer (CISO) reviews the monthly bill and realizes the self-managed keys are more costly than anticipated. Which of the following should the CISO recommend to reduce costs while maintaining a strong security posture?",
    "options": {
      "A": "Utilize an on-premises HSM to locally manage keys.",
      "B": "Adjust the configuration for cloud provider keys on data that is classified as public.",
      "C": "Begin using cloud-managed keys on all new resources deployed in the cloud.",
      "D": "Extend the key rotation period to one year so that the cloud provider can use cached keys."
    },
    "correct_answer": "B",
    "explanation": "Comprehensive and Detailed Step-by-Step Explanation:\nUnderstanding the Scenario: The organization is using customer-managed encryption keys in the cloud, which is more expensive than using the cloud provider's free managed keys. The CISO needs to find a way to reduce costs without significantly weakening the security posture.\nAnalyzing the Answer Choices:\nA. Utilize an on-premises HSM to locally manage keys: While on-premises HSMs offer strong security, they introduce additional costs and complexity (procurement, maintenance, etc.). This option is unlikely to reduce costs compared to cloud-based key management.\nB. Adjust the configuration for cloud provider keys on data that is classified as public: This is the most practical and cost-effective approach. Data classified as public doesn't require the same level of protection as sensitive data. Using the cloud provider's free managed keys for public data can significantly reduce costs without compromising security, as the data is intended to be publicly accessible anyway.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 169,
    "question_text": "Which of the following supports the process of collecting a large pool of behavioral observations to inform decision-making?",
    "options": {
      "A": "Linear regression",
      "B": "Distributed consensus",
      "C": "Big Data",
      "D": "Machine learning"
    },
    "correct_answer": "C",
    "explanation": "Collecting a large pool of behavioral observations requires handling vast datasets, which is the domain of Big Data. Big Data technologies enable the storage, processing, and analysis of large-scale data (e.g., user behavior logs) to inform decisions, a key capability in security analytics.\nOption A: Linear regression is a statistical method for modeling relationships, not collecting data.\nOption B: Distributed consensus relates to agreement in distributed systems (e.g., blockchain), not data collection.\nOption C: Big Data directly supports collecting and analyzing large datasets for insights, fitting the question perfectly.\nOption D: Machine learning uses data to train models but relies on data being collected first, often via Big Data.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 170,
    "question_text": "A company updates its cloud-based services by saving infrastructure code in a remote repository. The code is automatically deployed into the development environment every time the code is saved to the repository. The developers express concern that the deployment often fails, citing minor code issues and occasional security control check failures in the development environment. Which of the following should a security engineer recommend to reduce the deployment failures? (Select two).",
    "options": {
      "A": "Software composition analysis",
      "B": "Pre-commit code linting",
      "C": "Repository branch protection",
      "D": "Automated regression testing",
      "E": "Code submit authorization workflow",
      "F": "Pipeline compliance scanning"
    },
    "correct_answer": "B D",
    "explanation": "B. Pre-commit code linting: Linting tools analyze code for syntax errors and adherence to coding standards before the code is committed to the repository. This helps catch minor code issues early in the development process, reducing the likelihood of deployment failures.\nD. Automated regression testing: Automated regression tests ensure that new code changes do not introduce bugs or regressions into the existing codebase. By running these tests automatically during the deployment process, developers can catch issues early and ensure the stability of the development environment.\nOther options:\nA. Software composition analysis: This helps identify vulnerabilities in third-party components but does not directly address code quality or deployment failures.\nC. Repository branch protection: While this can help manage the code submission process, it does not directly prevent deployment failures caused by code issues or security check failures.\nE. Code submit authorization workflow: This manages who can submit code but does not address the quality of the code being submitted.\nF. Pipeline compliance scanning: This checks for compliance with security policies but does not address syntax or regression issues.",
    "is_simulation": false,
    "question_type": "multiple_choice_multiple_answer"
  },
  {
    "question_number": 171,
    "question_text": "A company plans to implement a research facility with Intellectual property data that should be protected. The following is the security diagram proposed by the security architect:\n[Image of diagram showing Users with MFA (log-in control), Local authentication, Enterprise CA, Network access control, Agent-based protection install, Updates, AV and integrity, IAM validation, DLP, Network DLP, EDR/SIEM, File server, Data encryption, Authentication required, Role-based access control, Secure VDI access, Lab facility (Intellectual property).]\nWhich of the following security architect models is illustrated by the diagram?",
    "options": {
      "A": "Identity and access management model",
      "B": "Agent based security model",
      "C": "Perimeter protection security model",
      "D": "Zero Trust security model"
    },
    "correct_answer": "D",
    "explanation": "The security diagram proposed by the security architect depicts a Zero Trust security model. Zero Trust is a security framework that assumes all entities, both inside and outside the network, cannot be trusted and must be verified before gaining access to resources.\nKey Characteristics of Zero Trust in the Diagram:\nRole-based Access Control: Ensures that users have access only to the resources necessary for their role.\nMandatory Access Control: Additional layer of security requiring authentication for access to sensitive areas.\nNetwork Access Control: Ensures that devices meet security standards before accessing the network.\nMulti-factor Authentication (MFA): Enhances security by requiring multiple forms of verification.\nThis model aligns with the Zero Trust principles of never trusting and always verifying access requests, regardless of their origin.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 172,
    "question_text": "A security analyst is reviewing suspicious log-in activity and sees the following data in the SIEM:\n[Image of SIEM data showing Account, Customer, Application, Authorization Level, Status, Risk.]\nWhich of the following is the most appropriate action for the analyst to take?",
    "options": {
      "A": "Update the log configuration settings on the directory server that is not being captured properly.",
      "B": "Have the admin account owner change their password to avoid credential stuffing.",
      "C": "Block employees from logging in to applications that are not part of their business area.",
      "D": "Implement automation to disable accounts that have been associated with high-risk activity."
    },
    "correct_answer": "D",
    "explanation": "The log-in activity indicates a security threat, particularly involving the ADMIN account with a high-risk failure status. This suggests that the account may be targeted by malicious activities such as credential stuffing or brute force attacks.\nUpdating log configuration settings (A) may help in better logging future activities but does not address the immediate threat.\nChanging the admin account password (B) is a good practice but may not fully mitigate the ongoing threat if the account has already been compromised.\nBlocking employees (C) from logging into non-business applications might help in reducing attack surfaces but doesn't directly address the compromised account issue.\nImplementing automation to disable accounts associated with high-risk activities ensures an immediate response to the detected threat, preventing further unauthorized access and allowing time for thorough investigation and remediation.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 173,
    "question_text": "A cybersecurity architect is reviewing the detection and monitoring capabilities for a global company that recently made multiple acquisitions. The architect discovers that the acquired companies use different vendors for detection and monitoring. The architect's goal is to:\n• Create a collection of use cases to help detect known threats\n• Include those use cases in a centralized library for use across all of the companies\nWhich of the following is the best way to achieve this goal?",
    "options": {
      "A": "Sigma rules",
      "B": "Ariel Query Language",
      "C": "UBA rules and use cases",
      "D": "TAXII/STIX library"
    },
    "correct_answer": "A",
    "explanation": "To create a collection of use cases for detecting known threats and include them in a centralized library for use across multiple companies with different vendors, Sigma rules are the best option. Here’s why:\nVendor-Agnostic Format: Sigma rules are a generic and open standard for writing SIEM (Security Information and Event Management) rules. They can be translated to specific query languages of different SIEM systems, making them highly versatile and applicable across various platforms.\nCentralized Rule Management: By using Sigma rules, the cybersecurity architect can create a centralized library of detection rules that can be easily shared and implemented across different detection and monitoring systems used by the acquired companies. This ensures consistency in threat detection capabilities.\nEase of Use and Flexibility: Sigma provides a structured and straightforward format for defining detection logic. It allows for the easy creation, modification, and sharing of rules, facilitating collaboration and standardization across the organization.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 174,
    "question_text": "A company was recently infected by malware. During the root cause analysis, the company determined that several users were installing their own applications. To prevent further compromises, the company has decided it will only allow authorized applications to run on its systems. Which of the following should the company implement?",
    "options": {
      "A": "Signing",
      "B": "Access control",
      "C": "HIPS",
      "D": "Permit listing"
    },
    "correct_answer": "D",
    "explanation": "To prevent unauthorized applications from running, the company needs a mechanism to explicitly define and enforce which applications are allowed to execute. \"Permit listing\" (often referred to as \"whitelisting\" in security contexts) is the most effective solution here. It involves creating a list of approved applications, and only those on the list are permitted to run, blocking all others by default. This directly addresses the root cause—users installing unapproved software—by restricting execution to only authorized programs.\nOption A (Signing): Code signing ensures the authenticity and integrity of software by verifying it comes from a trusted source and hasn’t been tampered with. While useful, it doesn’t inherently prevent unauthorized applications from running unless combined with a policy like whitelisting.\nOption B (Access control): Access control governs who can access systems or resources but doesn’t specifically restrict which applications can execute. It’s too broad for this scenario.\nOption C (HIPS): A Host-based Intrusion Prevention System (HIPS) can detect and block malicious behavior, but it’s reactive and relies on signatures or heuristics, not a proactive allow-only approach.\nOption D (Permit listing): This is the best fit, as it proactively enforces a policy where only explicitly authorized applications can run, preventing malware introduced by unauthorized software.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 175,
    "question_text": "An organization wants to create a threat model to identify vulnerabilities in its infrastructure. Which of the following should be prioritized first?",
    "options": {
      "A": "External-facing Infrastructure with known exploited vulnerabilities",
      "B": "Internal infrastructure with high-severity and Known exploited vulnerabilities",
      "C": "External facing Infrastructure with a low risk score and no known exploited vulnerabilities",
      "D": "External-facing infrastructure with a high risk score that can only be exploited with local access to the resource"
    },
    "correct_answer": "A",
    "explanation": "When creating a threat model to identify vulnerabilities in an organization's infrastructure, prioritizing external-facing infrastructure with known exploited vulnerabilities is critical. Here’s why:\nExposure to Attack: External-facing infrastructure is directly exposed to the internet, making it a primary target for attackers. Any vulnerabilities in this layer pose an immediate risk to the organization's security.\nKnown Exploited Vulnerabilities: Vulnerabilities that are already known and exploited in the wild are of higher concern because they are actively being used by attackers. Addressing these vulnerabilities reduces the risk of exploitation significantly.\nRisk Mitigation: By prioritizing external-facing infrastructure with known exploited vulnerabilities, the organization can mitigate the most immediate and impactful threats, thereby improving overall security posture.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 176,
    "question_text": "A security review revealed that not all of the client proxy traffic is being captured. Which of the following architectural changes best enables the capture of traffic for analysis?",
    "options": {
      "A": "Adding an additional proxy server to each segmented VLAN",
      "B": "Setting up a reverse proxy for client logging at the gateway",
      "C": "Configuring a span port on the perimeter firewall to ingest logs",
      "D": "Enabling client device logging and system event auditing"
    },
    "correct_answer": "C",
    "explanation": "Configuring a span port on the perimeter firewall to ingest logs is the best architectural change to ensure that all client proxy traffic is captured for analysis. Here’s why:\nComprehensive Traffic Capture: A span port (or mirror port) on the perimeter firewall can capture all inbound and outbound traffic, including traffic that might bypass the proxy. This ensures that all network traffic is available for analysis.\nCentralized Logging: By capturing logs at the perimeter firewall, the organization can centralize logging and analysis, making it easier to detect and investigate anomalies.\nMinimal Disruption: Implementing a span port is a non-intrusive method that does not require significant changes to the network architecture, thus minimizing disruption to existing services.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 177,
    "question_text": "A security engineer performed a code scan that resulted in many false positives. The security engineer must find a solution that improves the quality of scanning results before application deployment. Which of the following is the best solution?",
    "options": {
      "A": "Limiting the tool to a specific coding language and tuning the rule set",
      "B": "Configuring branch protection rules and dependency checks",
      "C": "Using an application vulnerability scanner to identify coding flaws in production",
      "D": "Performing updates on code libraries before code development"
    },
    "correct_answer": "A",
    "explanation": "To improve the quality of code scanning results and reduce false positives, the best solution is to limit the tool to a specific coding language and fine-tune the rule set. By configuring the code scanning tool to focus on the specific language used in the application, the tool can more accurately identify relevant issues and reduce the number of false positives. Additionally, tuning the rule set ensures that the tool's checks are appropriate for the application's context, further improving the accuracy of the scan results.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 178,
    "question_text": "An engineering team determines the cost to mitigate certain risks is higher than the asset values. The team must ensure the risks are prioritized appropriately. Which of the following is the best way to address the issue?",
    "options": {
      "A": "Data labeling",
      "B": "Branch protection",
      "C": "Vulnerability assessments",
      "D": "Purchasing insurance"
    },
    "correct_answer": "D",
    "explanation": "When the cost to mitigate certain risks is higher than the asset values, the best approach is to purchase insurance. This method allows the company to transfer the risk to an insurance provider, ensuring that financial losses are covered in the event of an incident. This approach is cost-effective and ensures that risks are prioritized appropriately without overspending on mitigation efforts.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 179,
    "question_text": "A company wants to use IoT devices to manage and monitor thermostats at all facilities. The thermostats must receive vendor security updates and limit access to other devices within the organization. Which of the following best addresses the company's requirements?",
    "options": {
      "A": "Only allowing Internet access to a set of specific domains",
      "B": "Operating IoT devices on a separate network with no access to other devices internally",
      "C": "Only allowing operation for IoT devices during a specified time window",
      "D": "Configuring IoT devices to always allow automatic updates"
    },
    "correct_answer": "B",
    "explanation": "The best approach for managing and monitoring IoT devices, such as thermostats, is to operate them on a separate network with no access to other internal devices. This segmentation ensures that the IoT devices are isolated from the main network, reducing the risk of potential security breaches affecting other critical systems. Additionally, this setup allows for secure vendor updates without exposing the broader network to potential vulnerabilities inherent in IoT devices.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 180,
    "question_text": "A security analyst is reviewing the following vulnerability assessment report:\n192.168.1.5, Host = Server1, CVSS 7.5, Web Server, Remotely Executable = Yes, Exploit = Yes\n205.1.3.5, Host = Server2, CVSS 6.5, Bind Server, Remotely Executable = Yes, Exploit = POC\n207.1.5.7, Host = Server3, CVSS 5.5, Email Server, Remotely Executable = Yes, Exploit = Yes\n192.168.1.6, Host = Server4, CVSS 9.8, Domain Controller, Remotely Executable = Yes, Exploit = Yes\nWhich of the following should be patched first to minimize attacks against internet-facing hosts?",
    "options": {
      "A": "Server1",
      "B": "Server2",
      "C": "Server3",
      "D": "Server4"
    },
    "correct_answer": "B",
    "explanation": "The question focuses on internet-facing hosts, implying external exposure. CVSS scores, remote executability, and exploit availability guide prioritization. Server2 (205.1.3.5, CVSS 6.5, Bind Server) has a public IP, suggesting it’s internet-facing, unlike Server1 and Server4 (192.168.x.x, private IPs). Server3 (207.1.5.7, CVSS 5.5) is also public but has a lower score and risk compared to Server2’s proof-of-concept (POC) exploit. Server2’s Bind Server (DNS) role is critical and commonly targeted, making it the priority.\nOption A: Server1 (CVSS 7.5) is private, not internet-facing.\nOption B: Server2 (CVSS 6.5) is internet-facing with an exploit POC, warranting immediate patching.\nOption C: Server3 (CVSS 5.5) is internet-facing but less severe.\nOption D: Server4 (CVSS 9.8) is critical but private, not internet-facing.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 181,
    "question_text": "A company that uses containers to run its applications is required to identify vulnerabilities on every container image in a private repository. The security team needs to be able to quickly evaluate whether to respond to a given vulnerability. Which of the following will allow the security team to achieve the objective with the least effort?",
    "options": {
      "A": "SAST scan reports",
      "B": "Centralized SBoM",
      "C": "CIS benchmark compliance reports",
      "D": "Credentialed vulnerability scan"
    },
    "correct_answer": "B",
    "explanation": "A centralized Software Bill of Materials (SBoM) is the best solution for identifying vulnerabilities in container images in a private repository. An SBoM provides a comprehensive inventory of all components, dependencies, and their versions within a container image, facilitating quick evaluation and response to vulnerabilities.\nWhy Centralized SBoM?\nComprehensive Inventory: An SBoM lists all software components, including their versions and dependencies, allowing for thorough vulnerability assessments.\nQuick Identification: Centralizing SBoM data enables rapid identification of affected containers when a vulnerability is disclosed.\nAutomation: SBoMs can be integrated into automated tools for continuous monitoring and alerting of vulnerabilities.\nRegulatory Compliance: Helps in meeting compliance requirements by providing a clear and auditable record of all software components used.\nOther options, while useful, do not provide the same level of comprehensive and efficient vulnerability management:\nA. SAST scan reports: Focuses on static analysis of code but may not cover all components in container images.\nC. CIS benchmark compliance reports: Ensures compliance with security benchmarks but does not provide detailed component inventory.\nD. Credentialed vulnerability scan: Useful for in-depth scans but may not be as efficient for quick vulnerability evaluation.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 182,
    "question_text": "A Chief Information Security Officer is concerned about the operational impact of ransomware. In the event of a ransomware attack, the business requires the integrity of the data to remain intact and an RPO of less than one hour. Which of the following storage strategies best satisfies the business requirements?",
    "options": {
      "A": "Full disk encryption",
      "B": "Remote journaling",
      "C": "Immutable",
      "D": "RAID 10"
    },
    "correct_answer": "B",
    "explanation": "Remote journaling continuously sends log updates to a remote system, ensuring near-real-time backup and an RPO (Recovery Point Objective) under one hour.\nKey concepts:\nRPO under one hour means minimal data loss.\nRemote journaling provides rapid recovery by keeping near-live backups.\nOther options:\nA (Full disk encryption) protects against unauthorized access but does not aid recovery.\nC (Immutable storage) prevents modification but does not ensure real-time backups.\nD (RAID 10) improves redundancy but does not help against ransomware.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 183,
    "question_text": "Audit findings indicate several user endpoints are not utilizing full disk encryption. During the remediation process, a compliance analyst reviews the testing details for the endpoints and notes the endpoint device configuration does not support full disk encryption. Which of the following is the most likely reason the device must be replaced?",
    "options": {
      "A": "The HSM is outdated and no longer supported by the manufacturer",
      "B": "The vTPM was not properly initialized and is corrupt.",
      "C": "The HSM is vulnerable to common exploits and a firmware upgrade is needed",
      "D": "The motherboard was not configured with a TPM from the OEM supplier.",
      "E": "The HSM does not support sealing storage"
    },
    "correct_answer": "D",
    "explanation": "The most likely reason the device must be replaced is that the motherboard was not configured with a TPM (Trusted Platform Module) from the OEM (Original Equipment Manufacturer) supplier.\nWhy TPM is Necessary for Full Disk Encryption:\nHardware-Based Security: TPM provides a hardware-based mechanism to store encryption keys securely, which is essential for full disk encryption.\nCompatibility: Full disk encryption solutions, such as BitLocker, require TPM to ensure that the encryption keys are securely stored and managed.\nIntegrity Checks: TPM enables system integrity checks during boot, ensuring that the device has not been tampered with.\nOther options do not directly address the requirement for TPM in supporting full disk encryption:\nA. The HSM is outdated: While HSM (Hardware Security Module) is important for security, it is not typically used for full disk encryption.\nB. The vTPM was not properly initialized: vTPM (virtual TPM) is less common and not typically a reason for requiring hardware replacement.\nC. The HSM is vulnerable to common exploits: This would require a firmware upgrade, not replacement of the device.\nE. The HSM does not support sealing storage: Sealing storage is relevant but not the primary reason for requiring TPM for full disk encryption.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 184,
    "question_text": "A company migrating to a remote work model requires that company-owned devices connect to a VPN before logging in to the device itself. The VPN gateway requires that a specific key extension is deployed to the machine certificates in the internal PKI. Which of the following best explains this requirement?",
    "options": {
      "A": "The certificate is an additional factor to meet regulatory MFA requirements for VPN access.",
      "B": "The VPN client selected the certificate with the correct key usage without user interaction.",
      "C": "The internal PKI certificate deployment allows for Wi-Fi connectivity before logging in to other systems.",
      "D": "The server connection uses SSL VPN, which uses certificates for secure communication."
    },
    "correct_answer": "B",
    "explanation": "Comprehensive and Detailed Explanation:\nThis scenario describes an enterprise VPN setup that requires machine authentication before a user logs in. The best explanation for this requirement is that the VPN client selects the appropriate certificate automatically based on the key extension in the machine certificate.\nUnderstanding the Key Extension Requirement:\nPKI (Public Key Infrastructure) issues machine certificates that include specific key usages such as Client Authentication or IPSec IKE Intermediate.\nKey usage extensions define how a certificate can be used, ensuring that only valid certificates are selected by the VPN client.\nWhy Option B is Correct:\nThe VPN automatically selects the correct machine certificate with the appropriate key extension.\nThe process occurs without user intervention, ensuring seamless VPN authentication before login.\nWhy Other Options Are Incorrect:\nA (MFA requirement): Certificates used in this scenario are for machine authentication, not user MFA. MFA typically involves user credentials plus a second factor (like OTPs or biometrics), which is not applicable here.\nC (Wi-Fi connectivity before login): This refers to pre-logon networking, which is a separate concept where devices authenticate to a Wi-Fi network before login, usually via 802.1X EAP-TLS. However, this question specifically mentions VPN authentication, not Wi-Fi authentication.\nD (SSL VPN with certificates): While SSL VPNs do use certificates, this scenario involves machine certificates issued by an internal PKI, which are commonly used in IPSec VPNs, not SSL VPNs.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 185,
    "question_text": "A security architect is mitigating a vulnerability that previously led to a web application data breach. An analysis into the root cause of the issue finds the following:\nAn administrator’s account was hijacked and used on several Autonomous System Numbers within 30 minutes.\nAll administrators use named accounts that require multifactor authentication.\nSingle sign-on is used for all company applications.\nWhich of the following should the security architect do to mitigate the issue?",
    "options": {
      "A": "Configure token theft detection on the single sign-on system with automatic account lockouts.",
      "B": "Enable context-based authentication when network locations change on administrator login attempts.",
      "C": "Decentralize administrator accounts and force unique passwords for each application.",
      "D": "Enforce biometric authentication requirements for the administrator’s named accounts."
    },
    "correct_answer": "B",
    "explanation": "Comprehensive and Detailed Explanation:\nThe hijacked administrator account was used across multiple ASNs (indicating different network locations) in a short time, despite MFA and SSO. This suggests a stolen session or token misuse. Let’s analyze:\nA. Token theft detection with lockouts: Useful for detecting stolen SSO tokens, but it’s reactive and may not prevent initial misuse across networks.\nB. Context-based authentication: This adds real-time checks (e.g., geolocation, IP changes) to verify login attempts. Given the rapid ASN changes, this proactively mitigates the issue by challenging suspicious logins, aligning with CAS-005’s focus on adaptive security.\nC. Decentralize accounts: This removes SSO, increasing complexity and weakening MFA enforcement, which isn’t practical or secure.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 186,
    "question_text": "Which of the following best describes the challenges associated with widespread adoption of homomorphic encryption techniques?",
    "options": {
      "A": "Incomplete mathematical primitives",
      "B": "No use cases to drive adoption",
      "C": "Quantum computers not yet capable",
      "D": "Insufficient coprocessor support"
    },
    "correct_answer": "D",
    "explanation": "Homomorphic encryption allows computations to be performed on encrypted data without decrypting it, providing strong privacy guarantees. However, the adoption of homomorphic encryption is challenging due to several factors:\nA. Incomplete mathematical primitives: This is not the primary barrier as the theoretical foundations of homomorphic encryption are well-developed.\nB. No use cases to drive adoption: There are several compelling use cases for homomorphic encryption, especially in privacy-sensitive fields like healthcare and finance.\nC. Quantum computers not yet capable: Quantum computing is not directly related to the challenges of adopting homomorphic encryption.\nD. Insufficient coprocessor support: The computational overhead of homomorphic encryption is significant, requiring substantial processing power. Current general-purpose processors are not optimized for the intensive computations required by homomorphic encryption, limiting its practical deployment. Specialized hardware or coprocessors designed to handle these computations more efficiently are not yet widely available.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  },
  {
    "question_number": 187,
    "question_text": "After an incident occurred, a team reported during the lessons-learned review that the team:\n* Lost important Information for further analysis.\n* Did not utilize the chain of communication\n* Did not follow the right steps for a proper response\nWhich of the following solutions is the best way to address these findings?",
    "options": {
      "A": "Requesting budget for better forensic tools to improve technical capabilities for Incident response operations",
      "B": "Building playbooks for different scenarios and performing regular table-top exercises",
      "C": "Requiring professional incident response certifications for each new team member",
      "D": "Publishing the incident response policy and enforcing it as part of the security awareness program"
    },
    "correct_answer": "B",
    "explanation": "Building playbooks for different scenarios and performing regular table-top exercises directly addresses the issues identified in the lessons-learned review. Here's why:\nLost important information for further analysis: Playbooks outline step-by-step procedures for incident response, ensuring that team members know exactly what to document and how to preserve evidence.\nDid not utilize the chain of communication: Playbooks include communication protocols, specifying who to notify and when. Regular table-top exercises reinforce these communication channels, ensuring they are followed during actual incidents.\nDid not follow the right steps for a proper response: Playbooks provide a clear sequence of actions to be taken during various types of incidents, helping the team to respond in a structured and effective manner. Regular exercises allow the team to practice these steps, identifying and correcting any deviations from the plan.\nInvesting in better forensic tools (Option A) or requiring certifications (Option C) are also valuable, but they do not directly address the procedural and communication gaps identified. Publishing and enforcing the incident response policy (Option D) is important but not as practical and hands-on as playbooks and exercises in ensuring the team is prepared.",
    "is_simulation": false,
    "question_type": "multiple_choice_single_answer"
  }
]